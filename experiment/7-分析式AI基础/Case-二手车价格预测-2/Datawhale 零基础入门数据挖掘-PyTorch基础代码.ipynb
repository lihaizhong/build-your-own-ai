{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6800ea",
   "metadata": {},
   "source": [
    "# cpu 独享\n",
    "`!pip install seaborn==0.12.2 loguru --user`\n",
    "\n",
    "# gpu 独享\n",
    "`!conda install seaborn==0.12.2 loguru scikit-learn -y`\n",
    "`!conda install -c conda-forge ipywidgets -y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805984ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "机器学习\n",
    "- 模型构建：得自己写\n",
    "- 模型权重求解：得自己写\n",
    "- 使用情况：直接调用包 scikit-learn/lightgbm/xgboost/catboost\n",
    "\n",
    "深度学习\n",
    "- 模型构建：得自己写\n",
    "- 模型权重求解：框架已打包\n",
    "- 使用情况：大量的基于框架（Torch/Paddle/Tesnorflow/...）开发\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0358557",
   "metadata": {},
   "source": [
    "# Torch 开发流程\n",
    "\n",
    "## 1. 数据准备\n",
    "\n",
    "- 数据编码\n",
    "    - 让计算机认识原始数据\n",
    "- Dataset 类构建\n",
    "    - 利用 Torch 的能力来做到一次取一个 batch 的数据\n",
    "\n",
    "## 2. 模型设计\n",
    "\n",
    "- 需要和 Dataloader 吐出来的数据形式对齐\n",
    "- 实现模型里面的子模块\n",
    "- 把子模块拼成最终的模型\n",
    "\n",
    "## 3. 跑模型流程\n",
    "\n",
    "- 八股文（不太需要定制化开发）\n",
    "- 训练流程\n",
    "- 验证/测试流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.init import xavier_normal_\n",
    "import torch.utils.data as D\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_project_path(*paths):\n",
    "    \"\"\"获取项目路径的统一方法\"\"\"\n",
    "    try:\n",
    "        return os.path.join(os.path.dirname(__file__), *paths)\n",
    "    except NameError:\n",
    "        return os.path.join(os.getcwd(), *paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "\n",
    "config = {\n",
    "    'train_path': get_project_path('data', 'used_car_train_20200313.csv'),\n",
    "    'test_path': get_project_path('data', 'used_car_testB_20200421.csv'),\n",
    "    'epoch': 15,\n",
    "    'batch_size': 512,\n",
    "    'lr': 0.001,\n",
    "    'model_ckpt_dir': get_project_path('user_data'),\n",
    "    # macOS设备不支持NVIDIA GPU\n",
    "    'device': 'cpu', # cuda:0/cpu\n",
    "    'num_cols': ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14'],\n",
    "    'cate_cols': ['model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'seller', 'notRepairedDamage']\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'is_use_cate_cols': True,\n",
    "    'embedding_dim': 4,\n",
    "    'hidden_units': [256, 128, 64, 32]\n",
    "}\n",
    "model_config['num_cols'] = config['num_cols']\n",
    "model_config['cate_cols'] = config['cate_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['train_path'], sep=' ')\n",
    "test_df = pd.read_csv(config['test_path'], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e40ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征异常值简单处理\n",
    "df.loc[df['power'] > 600, 'power'] = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293b3e5",
   "metadata": {},
   "source": [
    "## 1. 简易EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f38d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征\n",
    "for col in config['num_cols']:\n",
    "    # 绘制密度图\n",
    "    sns.kdeplot(df[col], fill=True)\n",
    "\n",
    "    # 设置图形标题与标签\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散特征\n",
    "for col in config['cate_cols']:\n",
    "    # 统计特征频次\n",
    "    counts = df[col].value_counts()\n",
    "\n",
    "    # 绘制条形图\n",
    "    counts.plot(kind='bar', figsize=(20, 8))\n",
    "\n",
    "    # 设置图形标题和标签\n",
    "    plt.title(f'{col} Frequencies')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffaf8da",
   "metadata": {},
   "source": [
    "## 2. 简易特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散特征编码\n",
    "vocab_map = defaultdict(dict)\n",
    "for col in tqdm(config['cate_cols']):\n",
    "    df[col] = df[col].fillna('-1')\n",
    "    map_dict = dict(zip(df[col].unique(), range(df[col].nunique())))\n",
    "    # label enc\n",
    "    df[col] = df[col].map(map_dict)\n",
    "\n",
    "model_config['vocab_map'] = vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b775b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['vocab_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d724a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续特征编码\n",
    "for col in config['num_cols']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "    df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b07b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['price'].notna()].reset_index(drop=True)\n",
    "# 标签范围太大不利于神经网络进行拟合，这里先对其进行 log 变换\n",
    "train_df['price'] = np.log(train_df['price'])\n",
    "test_df = df[df['price'].isna()].reset_index(drop=True)\n",
    "del test_df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2a5fa",
   "metadata": {},
   "source": [
    "## 3. 定义 DataSet\n",
    "\n",
    "- 理解数据原始形式\n",
    "- 理解数据编码方式\n",
    "- 理解如何进行数据 I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc871aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet 构造\n",
    "class SaleDataset(Dataset):\n",
    "    def __init__(self, df, cate_cols, num_cols):\n",
    "        self.df = df\n",
    "        self.feature_name = cate_cols + num_cols\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = dict()\n",
    "        for col in self.feature_name:\n",
    "            data[col] = torch.Tensor([self.df[col].iloc[index]]).squeeze(-1)\n",
    "        \n",
    "        if 'price' in self.df.columns:\n",
    "            data['price'] = torch.Tensor([self.df['price'].iloc[index]]).squeeze(-1)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "def get_dataloader(df, cate_cols, num_cols, batch_size=256, num_workers=2, shuffle=True):\n",
    "    dataset = SaleDataset(df, cate_cols, num_cols)\n",
    "    dataloader = D.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "train_dataset = SaleDataset(train_df, config['cate_cols'], config['num_cols'])\n",
    "train_dataset.__getitem__(888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49a69a",
   "metadata": {},
   "source": [
    "## 4. 定义模型\n",
    "\n",
    "- 定义各个子模块\n",
    "- 将子模块合并成最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user 进行 Embedding 化，有 4 个 user，我想把每个 user 编码成一个 8 维的向量\n",
    "num_user = 4\n",
    "emb_dim = 8\n",
    "user_emb_layer = nn.Embedding(num_user, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = torch.Tensor([0, 2]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0507508",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb_layer(query_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding层：用于对离散特征进行编码映射\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_map=None, embedding_dim=None):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.vocab_map = vocab_map\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = nn.ModuleDict()\n",
    "\n",
    "        self.emb_feature = []\n",
    "        # 使用字典来存储每个离散特征的 Embedding 标\n",
    "        for col in self.vocab_map.keys():\n",
    "            self.emb_feature.append(col)\n",
    "            self.embedding_layer.update({ col: nn.Embedding(self.vocab_map[col]['vocab_size'], self.embedding_dim) })\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 对所有的 sparse 特征挨个进行 embedding\n",
    "        feature_emb_list = []\n",
    "        for col in self.emb_feature:\n",
    "            inp = X[col].long().view(-1, 1)\n",
    "            feature_emb_list.append(self.embedding_layer[col](inp))\n",
    "        \n",
    "        return torch.cat(feature_emb_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim=None,\n",
    "        hidden_units=[],\n",
    "        hidden_activations='ReLU',\n",
    "        final_activation=None,\n",
    "        dropout_rates=0,\n",
    "        batch_norm=False,\n",
    "        use_bias=True\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        dense_layers = []\n",
    "        if not isinstance(dropout_rates, list):\n",
    "            dropout_rates = [dropout_rates] * len(hidden_units)\n",
    "\n",
    "        if not isinstance(hidden_activations, list):\n",
    "            hidden_activations = [hidden_activations] * len(hidden_units)\n",
    "\n",
    "        hidden_activations = [self.set_activation(x) for x in hidden_activations]\n",
    "        hidden_units = [input_dim] + hidden_units\n",
    "        for idx in range(len(hidden_units) - 1):\n",
    "            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n",
    "            if batch_norm:\n",
    "                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n",
    "            if hidden_activations[idx]:\n",
    "                dense_layers.append(hidden_activations[idx])\n",
    "            if dropout_rates[idx] > 0:\n",
    "                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n",
    "        \n",
    "        if output_dim is not None:\n",
    "            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n",
    "        \n",
    "        if final_activation is not None:\n",
    "            dense_layers.append(self.set_activation(final_activation))\n",
    "        self.dnn = nn.Sequential(*dense_layers) # * used to unpack list\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        if isinstance(activation, str):\n",
    "            if activation.lower() == 'relu':\n",
    "                return nn.ReLU()\n",
    "            elif activation.lower() == 'sigmoid':\n",
    "                return nn.Sigmoid()\n",
    "            elif activation.lower() == 'tanh':\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                return getattr(nn, activation)()\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.dnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaleModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_use_cate_cols=True,\n",
    "        vocab_map=None,\n",
    "        embedding_dim=16,\n",
    "        num_cols=None,\n",
    "        cate_cols=None,\n",
    "        hidden_units=[256, 128, 64, 32],\n",
    "        loss_fun='nn.L1Loss()'\n",
    "    ):\n",
    "        super(SaleModel, self).__init__()\n",
    "        self.is_use_cate_cols = is_use_cate_cols\n",
    "        self.vocab_map = vocab_map\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_cols = num_cols\n",
    "        self.num_nums_fea = len(num_cols)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.loss_fun = eval(loss_fun) # self.loss_fun = nn.L1Loss()\n",
    "\n",
    "        if is_use_cate_cols:\n",
    "            self.emb_layer = EmbeddingLayer(vocab_map, embedding_dim=embedding_dim)\n",
    "            self.mlp = MLP(\n",
    "                self.num_nums_fea + self.embedding_dim * len(vocab_map),\n",
    "                output_dim=1,\n",
    "                hidden_units=self.hidden_units,\n",
    "                hidden_activations='ReLU',\n",
    "                final_activation=None,\n",
    "                dropout_rates=0,\n",
    "                batch_norm=True,\n",
    "                use_bias=True\n",
    "            )\n",
    "        else:\n",
    "            self.mlp = MLP(\n",
    "                self.num_nums_fea,\n",
    "                output_dim=1,\n",
    "                hidden_units=self.hidden_units,\n",
    "                hidden_activations='ReLU',\n",
    "                final_activation=None,\n",
    "                dropout_rates=0,\n",
    "                batch_norm=True,\n",
    "                use_bias=True\n",
    "            )\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            xavier_normal_(module.weight.data)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            xavier_normal_(module.weight.data)\n",
    "    \n",
    "    def get_dense_input(self, data):\n",
    "        dense_input = []\n",
    "        for col in self.num_cols:\n",
    "            dense_input.append(data[col])\n",
    "        \n",
    "        return torch.stack(dense_input, dim=1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        dense_fea = self.get_dense_input(data) # [batch, num_nums_cols]\n",
    "        if self.is_use_cate_cols:\n",
    "            sparse_fea = self.emb_layer(data) # [batch, num_cate_cols, emb]\n",
    "            sparse_fea = torch.flatten(sparse_fea, start_dim=1) # [batch, num_cate_cols * emb]\n",
    "            mlp_input = torch.cat([sparse_fea, dense_fea], axis=-1) # [batch, num_nums_col + num_cate_cols * emb]\n",
    "        else:\n",
    "            mlp_input = dense_fea\n",
    "        \n",
    "        y_pred = self.mlp(mlp_input)\n",
    "        # 为了把复杂多变的 loss 计算对外不感知，所以写在 forward 里面\n",
    "        if 'price' in data.keys():\n",
    "            loss = self.loss_fun(y_pred.squeeze(), data['price'])\n",
    "            output_dict = { 'pred': y_pred, 'loss': loss }\n",
    "        else:\n",
    "            output_dict = { 'pred': y_pred }\n",
    "        \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e96f68",
   "metadata": {},
   "source": [
    "## 5. 训练 Pipeline\n",
    "\n",
    "- 训练 Pipeline\n",
    "- 验证/测试 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，验证模型，这里就是八股文，熟悉基础 pipeline\n",
    "def train_model(model, train_loader, optimizer, device, metric_list=['mean_absolute_error']):\n",
    "    model.train()\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    max_iter = int(train_loader.dataset.__len__() / train_loader.batch_size)\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        # 把数据拷贝在指定的 device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向 + Loss 计算\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "        loss = output['loss']\n",
    "        # 八股文完成模型权重更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "        label_list.extend(data['price'].squeeze(-1).detach().numpy())\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            logger.info(f'Iter: {idx}/{max_iter} Loss: {round(loss.item(), 4)}')\n",
    "    \n",
    "    res_dict = dict()\n",
    "    for metric in metric_list:\n",
    "        res_dict[metric] = eval(metric)(label_list, pred_list)\n",
    "    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_model(model, valid_loader, device, metric_list=['mean_absolute_error']):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for data in (valid_loader):\n",
    "        # 把数据拷贝在指定的 device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "\n",
    "        pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())\n",
    "        label_list.extend(data['price'].squeeze(-1).cpu().detach().numpy())\n",
    "\n",
    "    res_dict = dict()\n",
    "    for metric in metric_list:\n",
    "        res_dict[metric] = eval(metric)(label_list, pred_list)\n",
    "    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "\n",
    "    for data in test_loader:\n",
    "        # 把数据拷贝在指定的 device\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].to(device)\n",
    "        # 模型前向\n",
    "        output = model(data)\n",
    "        pred = output['pred']\n",
    "        pred_list.extend(pred.squeeze().cpu().detach().numpy())\n",
    "    \n",
    "    return np.array(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5f479",
   "metadata": {},
   "source": [
    "## 6. 交叉验证 + 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_dataloader(\n",
    "    test_df,\n",
    "    config['cate_cols'],\n",
    "    config['num_cols'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "n_fold = 5\n",
    "oof_pre = np.zeros(len(train_df))\n",
    "y_pre = np.zeros(len(test_df))\n",
    "device = torch.device(config['device'])\n",
    "\n",
    "kf = KFold(n_splits=n_fold)\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    logger.info(f'Fold {fold + 1}')\n",
    "    temp_train_df = train_df.iloc[trn_idx].reset_index(drop=True)\n",
    "    temp_valid_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_loader = get_dataloader(\n",
    "        temp_train_df,\n",
    "        config['cate_cols'],\n",
    "        config['num_cols'],\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "    valid_loader = get_dataloader(\n",
    "        temp_valid_df,\n",
    "        config['cate_cols'],\n",
    "        config['num_cols'],\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # 声明模型\n",
    "    model = SaleModel(**model_config)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=0\n",
    "    )\n",
    "    # 声明 Trainer\n",
    "    for epoch in range(config['epoch']):\n",
    "        # 模型训练\n",
    "        logger.info(f'Start Training Epoch: {epoch + 1}')\n",
    "        train_metric = train_model(model, train_loader, optimizer=optimizer, device=device)\n",
    "        logger.info(f'Train metric: {train_metric}')\n",
    "        # 模型验证\n",
    "        valid_metric = valid_model(model, valid_loader, device)\n",
    "        logger.info(f'Valid Metric: {valid_metric}')\n",
    "    \n",
    "    # 保存模型权重和 enc_dict\n",
    "    save_dict = { 'model': model.state_dict() }\n",
    "    torch.save(save_dict, os.path.join(config['model_ckpt_dir'], f'model_{fold}.pth'))\n",
    "    # oof 推理\n",
    "    oof_pre[val_idx] = test_model(model, valid_loader, device=device)\n",
    "    # 测试集推理\n",
    "    y_pre += np.array(test_model(model, test_loader, device=device)) / n_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际价格的预测结果\n",
    "oof_pre_ori = np.exp(oof_pre)\n",
    "price_ori = np.exp(train_df['price'])\n",
    "mean_absolute_error(price_ori, oof_pre_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame()\n",
    "res_df['SaleID'] = test_df['SaleID']\n",
    "res_df['price'] = np.exp(y_pre)\n",
    "res_df.to_csv(get_project_path('data', 'torch_baseline.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-ai (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
