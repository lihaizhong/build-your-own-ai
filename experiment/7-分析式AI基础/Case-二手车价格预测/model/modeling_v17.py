"""
V17ç‰ˆæœ¬æ¨¡å‹ - å†²å‡»500åˆ†ç›®æ ‡

åŸºäºV16çš„çªç ´æ€§è¿›å±•ï¼Œå®æ–½ä»¥ä¸‹é«˜çº§ä¼˜åŒ–ç­–ç•¥:
1. é«˜çº§ç‰¹å¾å·¥ç¨‹ - ä¸šåŠ¡é€»è¾‘ç‰¹å¾å’Œäº¤äº’ç‰¹å¾
2. è¶…å‚æ•°ç²¾ç»†è°ƒä¼˜ - åŸºäºV16æœ€ä½³é…ç½®çš„ç½‘æ ¼æœç´¢
3. Stackingé›†æˆç­–ç•¥ - çº¿æ€§å›å½’å…ƒå­¦ä¹ å™¨
4. æ•°æ®è´¨é‡æå‡ - ç²¾ç¡®å¼‚å¸¸å€¼å¤„ç†å’Œåˆ†å¸ƒä¸€è‡´æ€§
5. æ¨¡å‹èåˆä¼˜åŒ– - å¤šå±‚é›†æˆå’Œæ™ºèƒ½æ ¡å‡†
"""

import os
from typing import Tuple, Dict, Any, List, Union
import pandas as pd  # type: ignore
import numpy as np  # type: ignore
from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV  # type: ignore
from sklearn.ensemble import RandomForestRegressor, StackingRegressor  # type: ignore
from sklearn.linear_model import Ridge, LinearRegression  # type: ignore
from sklearn.preprocessing import LabelEncoder, RobustScaler, PolynomialFeatures  # type: ignore
from sklearn.metrics import mean_absolute_error  # type: ignore
from sklearn.feature_selection import SelectFromModel  # type: ignore
import lightgbm as lgb  # type: ignore
import xgboost as xgb  # type: ignore
from catboost import CatBoostRegressor  # type: ignore
import matplotlib.pyplot as plt  # type: ignore
from scipy import stats  # type: ignore
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths: str) -> str:
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths: str) -> str:
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

def load_and_preprocess_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    é«˜çº§æ•°æ®åŠ è½½å’Œé¢„å¤„ç† - ç²¾ç¡®å¼‚å¸¸å€¼å¤„ç†
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆå¹¶æ•°æ®è¿›è¡Œç»Ÿä¸€é¢„å¤„ç†
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # é«˜çº§powerå¼‚å¸¸å€¼å¤„ç† - åŸºäºç»Ÿè®¡åˆ†å¸ƒ
    if 'power' in all_df.columns:
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        Q1 = all_df['power'].quantile(0.25)
        Q3 = all_df['power'].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + 3 * IQR  # æ›´å®½æ¾çš„è¾¹ç•Œ
        all_df['power'] = np.clip(all_df['power'], 0, min(upper_bound, 600))
        
        # æ·»åŠ powerå¼‚å¸¸å€¼æ ‡è®°
        all_df['power_outlier'] = ((all_df['power'] <= 0) | (all_df['power'] >= upper_bound)).astype(int)
    
    # åˆ†ç±»ç‰¹å¾é«˜çº§ç¼ºå¤±å€¼å¤„ç†
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model']
    for col in categorical_cols:
        if col in all_df.columns:
            # åŸºäºå…¶ä»–ç‰¹å¾çš„æ™ºèƒ½å¡«å……
            if col == 'model' and 'brand' in all_df.columns:
                # åŒå“ç‰Œä¸‹æœ€å¸¸è§çš„å‹å·
                for brand in all_df['brand'].unique():
                    brand_mask = all_df['brand'] == brand
                    brand_mode = all_df[brand_mask][col].mode()  # type: ignore
                    if len(brand_mode) > 0:
                        all_df.loc[brand_mask & all_df[col].isnull(), col] = brand_mode.iloc[0]  # type: ignore
            
            # å…¨å±€ä¼—æ•°å¡«å……å‰©ä½™ç¼ºå¤±å€¼
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])  # type: ignore
            
            # ç¼ºå¤±å€¼æ ‡è®°ç‰¹å¾
            all_df[f'{col}_missing'] = (all_df[col].isnull()).astype(int)
    
    # æ—¶é—´ç‰¹å¾å·¥ç¨‹ - æ›´ç²¾ç»†çš„å¤„ç†
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(all_df['car_age'].median()).astype(int)
    
    # æ·»åŠ æ³¨å†Œæœˆä»½å’Œå­£åº¦ç‰¹å¾
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    all_df['reg_quarter'] = all_df['regDate'].dt.quarter.fillna(2).astype(int)
    all_df.drop(columns=['regDate'], inplace=True)
    
    # é«˜çº§å“ç‰Œç»Ÿè®¡ç‰¹å¾
    if 'price' in all_df.columns:
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'std', 'count', 'median']).reset_index()
        
        # æ›´æ™ºèƒ½çš„å¹³æ»‘ - åŸºäºæ ·æœ¬æ•°é‡è°ƒæ•´å¹³æ»‘å› å­
        brand_stats['smooth_factor'] = np.where(brand_stats['count'] < 10, 100, 
                                              np.where(brand_stats['count'] < 50, 50, 30))
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     all_df['price'].mean() * brand_stats['smooth_factor']) / 
                                    (brand_stats['count'] + brand_stats['smooth_factor']))
        
        # å“ç‰Œä»·æ ¼ç¨³å®šæ€§æŒ‡æ ‡
        brand_stats['price_cv'] = brand_stats['std'] / (brand_stats['mean'] + 1e-6)
        
        # æ˜ å°„å“ç‰Œç‰¹å¾
        brand_maps = {
            'brand_avg_price': brand_stats.set_index('brand')['smooth_mean'],
            'brand_price_std': brand_stats.set_index('brand')['std'].fillna(0),
            'brand_count': brand_stats.set_index('brand')['count'],
            'brand_price_cv': brand_stats.set_index('brand')['price_cv'].fillna(0)
        }
        
        for feature_name, brand_map in brand_maps.items():
            all_df[feature_name] = all_df['brand'].map(brand_map).fillna(all_df['price'].mean() if 'price' in brand_map else 0)  # type: ignore
    
    # æ ‡ç­¾ç¼–ç  - ä¿ç•™ç¼–ç æ˜ å°„ç”¨äºä¸€è‡´æ€§
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    label_encoders = {}
    
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
            label_encoders[col] = le
    
    # é«˜çº§æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                # åŸºäºç›¸å…³ç‰¹å¾çš„æ™ºèƒ½å¡«å……
                if col in ['kilometer', 'power']:
                    # åŸºäºè½¦é¾„çš„å¡«å……
                    for age_group in all_df['car_age'].quantile([0, 0.25, 0.5, 0.75, 1]).values:
                        age_mask = (all_df['car_age'] >= age_group) & (all_df['car_age'] <= age_group + 2)
                        group_median = all_df[age_mask][col].median()  # type: ignore
                        if not pd.isna(group_median):  # type: ignore
                            all_df.loc[age_mask & all_df[col].isnull(), col] = group_median  # type: ignore[arg-type]
                
                # æœ€ç»ˆä¸­ä½æ•°å¡«å……
                median_val = all_df[col].median()
                if not pd.isna(median_val):  # type: ignore
                    all_df[col] = all_df[col].fillna(median_val)  # type: ignore
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # é‡æ–°åˆ†ç¦»
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_df.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_df.shape}")
    
    return train_df, test_df

def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    é«˜çº§ç‰¹å¾å·¥ç¨‹ - ä¸šåŠ¡é€»è¾‘å’Œäº¤äº’ç‰¹å¾
    """
    df = df.copy()
    
    # 1. é«˜çº§åˆ†æ®µç‰¹å¾ - åŸºäºæ•°æ®åˆ†å¸ƒçš„è‡ªé€‚åº”åˆ†æ®µ
    df['age_segment'] = pd.qcut(df['car_age'], q=5, labels=False, duplicates='drop')
    
    if 'power' in df.columns:
        df['power_segment'] = pd.qcut(df['power'], q=4, labels=False, duplicates='drop')
    
    if 'kilometer' in df.columns:
        df['km_segment'] = pd.qcut(df['kilometer'], q=5, labels=False, duplicates='drop')
    
    # 2. ä¸šåŠ¡é€»è¾‘ç‰¹å¾
    if 'power' in df.columns and 'car_age' in df.columns:
        # åŠŸç‡è¡°å‡ç‰¹å¾
        df['power_decay'] = df['power'] / (df['car_age'] + 1)
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_per_year'] = df['power'] / np.maximum(df['car_age'], 1)
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        # é‡Œç¨‹ç›¸å…³ç‰¹å¾
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        df['km_age_ratio'] = df['kilometer'] / (df['car_age'] + 1)
        
        # é‡Œç¨‹ä½¿ç”¨å¼ºåº¦åˆ†ç±»
        df['usage_intensity'] = pd.cut(df['km_per_year'], 
                                     bins=[0, 10000, 20000, 30000, float('inf')],
                                     labels=['low', 'medium', 'high', 'very_high'])
        df['usage_intensity'] = df['usage_intensity'].cat.codes
    
    # 3. å“ç‰Œå’Œè½¦å‹çš„é«˜çº§ç‰¹å¾
    if 'brand' in df.columns and 'model' in df.columns:
        # å“ç‰Œå†…è½¦å‹æ’å
        brand_model_stats = df.groupby(['brand', 'model']).size().reset_index(name='count')  # type: ignore[arg-type]
        brand_model_rank = brand_model_stats.sort_values(['brand', 'count'], ascending=[True, False])
        brand_model_rank['rank_in_brand'] = brand_model_rank.groupby('brand').cumcount() + 1  # type: ignore
        
        # æ˜ å°„è½¦å‹çƒ­åº¦æ’å
        rank_map = brand_model_rank.set_index(['brand', 'model'])['rank_in_brand'].to_dict()
        df['model_popularity_rank'] = df.apply(lambda row: rank_map.get((row['brand'], row['model']), 999), axis=1)  # type: ignore
    
    # 4. æ—¶é—´ç‰¹å¾
    if 'reg_month' in df.columns:
        df['reg_season'] = df['reg_month'].map({1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 
                                               7:3, 8:3, 9:4, 10:4, 11:4, 12:4})  # type: ignore
    
    # 5. æ•°å€¼ç‰¹å¾çš„å¤šé¡¹å¼å’Œäº¤äº’ç‰¹å¾
    numeric_features = ['car_age', 'power', 'kilometer'] if 'power' in df.columns else ['car_age', 'kilometer']
    available_numeric = [col for col in numeric_features if col in df.columns]
    
    if len(available_numeric) >= 2:
        # åˆ›å»ºäº¤äº’ç‰¹å¾
        for i, col1 in enumerate(available_numeric):
            for col2 in available_numeric[i+1:]:
                # ä¹˜ç§¯ç‰¹å¾
                df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
                # æ¯”ç‡ç‰¹å¾
                df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)
    
    # 6. vç‰¹å¾çš„é«˜çº§ç»Ÿè®¡
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        # åŸºç¡€ç»Ÿè®¡
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)  # type: ignore
        df['v_max'] = df[v_cols].max(axis=1)
        df['v_min'] = df[v_cols].min(axis=1)
        df['v_range'] = df['v_max'] - df['v_min']
        df['v_skew'] = df[v_cols].skew(axis=1).fillna(0)  # type: ignore
        
        # ä¸»æˆåˆ†åˆ†æç‰¹å¾
        df['v_sum'] = df[v_cols].sum(axis=1)
        df['v_positive_count'] = (df[v_cols] > 0).sum(axis=1)
        df['v_negative_count'] = (df[v_cols] < 0).sum(axis=1)
    
    # 7. å¯¹æ•°å˜æ¢ç‰¹å¾
    log_features = ['car_age', 'kilometer', 'power'] if 'power' in df.columns else ['car_age', 'kilometer']
    for col in log_features:
        if col in df.columns:
            df[f'log_{col}'] = np.log1p(np.maximum(df[col], 0))
            df[f'sqrt_{col}'] = np.sqrt(np.maximum(df[col], 0))
    
    # 8. å¼‚å¸¸å€¼å’Œç‰¹æ®Šå€¼æ ‡è®°
    if 'power' in df.columns:
        df['power_zero'] = (df['power'] <= 0).astype(int)
        df['power_very_high'] = (df['power'] > 400).astype(int)
    
    if 'kilometer' in df.columns:
        df['km_very_high'] = (df['kilometer'] > 300000).astype(int)
        df['km_very_low'] = (df['kilometer'] < 10000).astype(int)
    
    # 9. æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # å¤„ç†æ— ç©·å¤§å€¼
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # å¤„ç†NaNå€¼
        null_count = df[col].isnull().sum()
        if null_count > 0:
            if col in ['price']:
                continue  # ä¿ç•™ç›®æ ‡å˜é‡çš„NaN
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val if not pd.isna(median_val) else 0)  # type: ignore
        
        # æ¸©å’Œçš„å¼‚å¸¸å€¼å¤„ç† - åŸºäºåˆ†å¸ƒçš„åŠ¨æ€æˆªæ–­
        if col not in ['SaleID', 'price'] and df[col].std() > 1e-8:
            # ä½¿ç”¨3-sigmaè§„åˆ™ï¼Œä½†æ›´å®½æ¾
            mean_val = df[col].mean()
            std_val = df[col].std()
            lower_bound = mean_val - 4 * std_val
            upper_bound = mean_val + 4 * std_val
            
            # åªå¤„ç†æç«¯å€¼
            df[col] = np.clip(df[col], lower_bound, upper_bound)
    
    return df

def optimize_hyperparameters(X_train: pd.DataFrame, y_train: pd.Series) -> Dict[str, Dict[str, Any]]:
    """
    ä¼˜åŒ–çš„è¶…å‚æ•°è°ƒä¼˜ - ä½¿ç”¨éšæœºæœç´¢å¤§å¹…æå‡é€Ÿåº¦
    """
    print("å¼€å§‹ä¼˜åŒ–çš„è¶…å‚æ•°è°ƒä¼˜...")
    
    # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
    y_train_log = np.log1p(y_train)
    
    # ç¼©å°çš„LightGBMå‚æ•°åˆ†å¸ƒ - åŸºäºç»éªŒæœ€ä½³èŒƒå›´
    lgb_param_dist = {
        'num_leaves': [31, 50, 70],  # å‡å°‘é€‰é¡¹
        'max_depth': [6, 8, 10],     # å‡å°‘é€‰é¡¹
        'learning_rate': [0.05, 0.1, 0.15],  # å‡å°‘é€‰é¡¹
        'feature_fraction': [0.8, 0.9],      # å‡å°‘é€‰é¡¹
        'lambda_l1': [0.1, 0.2],             # å‡å°‘é€‰é¡¹
        'lambda_l2': [0.1, 0.2]              # å‡å°‘é€‰é¡¹
    }
    
    # ç¼©å°çš„XGBoostå‚æ•°åˆ†å¸ƒ - åŸºäºç»éªŒæœ€ä½³èŒƒå›´
    xgb_param_dist = {
        'max_depth': [6, 8, 10],             # å‡å°‘é€‰é¡¹
        'learning_rate': [0.05, 0.1, 0.15],  # å‡å°‘é€‰é¡¹
        'subsample': [0.8, 0.9],             # å‡å°‘é€‰é¡¹
        'colsample_bytree': [0.8, 0.9],      # å‡å°‘é€‰é¡¹
        'reg_alpha': [0.3, 0.5],             # å‡å°‘é€‰é¡¹
        'reg_lambda': [0.3, 0.5]             # å‡å°‘é€‰é¡¹
    }
    
    # ç¼©å°çš„CatBoostå‚æ•°åˆ†å¸ƒ - åŸºäºç»éªŒæœ€ä½³èŒƒå›´
    catboost_param_dist = {
        'depth': [6, 8, 10],                 # å‡å°‘é€‰é¡¹
        'learning_rate': [0.05, 0.1, 0.15],  # å‡å°‘é€‰é¡¹
        'l2_leaf_reg': [1.0, 2.0],           # å‡å°‘é€‰é¡¹
        'random_strength': [0.3, 0.5]        # å‡å°‘é€‰é¡¹
    }
    
    # åŸºç¡€æ¨¡å‹ - å‡å°‘è®­ç»ƒè½®æ•°
    lgb_base = lgb.LGBMRegressor(objective='mae', metric='mae', random_state=42, n_estimators=100)  # å‡å°‘è½®æ•°
    xgb_base = xgb.XGBRegressor(objective='reg:absoluteerror', eval_metric='mae', random_state=42, n_estimators=100)  # å‡å°‘è½®æ•°
    cat_base = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', random_seed=42, iterations=100, verbose=False)  # å‡å°‘è½®æ•°
    
    # 3æŠ˜äº¤å‰éªŒè¯
    kfold = KFold(n_splits=3, shuffle=True, random_state=42)
    
    best_params = {}
    n_iter = 20  # éšæœºæœç´¢æ¬¡æ•°ï¼Œå¤§å¹…å‡å°‘æœç´¢ç©ºé—´
    
    # LightGBMè°ƒä¼˜ - ä½¿ç”¨RandomizedSearchCV
    print("è°ƒä¼˜LightGBM (éšæœºæœç´¢)...")
    import time
    start_time = time.time()
    lgb_search = RandomizedSearchCV(lgb_base, lgb_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    lgb_search.fit(X_train, y_train_log)
    best_params['lgb'] = lgb_search.best_params_
    lgb_time = time.time() - start_time
    print(f"LightGBMæœ€ä½³å‚æ•°: {lgb_search.best_params_}")
    print(f"LightGBMæœ€ä½³åˆ†æ•°: {-lgb_search.best_score_:.2f}")
    print(f"LightGBMè°ƒä¼˜ç”¨æ—¶: {lgb_time:.1f}ç§’")
    
    # XGBoostè°ƒä¼˜ - ä½¿ç”¨RandomizedSearchCV
    print("è°ƒä¼˜XGBoost (éšæœºæœç´¢)...")
    start_time = time.time()
    xgb_search = RandomizedSearchCV(xgb_base, xgb_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    xgb_search.fit(X_train, y_train_log)
    best_params['xgb'] = xgb_search.best_params_
    xgb_time = time.time() - start_time
    print(f"XGBoostæœ€ä½³å‚æ•°: {xgb_search.best_params_}")
    print(f"XGBoostæœ€ä½³åˆ†æ•°: {-xgb_search.best_score_:.2f}")
    print(f"XGBoostè°ƒä¼˜ç”¨æ—¶: {xgb_time:.1f}ç§’")
    
    # CatBoostè°ƒä¼˜ - ä½¿ç”¨RandomizedSearchCV
    print("è°ƒä¼˜CatBoost (éšæœºæœç´¢)...")
    start_time = time.time()
    cat_search = RandomizedSearchCV(cat_base, catboost_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    cat_search.fit(X_train, y_train_log)
    best_params['cat'] = cat_search.best_params_
    cat_time = time.time() - start_time
    print(f"CatBoostæœ€ä½³å‚æ•°: {cat_search.best_params_}")
    print(f"CatBoostæœ€ä½³åˆ†æ•°: {-cat_search.best_score_:.2f}")
    print(f"CatBoostè°ƒä¼˜ç”¨æ—¶: {cat_time:.1f}ç§’")
    
    print(f"\nè¶…å‚æ•°è°ƒä¼˜æ€»ç”¨æ—¶: {lgb_time + xgb_time + cat_time:.1f}ç§’")
    
    return best_params

def train_models_with_optimized_params(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, best_params: Dict[str, Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, List[float]]]:
    """
    ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹
    """
    print("ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹...")
    
    # å¯¹æ•°å˜æ¢
    y_train_log = np.log1p(y_train)
    
    # 5æŠ˜äº¤å‰éªŒè¯
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°
    lgb_params = best_params['lgb']
    lgb_params.update({
        'objective': 'mae',
        'metric': 'mae',
        'bagging_freq': 5,
        'min_child_samples': 20,
        'random_state': 42,
    })
    
    xgb_params = best_params['xgb']
    xgb_params.update({
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'min_child_weight': 10,
        'random_state': 42
    })
    
    catboost_params = best_params['cat']
    catboost_params.update({
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'iterations': 500,
        'random_seed': 42,
        'verbose': False
    })
    
    # å­˜å‚¨é¢„æµ‹ç»“æœ
    lgb_predictions = np.zeros(len(X_test))
    xgb_predictions = np.zeros(len(X_test))
    cat_predictions = np.zeros(len(X_test))
    
    # å­˜å‚¨éªŒè¯åˆ†æ•°
    lgb_scores, xgb_scores, cat_scores = [], [], []
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train), 1):
        print(f"è®­ç»ƒç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]  # type: ignore
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]  # type: ignore
        
        # è®­ç»ƒLightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1000)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])
        
        lgb_predictions += np.expm1(np.array(lgb_model.predict(X_test))) / 5
        lgb_val_pred = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_val_pred)
        lgb_scores.append(lgb_mae)
        
        # è®­ç»ƒXGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000, early_stopping_rounds=50)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        xgb_predictions += np.expm1(xgb_model.predict(X_test)) / 5
        xgb_val_pred = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_val_pred)
        xgb_scores.append(xgb_mae)
        
        # è®­ç»ƒCatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=50, 
                     verbose=False)
        
        cat_predictions += np.expm1(cat_model.predict(X_test)) / 5
        cat_val_pred = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_val_pred)
        cat_scores.append(cat_mae)
        
        print(f"  LightGBM MAE: {lgb_mae:.2f}, XGBoost MAE: {xgb_mae:.2f}, CatBoost MAE: {cat_mae:.2f}")
    
    print(f"\nå¹³å‡éªŒè¯åˆ†æ•°:")
    print(f"  LightGBM: {np.mean(lgb_scores):.2f} (Â±{np.std(lgb_scores):.2f})")
    print(f"  XGBoost: {np.mean(xgb_scores):.2f} (Â±{np.std(xgb_scores):.2f})")
    print(f"  CatBoost: {np.mean(cat_scores):.2f} (Â±{np.std(cat_scores):.2f})")
    
    return lgb_predictions, xgb_predictions, cat_predictions, {
        'lgb_scores': lgb_scores,
        'xgb_scores': xgb_scores,
        'cat_scores': cat_scores
    }

def stacking_ensemble(X_train: pd.DataFrame, y_train: Union[pd.Series, pd.DataFrame], X_test: pd.DataFrame, lgb_pred: np.ndarray, xgb_pred: np.ndarray, cat_pred: np.ndarray) -> np.ndarray:
    """
    Stackingé›†æˆç­–ç•¥ - çº¿æ€§å›å½’å…ƒå­¦ä¹ å™¨
    """
    print("æ‰§è¡ŒStackingé›†æˆ...")
    
    # å¯¹æ•°å˜æ¢
    y_train_log = np.log1p(y_train)
    
    # åˆ›å»ºå…ƒç‰¹å¾
    meta_features_train = np.column_stack([
        # ä½¿ç”¨äº¤å‰éªŒè¯è·å¾—è®­ç»ƒé›†çš„å…ƒç‰¹å¾
        np.zeros(len(X_train)),  # LightGBMé¢„æµ‹
        np.zeros(len(X_train)),  # XGBoosté¢„æµ‹
        np.zeros(len(X_train))   # CatBoosté¢„æµ‹
    ])
    
    meta_features_test = np.column_stack([lgb_pred, xgb_pred, cat_pred])
    
    # 5æŠ˜äº¤å‰éªŒè¯ç”Ÿæˆè®­ç»ƒé›†å…ƒç‰¹å¾
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]  # type: ignore
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]  # type: ignore
        
        # è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨ç®€åŒ–å‚æ•°ä»¥èŠ‚çœæ—¶é—´ï¼‰
        lgb_model = lgb.LGBMRegressor(objective='mae', num_leaves=31, max_depth=6, 
                                    learning_rate=0.1, random_state=42, n_estimators=200)
        lgb_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 0] = np.expm1(lgb_model.predict(X_val))
        
        xgb_model = xgb.XGBRegressor(objective='reg:absoluteerror', max_depth=6, 
                                    learning_rate=0.1, random_state=42, n_estimators=200)
        xgb_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 1] = np.expm1(xgb_model.predict(X_val))  # type: ignore
        
        cat_model = CatBoostRegressor(loss_function='MAE', depth=6, learning_rate=0.1, 
                                    random_seed=42, iterations=200, verbose=False)
        cat_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 2] = np.expm1(cat_model.predict(X_val))  # type: ignore
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨
    meta_learner = LinearRegression()
    meta_learner.fit(meta_features_train, y_train)
    
    # é¢„æµ‹æµ‹è¯•é›†
    stacking_pred = meta_learner.predict(meta_features_test)
    
    print(f"Stackingæƒé‡: {meta_learner.coef_}")
    print(f"Stackingæˆªè·: {meta_learner.intercept_}")
    
    return stacking_pred

def advanced_ensemble_and_calibration(lgb_pred: np.ndarray, xgb_pred: np.ndarray, cat_pred: np.ndarray, stacking_pred: np.ndarray, y_train: pd.Series, scores_info: Dict[str, List[float]]) -> np.ndarray:
    """
    é«˜çº§é›†æˆå’Œæ™ºèƒ½æ ¡å‡†
    """
    print("æ‰§è¡Œé«˜çº§é›†æˆå’Œæ™ºèƒ½æ ¡å‡†...")
    
    # 1. åŸºäºæ€§èƒ½çš„è‡ªé€‚åº”æƒé‡
    lgb_score = np.mean(scores_info['lgb_scores'])
    xgb_score = np.mean(scores_info['xgb_scores'])
    cat_score = np.mean(scores_info['cat_scores'])
    
    # è¯„ä¼°Stackingæ€§èƒ½ï¼ˆä½¿ç”¨äº¤å‰éªŒè¯ä¼°è®¡ï¼‰
    # ç”±äºstacking_predæ˜¯æµ‹è¯•é›†é¢„æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¶ä»–æ¨¡å‹çš„å¹³å‡æ€§èƒ½ä½œä¸ºä¼°è®¡
    stacking_score = (lgb_score + xgb_score + cat_score) / 3
    
    # è®¡ç®—æƒé‡
    models_scores = {
        'lgb': lgb_score,
        'xgb': xgb_score,
        'cat': cat_score,
        'stacking': stacking_score
    }
    
    # æ€§èƒ½è¶Šå¥½æƒé‡è¶Šå¤§
    total_inv_score = sum(1/score for score in models_scores.values())
    weights = {model: (1/score) / total_inv_score for model, score in models_scores.items()}
    
    print(f"\næ¨¡å‹æƒé‡:")
    for model, weight in weights.items():
        print(f"  {model}: {weight:.3f}")
    
    # 2. å¤šå±‚é›†æˆ
    models_preds = {
        'lgb': lgb_pred,
        'xgb': xgb_pred,
        'cat': cat_pred,
        'stacking': stacking_pred
    }
    
    # åŠ æƒå¹³å‡
    weighted_ensemble = sum(weights[model] * pred for model, pred in models_preds.items())
    
    # 3. æ™ºèƒ½æ ¡å‡†
    # åˆ†ä½æ•°æ ¡å‡†
    train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])
    pred_quantiles = np.percentile(weighted_ensemble, [10, 25, 50, 75, 90])
    
    # åˆ›å»ºæ ¡å‡†æ˜ å°„
    calibration_map = {}
    for i in range(len(train_quantiles)):
        if pred_quantiles[i] > 0:
            calibration_map[pred_quantiles[i]] = train_quantiles[i]
    
    # åº”ç”¨æ ¡å‡†
    calibrated_pred = np.copy(weighted_ensemble)
    for i, pred_val in enumerate(weighted_ensemble):  # type: ignore[arg-type]
        # æ‰¾åˆ°æœ€è¿‘çš„åˆ†ä½ç‚¹è¿›è¡Œæ ¡å‡†
        closest_quantile = pred_quantiles[np.argmin(np.abs(pred_quantiles - pred_val))]
        if closest_quantile in calibration_map:
            calibration_factor = calibration_map[closest_quantile] / closest_quantile
            calibrated_pred[i] *= calibration_factor
    
    # 4. æœ€ç»ˆçº¦æŸæ£€æŸ¥
    calibrated_pred = np.maximum(calibrated_pred, 0)  # ç¡®ä¿éè´Ÿ
    
    # åŸºäºè®­ç»ƒé›†åˆ†å¸ƒçš„æœ€ç»ˆè°ƒæ•´
    train_mean, train_std = y_train.mean(), y_train.std()
    pred_mean, pred_std = calibrated_pred.mean(), calibrated_pred.std()
    
    # è°ƒæ•´å‡å€¼å’Œæ ‡å‡†å·®
    if pred_std > 0:
        calibrated_pred = (calibrated_pred - pred_mean) * (train_std / pred_std) + train_mean
    
    calibrated_pred = np.maximum(calibrated_pred, 0)
    
    print(f"\næ ¡å‡†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: å‡å€¼={train_mean:.2f}, æ ‡å‡†å·®={train_std:.2f}")
    print(f"  æ ¡å‡†å‰: å‡å€¼={pred_mean:.2f}, æ ‡å‡†å·®={pred_std:.2f}")
    print(f"  æ ¡å‡†å: å‡å€¼={calibrated_pred.mean():.2f}, æ ‡å‡†å·®={calibrated_pred.std():.2f}")
    
    return calibrated_pred

def create_comprehensive_analysis_plots(y_train: pd.Series, predictions: np.ndarray, scores_info: Dict[str, List[float]], model_name: str = "modeling_v17") -> None:
    """
    åˆ›å»ºå…¨é¢çš„åˆ†æå›¾è¡¨
    """
    print("ç”Ÿæˆå…¨é¢åˆ†æå›¾è¡¨...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    analysis_dir = get_user_data_path()
    os.makedirs(analysis_dir, exist_ok=True)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    
    # 1. ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”
    axes[0, 0].hist(y_train, bins=50, alpha=0.7, label='è®­ç»ƒé›†çœŸå®ä»·æ ¼', color='blue', density=True)
    axes[0, 0].hist(predictions, bins=50, alpha=0.7, label='æµ‹è¯•é›†é¢„æµ‹ä»·æ ¼', color='red', density=True)
    axes[0, 0].set_xlabel('ä»·æ ¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”')
    axes[0, 0].legend()
    
    # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    models = ['LightGBM', 'XGBoost', 'CatBoost']
    scores = [np.mean(scores_info['lgb_scores']), 
              np.mean(scores_info['xgb_scores']), 
              np.mean(scores_info['cat_scores'])]
    
    axes[0, 1].bar(models, scores, color=['lightblue', 'lightgreen', 'lightcoral'])
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].set_title('å„æ¨¡å‹éªŒè¯æ€§èƒ½')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. Q-Qå›¾æ£€æŸ¥åˆ†å¸ƒ
    from scipy import stats
    stats.probplot(predictions, dist="norm", plot=axes[0, 2])
    axes[0, 2].set_title('é¢„æµ‹å€¼Q-Qå›¾')
    
    # 4. é¢„æµ‹å€¼åˆ†å¸ƒç›´æ–¹å›¾
    axes[1, 0].hist(predictions, bins=50, alpha=0.7, color='red')
    axes[1, 0].set_xlabel('é¢„æµ‹ä»·æ ¼')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_title('é¢„æµ‹å€¼åˆ†å¸ƒ')
    
    # 5. ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
    sorted_pred = np.sort(predictions)
    cumulative = np.arange(1, len(sorted_pred) + 1) / len(sorted_pred)
    axes[1, 1].plot(sorted_pred, cumulative)
    axes[1, 1].set_xlabel('é¢„æµ‹ä»·æ ¼')
    axes[1, 1].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
    axes[1, 1].set_title('é¢„æµ‹å€¼ç´¯ç§¯åˆ†å¸ƒ')
    
    # 6. ä»·æ ¼åŒºé—´åˆ†æ
    price_bins = [0, 5000, 10000, 20000, 50000, 100000, float('inf')]
    price_labels = ['<5K', '5K-10K', '10K-20K', '20K-50K', '50K-100K', '>100K']
    pred_categories = pd.cut(predictions, bins=price_bins, labels=price_labels)
    category_counts = pred_categories.value_counts().sort_index()  # type: ignore[attr-defined]
    
    axes[1, 2].bar(category_counts.index, category_counts.values)
    axes[1, 2].set_xlabel('ä»·æ ¼åŒºé—´')
    axes[1, 2].set_ylabel('è½¦è¾†æ•°é‡')
    axes[1, 2].set_title('é¢„æµ‹ä»·æ ¼åŒºé—´åˆ†å¸ƒ')
    axes[1, 2].tick_params(axis='x', rotation=45)
    
    # 7. è®­ç»ƒé›†vsé¢„æµ‹é›†ç»Ÿè®¡å¯¹æ¯”
    train_stats = [y_train.mean(), y_train.std(), y_train.min(), y_train.max()]
    pred_stats = [predictions.mean(), predictions.std(), predictions.min(), predictions.max()]
    stats_labels = ['å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼']
    
    x = np.arange(len(stats_labels))
    width = 0.35
    
    axes[2, 0].bar(x - width/2, train_stats, width, label='è®­ç»ƒé›†', color='blue', alpha=0.7)
    axes[2, 0].bar(x + width/2, pred_stats, width, label='é¢„æµ‹é›†', color='red', alpha=0.7)
    axes[2, 0].set_xlabel('ç»Ÿè®¡æŒ‡æ ‡')
    axes[2, 0].set_ylabel('å€¼')
    axes[2, 0].set_title('ç»Ÿè®¡æŒ‡æ ‡å¯¹æ¯”')
    axes[2, 0].set_xticks(x)
    axes[2, 0].set_xticklabels(stats_labels)
    axes[2, 0].legend()
    
    # 8. é¢„æµ‹å€¼ç®±çº¿å›¾
    axes[2, 1].boxplot(predictions)
    axes[2, 1].set_ylabel('é¢„æµ‹ä»·æ ¼')
    axes[2, 1].set_title('é¢„æµ‹å€¼ç®±çº¿å›¾')
    
    # 9. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    stats_text = f"""
    modeling_v17ç‰ˆæœ¬è¯¦ç»†ç»Ÿè®¡:
    
    è®­ç»ƒé›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(y_train):,}
    å‡å€¼: {y_train.mean():.2f}
    æ ‡å‡†å·®: {y_train.std():.2f}
    ä¸­ä½æ•°: {y_train.median():.2f}
    èŒƒå›´: {y_train.min():.2f} - {y_train.max():.2f}
    
    é¢„æµ‹é›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(predictions):,}
    å‡å€¼: {predictions.mean():.2f}
    æ ‡å‡†å·®: {predictions.std():.2f}
    ä¸­ä½æ•°: {np.median(predictions):.2f}
    èŒƒå›´: {predictions.min():.2f} - {predictions.max():.2f}
    
    æ¨¡å‹æ€§èƒ½:
    LightGBM: {np.mean(scores_info["lgb_scores"]):.2f}
    XGBoost: {np.mean(scores_info["xgb_scores"]):.2f}
    CatBoost: {np.mean(scores_info["cat_scores"]):.2f}
    """
    axes[2, 2].text(0.05, 0.95, stats_text, transform=axes[2, 2].transAxes, 
                    fontsize=9, verticalalignment='top', fontfamily='monospace')
    axes[2, 2].set_title('è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯')
    axes[2, 2].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_path = os.path.join(analysis_dir, f'{model_name}_comprehensive_analysis.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    plt.show()

def v17_optimize_model() -> Tuple[np.ndarray, Dict[str, List[float]]]:
    """
    V17ä¼˜åŒ–æ¨¡å‹è®­ç»ƒæµç¨‹ - å†²å‡»500åˆ†ç›®æ ‡
    """
    print("=" * 80)
    print("å¼€å§‹V17ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ - å†²å‡»500åˆ†ç›®æ ‡")
    print("=" * 80)
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("æ­¥éª¤1: é«˜çº§æ•°æ®åŠ è½½å’Œé¢„å¤„ç†...")
    train_df, test_df = load_and_preprocess_data()
    
    print("æ­¥éª¤2: é«˜çº§ç‰¹å¾å·¥ç¨‹...")
    train_df = create_advanced_features(train_df)
    test_df = create_advanced_features(test_df)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # ç‰¹å¾é€‰æ‹© - åŸºäºé‡è¦æ€§é€‰æ‹©
    print("\næ­¥éª¤3: ç‰¹å¾é€‰æ‹©...")
    selector = SelectFromModel(estimator=RandomForestRegressor(n_estimators=100, random_state=42), 
                             threshold='median')
    selector.fit(X_train, y_train)
    
    selected_features = X_train.columns[selector.get_support()].tolist()
    print(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}")
    
    X_train_selected: pd.DataFrame = X_train[selected_features].copy()  # type: ignore[assignment]
    X_test_selected: pd.DataFrame = X_test[selected_features].copy()  # type: ignore[assignment]
    
    # ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤4: ç‰¹å¾ç¼©æ”¾...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            # æ£€æŸ¥å’Œå¤„ç†æ•°å€¼é—®é¢˜
            inf_mask = np.isinf(X_train_selected[col]) | np.isinf(X_test_selected[col])
            if inf_mask.any():  # type: ignore[func-returns-value]
                X_train_selected.loc[inf_mask[inf_mask.index.isin(X_train_selected.index)].index, col] = 0  # type: ignore[index]
                X_test_selected.loc[inf_mask[inf_mask.index.isin(X_test_selected.index)].index, col] = 0  # type: ignore[index]
            
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())  # type: ignore[assignment]
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())  # type: ignore[assignment]
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # è¶…å‚æ•°è°ƒä¼˜
    print("\næ­¥éª¤5: è¶…å‚æ•°ç²¾ç»†è°ƒä¼˜...")
    best_params = optimize_hyperparameters(X_train_selected, y_train)  # type: ignore[arg-type]
    
    # ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹
    print("\næ­¥éª¤6: ä½¿ç”¨ä¼˜åŒ–å‚æ•°è®­ç»ƒæ¨¡å‹...")
    lgb_pred, xgb_pred, cat_pred, scores_info = train_models_with_optimized_params(
        X_train_selected, y_train, X_test_selected, best_params)  # type: ignore[arg-type]
    
    # Stackingé›†æˆ
    print("\næ­¥éª¤7: Stackingé›†æˆ...")
    stacking_pred = stacking_ensemble(X_train_selected, y_train, X_test_selected, 
                                    lgb_pred, xgb_pred, cat_pred)  # type: ignore[arg-type]
    
    # é«˜çº§é›†æˆå’Œæ ¡å‡†
    print("\næ­¥éª¤8: é«˜çº§é›†æˆå’Œæ™ºèƒ½æ ¡å‡†...")
    final_predictions = advanced_ensemble_and_calibration(
        lgb_pred, xgb_pred, cat_pred, stacking_pred, y_train, scores_info)  # type: ignore[arg-type]
    
    # åˆ›å»ºå…¨é¢åˆ†æå›¾è¡¨
    print("\næ­¥éª¤9: ç”Ÿæˆåˆ†æå›¾è¡¨...")
    create_comprehensive_analysis_plots(y_train, final_predictions, scores_info, "modeling_v17")  # type: ignore[arg-type]
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\næœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    print(f"ä¸­ä½æ•°: {np.median(final_predictions):.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v17_result_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("V17ä¼˜åŒ–æ€»ç»“ - å†²å‡»500åˆ†ç›®æ ‡")
    print("=" * 80)
    print("âœ… é«˜çº§æ•°æ®é¢„å¤„ç† - ç²¾ç¡®å¼‚å¸¸å€¼å¤„ç†å’Œæ™ºèƒ½ç¼ºå¤±å€¼å¡«å……")
    print("âœ… é«˜çº§ç‰¹å¾å·¥ç¨‹ - ä¸šåŠ¡é€»è¾‘ç‰¹å¾å’Œäº¤äº’ç‰¹å¾")
    print("âœ… ç‰¹å¾é€‰æ‹© - åŸºäºé‡è¦æ€§çš„æ™ºèƒ½ç‰¹å¾ç­›é€‰")
    print("âœ… è¶…å‚æ•°ç²¾ç»†è°ƒä¼˜ - ç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³é…ç½®")
    print("âœ… Stackingé›†æˆç­–ç•¥ - çº¿æ€§å›å½’å…ƒå­¦ä¹ å™¨")
    print("âœ… é«˜çº§é›†æˆå’Œæ ¡å‡† - å¤šå±‚é›†æˆå’Œåˆ†ä½æ•°æ ¡å‡†")
    print("âœ… å…¨é¢åˆ†æå›¾è¡¨ - æ·±å…¥ç†è§£æ¨¡å‹æ€§èƒ½")
    print("=" * 80)
    
    return final_predictions, scores_info

if __name__ == "__main__":
    test_pred, scores_info = v17_optimize_model()
    print("V17ä¼˜åŒ–å®Œæˆ! æœŸå¾…çªç ´500åˆ†ç›®æ ‡! ğŸ¯")
