# -*- coding: utf-8 -*-
"""
éšæœºæ£®æ—å®Œæ•´è®­ç»ƒçŠ¶æ€åˆ†æè„šæœ¬
ç”Ÿæˆ5ä¸ªæ ¸å¿ƒè¯Šæ–­å›¾è¡¨ï¼šå­¦ä¹ æ›²çº¿ã€æ”¶æ•›åˆ†æã€ç‰¹å¾é‡è¦æ€§ã€å‚æ•°éªŒè¯ã€æ®‹å·®åˆ†æ
åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¾¾åˆ°æœ€ä½³è®­ç»ƒçŠ¶æ€
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import (learning_curve, validation_curve, cross_val_score, 
                                   cross_val_predict, KFold)
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_palette("husl")

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    return os.path.join(project_root, *paths)

def load_and_preprocess_data():
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® - åŸºäºé¡¹ç›®è§„èŒƒ"""
    print("ğŸ”„ åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    train_data = pd.read_csv(train_path, sep=' ')
    print(f"åŸå§‹æ•°æ®: {train_data.shape}")
    
    # å¤„ç†å¼‚å¸¸å€¼ - powerå­—æ®µ
    if 'power' in train_data.columns:
        power_outliers = (train_data['power'] > 600).sum()
        train_data.loc[train_data['power'] > 600, 'power'] = 600
        print(f"ä¿®æ­£ {power_outliers} ä¸ªpowerå¼‚å¸¸å€¼")
    
    # å¤„ç†ç¼ºå¤±å€¼
    numeric_cols = train_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != 'price':
            train_data[col] = train_data[col].fillna(train_data[col].median())
    
    # åˆ†ç±»ç‰¹å¾ç¼–ç 
    categorical_cols = ['brand', 'model', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage']
    for col in categorical_cols:
        if col in train_data.columns:
            train_data[col] = train_data[col].fillna('unknown')
            le = LabelEncoder()
            train_data[col] = le.fit_transform(train_data[col].astype(str))
    
    # ä»·æ ¼å¼‚å¸¸å€¼å¤„ç† - ä½¿ç”¨ä¿å®ˆç­–ç•¥
    price_q005 = train_data['price'].quantile(0.005)
    price_q995 = train_data['price'].quantile(0.995)
    valid_idx = (train_data['price'] >= price_q005) & (train_data['price'] <= price_q995)
    removed_count = len(train_data) - valid_idx.sum()
    train_data = train_data[valid_idx].reset_index(drop=True)
    print(f"ç§»é™¤ {removed_count} ä¸ªä»·æ ¼å¼‚å¸¸æ ·æœ¬")
    
    # åˆ›å»ºè½¦é¾„ç‰¹å¾
    if 'regDate' in train_data.columns:
        current_year = 2020
        train_data['reg_year'] = train_data['regDate'] // 10000
        train_data['car_age'] = current_year - train_data['reg_year']
        train_data['car_age'] = np.maximum(train_data['car_age'], 1)
        
        # å¹´å‡é‡Œç¨‹æ•°ç‰¹å¾
        if 'kilometer' in train_data.columns:
            train_data['km_per_year'] = train_data['kilometer'] / train_data['car_age']
    
    # å‡†å¤‡å»ºæ¨¡æ•°æ®
    feature_cols = [col for col in train_data.columns if col != 'price']
    X = train_data[feature_cols]
    y = train_data['price']
    
    print(f"é¢„å¤„ç†å®Œæˆ: X={X.shape}, y={y.shape}")
    return X, y

def plot_learning_curve_analysis(X, y, ax):
    """ğŸ” å­¦ä¹ æ›²çº¿åˆ†æ - åˆ¤æ–­è®­ç»ƒ/éªŒè¯Gap"""
    print("1ï¸âƒ£ ç”Ÿæˆå­¦ä¹ æ›²çº¿...")
    
    # æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ - åŸºäºè®°å¿†è§„èŒƒ
    rf_model = RandomForestRegressor(
        n_estimators=150,  # æ§åˆ¶åœ¨300ä»¥å†…
        max_depth=18,      # æ§åˆ¶åœ¨20ä»¥å†…
        min_samples_split=12,
        min_samples_leaf=6,
        random_state=42,
        n_jobs=-1
    )
    
    # è®¡ç®—å­¦ä¹ æ›²çº¿
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_sizes, train_scores, val_scores = learning_curve(
        rf_model, X, y, 
        cv=3,  # åŠ å¿«è®¡ç®—é€Ÿåº¦
        train_sizes=train_sizes,
        scoring='neg_mean_absolute_error',
        n_jobs=-1
    )
    
    # è½¬æ¢ä¸ºMAE
    train_mae = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mae = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    # ç»˜åˆ¶æ›²çº¿
    ax.plot(train_sizes, train_mae, 'o-', color='#2E86AB', linewidth=2.5, 
            label='è®­ç»ƒé›†MAE', markersize=6)
    ax.fill_between(train_sizes, train_mae - train_std, train_mae + train_std, 
                    alpha=0.2, color='#2E86AB')
    
    ax.plot(train_sizes, val_mae, 'o-', color='#F24236', linewidth=2.5, 
            label='éªŒè¯é›†MAE', markersize=6)
    ax.fill_between(train_sizes, val_mae - val_std, val_mae + val_std, 
                    alpha=0.2, color='#F24236')
    
    # æ ·å¼è®¾ç½®
    ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°é‡', fontsize=11, fontweight='bold')
    ax.set_ylabel('MAE (å¹³å‡ç»å¯¹è¯¯å·®)', fontsize=11, fontweight='bold')
    ax.set_title('ğŸ” å­¦ä¹ æ›²çº¿åˆ†æ - è®­ç»ƒvséªŒè¯Gap', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # åˆ†æç»“æœ
    final_gap = val_mae[-1] - train_mae[-1]
    final_train_mae = train_mae[-1]
    final_val_mae = val_mae[-1]
    
    # æ·»åŠ åˆ†ææ–‡æœ¬
    status_color = '#28a745' if final_gap < 80 else '#ffc107' if final_gap < 150 else '#dc3545'
    status_text = 'âœ… è‰¯å¥½' if final_gap < 80 else 'âš–ï¸ é€‚ä¸­' if final_gap < 150 else 'âš ï¸ è¿‡æ‹Ÿåˆ'
    
    ax.text(0.05, 0.95, f'Gap: {final_gap:.1f}', transform=ax.transAxes, 
            fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
    ax.text(0.05, 0.88, f'çŠ¶æ€: {status_text}', transform=ax.transAxes, 
            fontsize=10, color=status_color, fontweight='bold')
    
    return final_train_mae, final_val_mae, final_gap

def plot_convergence_analysis(X, y, ax):
    """ğŸŒ³ æ”¶æ•›åˆ†æ - æ¨¡æ‹Ÿè¿­ä»£è¿‡ç¨‹"""
    print("2ï¸âƒ£ ç”Ÿæˆæ”¶æ•›åˆ†æ...")
    
    # æ ‘æ•°é‡èŒƒå›´ - æ§åˆ¶åœ¨300ä»¥å†…
    n_estimators_range = [10, 25, 50, 75, 100, 125, 150, 200, 250, 300]
    
    train_scores = []
    val_scores = []
    
    for n_est in n_estimators_range:
        rf = RandomForestRegressor(
            n_estimators=n_est,
            max_depth=18,
            min_samples_split=12,
            min_samples_leaf=6,
            random_state=42,
            n_jobs=-1
        )
        
        # äº¤å‰éªŒè¯å¾—åˆ†
        cv_scores = cross_val_score(rf, X, y, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)
        val_scores.append(-cv_scores.mean())
        
        # è®­ç»ƒé›†å¾—åˆ†
        rf.fit(X, y)
        train_pred = rf.predict(X)
        train_mae = mean_absolute_error(y, train_pred)
        train_scores.append(train_mae)
    
    # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
    ax.plot(n_estimators_range, train_scores, 'o-', color='#2E86AB', 
            linewidth=2.5, label='è®­ç»ƒé›†MAE', markersize=6)
    ax.plot(n_estimators_range, val_scores, 'o-', color='#F24236', 
            linewidth=2.5, label='éªŒè¯é›†MAE', markersize=6)
    
    # æ‰¾åˆ°æœ€ä½³ç‚¹
    best_idx = np.argmin(val_scores)
    best_n_est = n_estimators_range[best_idx]
    best_val_mae = val_scores[best_idx]
    
    # æ ‡è®°æœ€ä½³ç‚¹
    ax.axvline(x=best_n_est, color='#28a745', linestyle='--', alpha=0.7, linewidth=2)
    ax.text(best_n_est + 15, best_val_mae + 10, f'æœ€ä½³: {best_n_est}æ£µæ ‘', 
            fontsize=9, color='#28a745', fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
    
    ax.set_xlabel('æ ‘çš„æ•°é‡ (æ¨¡æ‹Ÿè®­ç»ƒè¿­ä»£)', fontsize=11, fontweight='bold')
    ax.set_ylabel('MAE', fontsize=11, fontweight='bold')
    ax.set_title('ğŸŒ³ æ”¶æ•›åˆ†æ - æ‰¾åˆ°æœ€ä½³æ ‘æ•°é‡', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    return best_n_est, best_val_mae, val_scores

def plot_feature_importance(X, y, ax):
    """ğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ"""
    print("3ï¸âƒ£ ç”Ÿæˆç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # è®­ç»ƒæ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
    rf = RandomForestRegressor(n_estimators=150, max_depth=18, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    
    # åˆ›å»ºé‡è¦æ€§æ•°æ®æ¡†
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=True).tail(15)  # å–å‰15ä¸ª
    
    # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
    bars = ax.barh(range(len(importance_df)), importance_df['importance'], 
                   color=plt.cm.viridis(np.linspace(0, 1, len(importance_df))))
    
    ax.set_yticks(range(len(importance_df)))
    ax.set_yticklabels(importance_df['feature'])
    ax.set_xlabel('ç‰¹å¾é‡è¦æ€§', fontsize=11, fontweight='bold')
    ax.set_title('ğŸ“Š Top15 é‡è¦ç‰¹å¾åˆ†æ', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                f'{width:.3f}', ha='left', va='center', fontsize=8)
    
    return importance_df

def plot_parameter_validation(X, y, ax):
    """âš–ï¸ å‚æ•°éªŒè¯æ›²çº¿ - æ‰¾æœ€ä¼˜å‚æ•°"""
    print("4ï¸âƒ£ ç”Ÿæˆå‚æ•°éªŒè¯åˆ†æ...")
    
    # æµ‹è¯•max_depthå‚æ•°
    depth_range = [8, 12, 15, 18, 20, 25, 30]
    
    train_scores, val_scores = validation_curve(
        RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        X, y, param_name='max_depth', param_range=depth_range,
        cv=3, scoring='neg_mean_absolute_error', n_jobs=-1
    )
    
    train_mae = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mae = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    # ç»˜åˆ¶éªŒè¯æ›²çº¿
    ax.plot(depth_range, train_mae, 'o-', color='#2E86AB', 
            linewidth=2.5, label='è®­ç»ƒé›†MAE', markersize=6)
    ax.fill_between(depth_range, train_mae - train_std, train_mae + train_std, 
                    alpha=0.2, color='#2E86AB')
    
    ax.plot(depth_range, val_mae, 'o-', color='#F24236', 
            linewidth=2.5, label='éªŒè¯é›†MAE', markersize=6)
    ax.fill_between(depth_range, val_mae - val_std, val_mae + val_std, 
                    alpha=0.2, color='#F24236')
    
    # æ‰¾åˆ°æœ€ä½³æ·±åº¦
    best_idx = np.argmin(val_mae)
    best_depth = depth_range[best_idx]
    best_mae = val_mae[best_idx]
    
    ax.axvline(x=best_depth, color='#28a745', linestyle='--', alpha=0.7, linewidth=2)
    ax.text(best_depth + 1, best_mae + 5, f'æœ€ä½³æ·±åº¦: {best_depth}', 
            fontsize=9, color='#28a745', fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
    
    ax.set_xlabel('æœ€å¤§æ·±åº¦', fontsize=11, fontweight='bold')
    ax.set_ylabel('MAE', fontsize=11, fontweight='bold')
    ax.set_title('âš–ï¸ å‚æ•°éªŒè¯ - æœ€ä¼˜æ·±åº¦æœç´¢', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    return best_depth, best_mae

def plot_residual_analysis(X, y, ax):
    """ğŸ“ˆ æ®‹å·®åˆ†æ - é¢„æµ‹è´¨é‡æ£€æŸ¥"""
    print("5ï¸âƒ£ ç”Ÿæˆæ®‹å·®åˆ†æ...")
    
    # ä½¿ç”¨äº¤å‰éªŒè¯é¢„æµ‹
    rf = RandomForestRegressor(n_estimators=150, max_depth=18, random_state=42, n_jobs=-1)
    y_pred = cross_val_predict(rf, X, y, cv=3)
    
    # è®¡ç®—æ®‹å·®
    residuals = y - y_pred
    
    # ç»˜åˆ¶æ®‹å·®vsé¢„æµ‹å€¼æ•£ç‚¹å›¾
    ax.scatter(y_pred, residuals, alpha=0.5, s=20, color='#A663CC')
    ax.axhline(y=0, color='#F24236', linestyle='--', alpha=0.8, linewidth=2)
    
    ax.set_xlabel('é¢„æµ‹å€¼', fontsize=11, fontweight='bold')
    ax.set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)', fontsize=11, fontweight='bold')
    ax.set_title('ğŸ“ˆ æ®‹å·®åˆ†æ - é¢„æµ‹è´¨é‡æ£€æŸ¥', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2 = r2_score(y, y_pred)
    
    # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    metrics_text = f'MAE: {mae:.1f}\nRMSE: {rmse:.1f}\nRÂ²: {r2:.3f}'
    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes, fontsize=10, 
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8),
            verticalalignment='top')
    
    return mae, rmse, r2, residuals

def generate_analysis_report(results):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ¯ éšæœºæ£®æ—è®­ç»ƒçŠ¶æ€åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # è§£æç»“æœ
    final_train_mae, final_val_mae, gap = results['learning_curve']
    best_n_est, best_val_mae, _ = results['convergence']
    importance_df = results['feature_importance']
    best_depth, _ = results['parameter_validation']
    mae, rmse, r2, _ = results['residual_analysis']
    
    print(f"\n1ï¸âƒ£ ğŸ“ˆ å­¦ä¹ æ›²çº¿åˆ†æ:")
    print(f"   è®­ç»ƒMAE: {final_train_mae:.1f}")
    print(f"   éªŒè¯MAE: {final_val_mae:.1f}")
    print(f"   Gap: {gap:.1f}")
    
    if gap > 150:
        print("   âŒ æ¨¡å‹è¿‡æ‹Ÿåˆä¸¥é‡ï¼å»ºè®®:")
        print("      â€¢ å¢åŠ min_samples_split (15â†’20)")
        print("      â€¢ å‡å°‘max_depth (18â†’15)")
        print("      â€¢ å¢åŠ min_samples_leaf")
    elif gap > 80:
        print("   âš ï¸  å­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¾®è°ƒå‚æ•°")
    else:
        print("   âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
    
    print(f"\n2ï¸âƒ£ ğŸŒ³ æ”¶æ•›åˆ†æ:")
    print(f"   æœ€ä½³æ ‘æ•°é‡: {best_n_est}")
    print(f"   æœ€ä½³éªŒè¯MAE: {best_val_mae:.1f}")
    
    if best_n_est >= 250:
        print("   ğŸ’¡ å¯è€ƒè™‘å¢åŠ æ›´å¤šæ ‘æ¥è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("   âœ… æ ‘æ•°é‡å·²åŸºæœ¬æ”¶æ•›")
    
    print(f"\n3ï¸âƒ£ ğŸ“Š ç‰¹å¾é‡è¦æ€§:")
    print("   Top5é‡è¦ç‰¹å¾:")
    for _, row in importance_df.tail(5).iloc[::-1].iterrows():
        print(f"   â€¢ {row['feature']}: {row['importance']:.4f}")
    
    print(f"\n4ï¸âƒ£ âš–ï¸  å‚æ•°ä¼˜åŒ–:")
    print(f"   å»ºè®®æœ€å¤§æ·±åº¦: {best_depth}")
    
    print(f"\n5ï¸âƒ£ ğŸ“ˆ æ¨¡å‹æ€§èƒ½:")
    print(f"   MAE: {mae:.1f}")
    print(f"   RMSE: {rmse:.1f}")
    print(f"   RÂ²: {r2:.3f}")
    
    # æ€»ä½“å»ºè®®
    print(f"\nğŸ¯ æ€»ä½“å»ºè®®:")
    if gap > 100 and mae > 600:
        print("  âŒ ä¼˜å…ˆçº§: è§£å†³è¿‡æ‹Ÿåˆ > æå‡æ€§èƒ½")
        print("  ğŸ“ å…·ä½“è¡ŒåŠ¨: å¢åŠ æ­£åˆ™åŒ–ï¼Œå‡å°‘æ¨¡å‹å¤æ‚åº¦")
    elif mae > 600:
        print("  ğŸš€ ä¼˜å…ˆçº§: æå‡æ¨¡å‹æ€§èƒ½")
        print("  ğŸ“ å…·ä½“è¡ŒåŠ¨: ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ï¼Œå°è¯•å‚æ•°è°ƒä¼˜")
    else:
        print("  âœ… æ¨¡å‹çŠ¶æ€è‰¯å¥½ï¼Œå¯è¿›è¡Œæœ€åçš„å¾®è°ƒ")
        print("  ğŸ“ å…·ä½“è¡ŒåŠ¨: ä¿å­˜æ¨¡å‹ï¼Œå‡†å¤‡é›†æˆ")

def main():
    """ä¸»å‡½æ•° - ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒçŠ¶æ€åˆ†æ"""
    print("ğŸ¯ å¼€å§‹éšæœºæ£®æ—å®Œæ•´è®­ç»ƒçŠ¶æ€åˆ†æ")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    X, y = load_and_preprocess_data()
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    results_dir = get_project_path('prediction_result')
    os.makedirs(results_dir, exist_ok=True)
    
    # åˆ›å»ºå¤§å›¾ - 2x3å¸ƒå±€
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ğŸ¯ éšæœºæ£®æ—è®­ç»ƒçŠ¶æ€å®Œæ•´åˆ†æ', fontsize=16, fontweight='bold', y=0.95)
    
    # å­˜å‚¨åˆ†æç»“æœ
    results = {}
    
    # 1. å­¦ä¹ æ›²çº¿åˆ†æ
    results['learning_curve'] = plot_learning_curve_analysis(X, y, axes[0, 0])
    
    # 2. æ”¶æ•›åˆ†æ
    results['convergence'] = plot_convergence_analysis(X, y, axes[0, 1])
    
    # 3. ç‰¹å¾é‡è¦æ€§
    results['feature_importance'] = plot_feature_importance(X, y, axes[0, 2])
    
    # 4. å‚æ•°éªŒè¯
    results['parameter_validation'] = plot_parameter_validation(X, y, axes[1, 0])
    
    # 5. æ®‹å·®åˆ†æ
    results['residual_analysis'] = plot_residual_analysis(X, y, axes[1, 1])
    
    # 6. æ·»åŠ æ€»ç»“æ–‡æœ¬
    axes[1, 2].axis('off')
    summary_text = """
    ğŸ¯ è®­ç»ƒçŠ¶æ€åˆ¤æ–­æ ‡å‡†
    
    âœ… æ¨¡å‹å·²è¾¾æœ€ä½³çŠ¶æ€:
    â€¢ è®­ç»ƒ/éªŒè¯Gap < 80
    â€¢ éªŒè¯æ›²çº¿è¶‹äºå¹³ç¼“
    â€¢ æ®‹å·®éšæœºåˆ†å¸ƒ
    
    âš ï¸ éœ€è¦è°ƒä¼˜:
    â€¢ Gap 80-150: å¾®è°ƒå‚æ•°
    â€¢ Gap > 150: è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ­£åˆ™åŒ–
    
    ğŸš€ å¯ç»§ç»­ä¼˜åŒ–:
    â€¢ éªŒè¯æ€§èƒ½è¿˜åœ¨æå‡
    â€¢ æ”¶æ•›æ›²çº¿æœªè¾¾å¹³å°æœŸ
    â€¢ ç‰¹å¾å·¥ç¨‹æœ‰æ”¹è¿›ç©ºé—´
    
    ğŸ’¡ ç±»ä¼¼æ·±åº¦å­¦ä¹ çš„æ—©åœ:
    è§‚å¯ŸéªŒè¯MAEæ˜¯å¦åˆ°è¾¾æœ€ä½ç‚¹
    """
    
    axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, 
                    fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.7))
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(results_dir, f'rf_complete_analysis_{timestamp}.png')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å®Œæ•´åˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    plt.show()
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_analysis_report(results)
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼å·²ç”Ÿæˆ5ä¸ªæ ¸å¿ƒè¯Šæ–­å›¾è¡¨")
    print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®: {save_path}")
    
    return results

if __name__ == "__main__":
    analysis_results = main()