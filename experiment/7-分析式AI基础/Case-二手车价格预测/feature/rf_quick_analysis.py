# -*- coding: utf-8 -*-
"""
éšæœºæ£®æ—å¿«é€Ÿåˆ†æ - è½»é‡ç‰ˆæœ¬
å¿«é€Ÿç”Ÿæˆæ ¸å¿ƒè¯Šæ–­å›¾è¡¨ï¼Œåˆ¤æ–­è®­ç»ƒçŠ¶æ€
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    return os.path.join(project_root, *paths)

def load_data_fast():
    """å¿«é€ŸåŠ è½½æ•°æ® - ä½¿ç”¨é‡‡æ ·å‡å°‘è®¡ç®—é‡"""
    print("ğŸ”„ å¿«é€ŸåŠ è½½æ•°æ®...")
    
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    train_data = pd.read_csv(train_path, sep=' ')
    
    # éšæœºé‡‡æ ·å‡å°‘æ•°æ®é‡ï¼ŒåŠ å¿«åˆ†æé€Ÿåº¦
    sample_size = min(20000, len(train_data))  # æœ€å¤šä½¿ç”¨2ä¸‡æ ·æœ¬
    train_data = train_data.sample(n=sample_size, random_state=42)
    print(f"é‡‡æ ·æ•°æ®: {train_data.shape}")
    
    # å¿«é€Ÿé¢„å¤„ç†
    numeric_cols = train_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != 'price':
            train_data[col] = train_data[col].fillna(train_data[col].median())
    
    # åˆ†ç±»ç‰¹å¾ç®€å•å¤„ç†
    categorical_cols = ['brand', 'model', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage']
    for col in categorical_cols:
        if col in train_data.columns:
            train_data[col] = train_data[col].fillna(0)
            # ç®€å•çš„æ ‡ç­¾ç¼–ç 
            train_data[col] = pd.Categorical(train_data[col]).codes
    
    # å¤„ç†å¼‚å¸¸å€¼
    if 'power' in train_data.columns:
        train_data.loc[train_data['power'] > 600, 'power'] = 600
    
    # ä»·æ ¼å¼‚å¸¸å€¼
    train_data = train_data[(train_data['price'] >= 500) & (train_data['price'] <= 50000)]
    
    # ç®€å•è½¦é¾„ç‰¹å¾
    if 'regDate' in train_data.columns:
        train_data['car_age'] = 2020 - (train_data['regDate'] // 10000)
        train_data['car_age'] = np.clip(train_data['car_age'], 1, 30)
    
    feature_cols = [col for col in train_data.columns if col != 'price']
    X = train_data[feature_cols]
    y = train_data['price']
    
    print(f"é¢„å¤„ç†å®Œæˆ: {X.shape}")
    return X, y

def analyze_rf_performance(X, y):
    """å¿«é€Ÿåˆ†æéšæœºæ£®æ—æ€§èƒ½"""
    print("ğŸ¯ å¿«é€Ÿæ€§èƒ½åˆ†æ...")
    
    results = {}
    
    # 1. ç®€å•çš„å­¦ä¹ æ›²çº¿ - ä¸åŒè®­ç»ƒé›†å¤§å°
    print("1ï¸âƒ£ å­¦ä¹ æ›²çº¿åˆ†æ...")
    train_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]
    train_mae_list = []
    val_mae_list = []
    
    for size in train_sizes:
        n_samples = int(len(X) * size)
        X_subset = X.iloc[:n_samples]
        y_subset = y.iloc[:n_samples]
        
        # ç®€å•æ¨¡å‹
        rf = RandomForestRegressor(n_estimators=50, max_depth=15, random_state=42, n_jobs=2)
        
        # è®­ç»ƒé›†æ€§èƒ½
        rf.fit(X_subset, y_subset)
        train_pred = rf.predict(X_subset)
        train_mae = mean_absolute_error(y_subset, train_pred)
        train_mae_list.append(train_mae)
        
        # éªŒè¯é›†æ€§èƒ½ï¼ˆäº¤å‰éªŒè¯ï¼‰
        val_scores = cross_val_score(rf, X_subset, y_subset, cv=3, 
                                   scoring='neg_mean_absolute_error', n_jobs=2)
        val_mae = -val_scores.mean()
        val_mae_list.append(val_mae)
    
    results['learning_curve'] = (train_sizes, train_mae_list, val_mae_list)
    
    # 2. æ”¶æ•›åˆ†æ - ä¸åŒæ ‘æ•°é‡
    print("2ï¸âƒ£ æ”¶æ•›åˆ†æ...")
    n_estimators_range = [10, 25, 50, 75, 100, 150]
    convergence_scores = []
    
    for n_est in n_estimators_range:
        rf = RandomForestRegressor(n_estimators=n_est, max_depth=15, random_state=42, n_jobs=2)
        scores = cross_val_score(rf, X, y, cv=3, scoring='neg_mean_absolute_error', n_jobs=2)
        convergence_scores.append(-scores.mean())
    
    results['convergence'] = (n_estimators_range, convergence_scores)
    
    # 3. ç‰¹å¾é‡è¦æ€§
    print("3ï¸âƒ£ ç‰¹å¾é‡è¦æ€§...")
    rf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=2)
    rf.fit(X, y)
    
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    results['feature_importance'] = importance_df
    
    # 4. å‚æ•°å¯¹æ¯”
    print("4ï¸âƒ£ å‚æ•°å¯¹æ¯”...")
    depth_range = [8, 12, 15, 18, 20]
    depth_scores = []
    
    for depth in depth_range:
        rf = RandomForestRegressor(n_estimators=50, max_depth=depth, random_state=42, n_jobs=2)
        scores = cross_val_score(rf, X, y, cv=3, scoring='neg_mean_absolute_error', n_jobs=2)
        depth_scores.append(-scores.mean())
    
    results['parameter_analysis'] = (depth_range, depth_scores)
    
    return results

def create_analysis_plots(results):
    """åˆ›å»ºåˆ†æå›¾è¡¨"""
    print("ğŸ“Š ç”Ÿæˆåˆ†æå›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('éšæœºæ£®æ—è®­ç»ƒçŠ¶æ€å¿«é€Ÿåˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. å­¦ä¹ æ›²çº¿
    train_sizes, train_mae_list, val_mae_list = results['learning_curve']
    
    axes[0,0].plot([int(s*20000) for s in train_sizes], train_mae_list, 'o-', 
                   label='è®­ç»ƒé›†MAE', color='blue', linewidth=2)
    axes[0,0].plot([int(s*20000) for s in train_sizes], val_mae_list, 'o-', 
                   label='éªŒè¯é›†MAE', color='red', linewidth=2)
    axes[0,0].set_title('ğŸ” å­¦ä¹ æ›²çº¿ - è®­ç»ƒvséªŒè¯Gap')
    axes[0,0].set_xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    axes[0,0].set_ylabel('MAE')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # è®¡ç®—Gap
    final_gap = val_mae_list[-1] - train_mae_list[-1]
    gap_status = "âœ…è‰¯å¥½" if final_gap < 80 else "âš ï¸æ³¨æ„" if final_gap < 150 else "âŒè¿‡æ‹Ÿåˆ"
    axes[0,0].text(0.05, 0.95, f'Gap: {final_gap:.1f}\nçŠ¶æ€: {gap_status}', 
                   transform=axes[0,0].transAxes, fontsize=10,
                   bbox=dict(boxstyle="round", facecolor="lightblue", alpha=0.7),
                   verticalalignment='top')
    
    # 2. æ”¶æ•›åˆ†æ
    n_estimators_range, convergence_scores = results['convergence']
    
    axes[0,1].plot(n_estimators_range, convergence_scores, 'o-', color='green', linewidth=2)
    axes[0,1].set_title('ğŸŒ³ æ”¶æ•›åˆ†æ - æ ‘æ•°é‡ä¼˜åŒ–')
    axes[0,1].set_xlabel('æ ‘æ•°é‡')
    axes[0,1].set_ylabel('éªŒè¯é›†MAE')
    axes[0,1].grid(True, alpha=0.3)
    
    # æ ‡è®°æœ€ä½³ç‚¹
    best_idx = np.argmin(convergence_scores)
    best_n_est = n_estimators_range[best_idx]
    best_mae = convergence_scores[best_idx]
    axes[0,1].axvline(x=best_n_est, color='red', linestyle='--', alpha=0.7)
    axes[0,1].text(best_n_est + 5, best_mae + 10, f'æœ€ä½³: {best_n_est}æ£µæ ‘', 
                   fontsize=9, color='red', fontweight='bold')
    
    # 3. ç‰¹å¾é‡è¦æ€§ Top10
    importance_df = results['feature_importance']
    top_features = importance_df.head(10)
    
    axes[1,0].barh(range(len(top_features)), top_features['importance'][::-1], 
                   color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))
    axes[1,0].set_yticks(range(len(top_features)))
    axes[1,0].set_yticklabels(top_features['feature'][::-1])
    axes[1,0].set_title('ğŸ“Š Top10 é‡è¦ç‰¹å¾')
    axes[1,0].set_xlabel('é‡è¦æ€§')
    
    # 4. å‚æ•°åˆ†æ
    depth_range, depth_scores = results['parameter_analysis']
    
    axes[1,1].plot(depth_range, depth_scores, 'o-', color='orange', linewidth=2)
    axes[1,1].set_title('âš–ï¸ æ·±åº¦å‚æ•°åˆ†æ')
    axes[1,1].set_xlabel('max_depth')
    axes[1,1].set_ylabel('éªŒè¯é›†MAE')
    axes[1,1].grid(True, alpha=0.3)
    
    # æ ‡è®°æœ€ä½³æ·±åº¦
    best_depth_idx = np.argmin(depth_scores)
    best_depth = depth_range[best_depth_idx]
    axes[1,1].axvline(x=best_depth, color='green', linestyle='--', alpha=0.7)
    axes[1,1].text(best_depth + 0.5, min(depth_scores) + 5, f'æœ€ä½³: {best_depth}', 
                   fontsize=9, color='green', fontweight='bold')
    
    return fig, final_gap, best_n_est, best_depth

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ éšæœºæ£®æ—å¿«é€Ÿè®­ç»ƒçŠ¶æ€åˆ†æ")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    X, y = load_data_fast()
    
    # åˆ†ææ€§èƒ½
    results = analyze_rf_performance(X, y)
    
    # åˆ›å»ºå›¾è¡¨
    fig, final_gap, best_n_est, best_depth = create_analysis_plots(results)
    
    # ä¿å­˜å›¾è¡¨
    results_dir = get_project_path('prediction_result')
    os.makedirs(results_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(results_dir, f'rf_quick_analysis_{timestamp}.png')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å¿«é€Ÿåˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()
    
    # ç”Ÿæˆç®€è¦æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ¯ å¿«é€Ÿåˆ†ææŠ¥å‘Š")
    print("="*50)
    
    print(f"ğŸ“ˆ å­¦ä¹ æ›²çº¿åˆ†æ:")
    print(f"   è®­ç»ƒ/éªŒè¯Gap: {final_gap:.1f}")
    if final_gap > 150:
        print("   âŒ è¿‡æ‹Ÿåˆä¸¥é‡ï¼Œéœ€è¦å¢åŠ æ­£åˆ™åŒ–")
    elif final_gap > 80:
        print("   âš ï¸ è½»å¾®è¿‡æ‹Ÿåˆï¼Œå»ºè®®è°ƒå‚")
    else:
        print("   âœ… æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
    
    print(f"\nğŸŒ³ æ”¶æ•›åˆ†æ:")
    print(f"   å»ºè®®æ ‘æ•°é‡: {best_n_est}")
    if best_n_est >= 100:
        print("   ğŸ’¡ å¯è€ƒè™‘å¢åŠ æ›´å¤šæ ‘")
    else:
        print("   âœ… æ ‘æ•°é‡å·²åŸºæœ¬å¤Ÿç”¨")
    
    print(f"\nâš–ï¸ å‚æ•°ä¼˜åŒ–:")
    print(f"   å»ºè®®æœ€å¤§æ·±åº¦: {best_depth}")
    
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§:")
    importance_df = results['feature_importance']
    print("   Top3ç‰¹å¾:")
    for _, row in importance_df.head(3).iterrows():
        print(f"   â€¢ {row['feature']}: {row['importance']:.4f}")
    
    print(f"\nğŸ¯ æ€»ä½“å»ºè®®:")
    if final_gap > 100:
        print("  ä¼˜å…ˆè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜:")
        print("  â€¢ å¢åŠ min_samples_split (10â†’15)")
        print("  â€¢ å¢åŠ min_samples_leaf (5â†’8)")
        print(f"  â€¢ è°ƒæ•´max_depthåˆ°{max(best_depth-2, 10)}")
    else:
        print("  æ¨¡å‹çŠ¶æ€è‰¯å¥½ï¼Œå¯è¿›è¡Œå¾®è°ƒ:")
        print(f"  â€¢ ä½¿ç”¨{best_n_est}æ£µæ ‘")
        print(f"  â€¢ è®¾ç½®max_depth={best_depth}")
        print("  â€¢ å¯å°è¯•ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–")
    
    print(f"\nğŸ’¡ è¿™äº›å›¾è¡¨æä¾›äº†ç±»ä¼¼æ·±åº¦å­¦ä¹ 'è®­ç»ƒæŸå¤±vséªŒè¯æŸå¤±'çš„åˆ¤æ–­ä¾æ®")
    print(f"âœ… å¯ä»¥æ®æ­¤åˆ¤æ–­éšæœºæ£®æ—æ˜¯å¦è¾¾åˆ°æœ€ä½³è®­ç»ƒçŠ¶æ€")
    
    return save_path

if __name__ == "__main__":
    result_path = main()