# -*- coding: utf-8 -*-
"""
é’ˆå¯¹æ€§ä¼˜åŒ–è„šæœ¬ - åŸºäºæ·±åº¦åˆ†æç»“æœ
ç›®æ ‡: å°†MAEä»698é™åˆ°500ä»¥å†…
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.model_selection import cross_val_score, KFold, learning_curve, validation_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import warnings
import joblib

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)

        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths):
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

def save_models(models, version_name):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°modelç›®å½•
    
    Parameters:
    -----------
    models : dict
        æ¨¡å‹å­—å…¸ï¼Œkeyä¸ºæ¨¡å‹åç§°ï¼Œvalueä¸ºæ¨¡å‹å¯¹è±¡
    version_name : str
        ç‰ˆæœ¬åç§°ï¼Œå¦‚'rf_baseline'
    """
    model_dir = get_project_path('model')
    os.makedirs(model_dir, exist_ok=True)
    
    saved_files = []
    for model_name, model_obj in models.items():
        if model_obj is not None:
            model_file = os.path.join(model_dir, f'{version_name}_{model_name}_model.pkl')
            joblib.dump(model_obj, model_file)
            saved_files.append(model_file)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_file}")
    
    return saved_files

def advanced_missing_value_handler(train_data, test_data):
    """
    é«˜çº§ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ - ä¸“ä¸ºéšæœºæ£®æ—ä¼˜åŒ–

    ç­–ç•¥1: åˆ†ç»„å¡«å…… - æ ¹æ®ç›¸å…³ç‰¹å¾åˆ†ç»„è®¡ç®—ç»Ÿè®¡å€¼
    ç­–ç•¥2: ç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡ - ä¸ºé«˜ç¼ºå¤±ç‡ç‰¹å¾åˆ›å»ºæŒ‡ç¤ºå˜é‡
    ç­–ç•¥3: å¤šé‡æ’å€¼ - å¯¹å…³é”®ç‰¹å¾ä½¿ç”¨å¤šç§æ–¹æ³•
    ç­–ç•¥4: ä¸šåŠ¡é€»è¾‘å¡«å…… - åŸºäºé¢†åŸŸçŸ¥è¯†çš„å¡«å……

    Args:
        train_data: è®­ç»ƒæ•°æ®é›†
        test_data: æµ‹è¯•æ•°æ®é›†
    
    Returns:
        å¤„ç†åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    """
    print("\n=== é«˜çº§ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ ===")

    # åˆ›å»ºæ•°æ®å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    train_enhanced = train_data.copy()
    test_enhanced = test_data.copy()

    missing_report = {}

    # ç­–ç•¥1: å…³é”®æ•°å€¼ç‰¹å¾çš„æ™ºèƒ½åˆ†ç»„å¡«å……
    key_features_config = {
        'power': {
            'group_by': ['brand', 'bodyType'],  # æŒ‰å“ç‰Œå’Œè½¦èº«ç±»å‹åˆ†ç»„
            'fallback_groups': ['brand'],  # å¦‚æœç»„åˆåˆ†ç»„æ ·æœ¬ä¸è¶³ï¼Œå›é€€åˆ°å•ä¸€åˆ†ç»„
            'method': 'median',
            'create_missing_indicator': True
        },
        'kilometer': {
            'group_by': ['brand', 'car_age'] if 'car_age' in train_data.columns else ['brand'],
            'fallback_groups': ['brand'],
            'method': 'median',
            'create_missing_indicator': True
        }
    }

    for feature, config in key_features_config.items():
        if feature in train_enhanced.columns:
            missing_count = train_enhanced[feature].isnull().sum()
            if missing_count > 0:
                print(f"\nå¤„ç†å…³é”®ç‰¹å¾ {feature} ({missing_count} ä¸ªç¼ºå¤±å€¼)...")

                # åˆ›å»ºç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡
                if config['create_missing_indicator']:
                    train_enhanced[f'{feature}_was_missing'] = train_enhanced[feature].isnull().astype(int)
                    if feature in test_enhanced.columns:
                        test_enhanced[f'{feature}_was_missing'] = test_enhanced[feature].isnull().astype(int)

                # æ™ºèƒ½åˆ†ç»„å¡«å……
                filled_count = 0

                # å°è¯•ä¸»è¦åˆ†ç»„ç­–ç•¥
                if all(col in train_enhanced.columns for col in config['group_by']):
                    group_stats = train_enhanced.groupby(config['group_by'])[feature].agg(['median', 'count'])

                    for group_key, group_data in group_stats.iterrows():
                        if group_data['count'] >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰ç”¨åˆ†ç»„ç»Ÿè®¡
                            fill_value = group_data['median']

                            # æ„å»ºç­›é€‰æ¡ä»¶
                            if len(config['group_by']) == 2:
                                mask_train = (train_enhanced[config['group_by'][0]] == group_key[0]) & \
                                           (train_enhanced[config['group_by'][1]] == group_key[1]) & \
                                           train_enhanced[feature].isnull()
                                mask_test = (test_enhanced[config['group_by'][0]] == group_key[0]) & \
                                          (test_enhanced[config['group_by'][1]] == group_key[1]) & \
                                          test_enhanced[feature].isnull()
                            else:
                                mask_train = (train_enhanced[config['group_by'][0]] == group_key) & \
                                           train_enhanced[feature].isnull()
                                mask_test = (test_enhanced[config['group_by'][0]] == group_key) & \
                                          test_enhanced[feature].isnull()

                            # å¡«å……
                            count_filled = mask_train.sum()
                            if count_filled > 0:
                                train_enhanced.loc[mask_train, feature] = fill_value
                                filled_count += count_filled
                            
                            if feature in test_enhanced.columns:
                                test_enhanced.loc[mask_test, feature] = fill_value
                
                # å¤„ç†å‰©ä½™ç¼ºå¤±å€¼ - ä½¿ç”¨å›é€€ç­–ç•¥
                remaining_missing = train_enhanced[feature].isnull().sum()
                if remaining_missing > 0:
                    print(f"  ä½¿ç”¨å›é€€ç­–ç•¥å¤„ç†å‰©ä½™ {remaining_missing} ä¸ªç¼ºå¤±å€¼...")
                    global_median = train_enhanced[feature].median()
                    train_enhanced[feature] = train_enhanced[feature].fillna(global_median)
                    if feature in test_enhanced.columns:
                        test_enhanced[feature] = test_enhanced[feature].fillna(global_median)
                
                missing_report[feature] = {
                    'original_missing': missing_count,
                    'group_filled': filled_count,
                    'global_filled': remaining_missing,
                    'strategy': 'æ™ºèƒ½åˆ†ç»„å¡«å……+ç¼ºå¤±æŒ‡ç¤º'
                }

    # ç­–ç•¥2: åˆ†ç±»ç‰¹å¾çš„é«˜çº§å¤„ç†
    categorical_features = ['fuelType', 'gearbox', 'bodyType', 'model']

    for feature in categorical_features:
        if feature in train_enhanced.columns:
            missing_count = train_enhanced[feature].isnull().sum()
            missing_rate = missing_count / len(train_enhanced)

            if missing_count > 0:
                print(f"\nå¤„ç†åˆ†ç±»ç‰¹å¾ {feature} ({missing_count} ä¸ªç¼ºå¤±å€¼, {missing_rate:.2%})...")

                # é«˜ç¼ºå¤±ç‡ç‰¹å¾åˆ›å»ºæŒ‡ç¤ºå˜é‡
                if missing_rate >= 0.03:  # 3%ä»¥ä¸Šåˆ›å»ºæŒ‡ç¤ºå˜é‡
                    train_enhanced[f'{feature}_was_missing'] = train_enhanced[feature].isnull().astype(int)
                    if feature in test_enhanced.columns:
                        test_enhanced[f'{feature}_was_missing'] = test_enhanced[feature].isnull().astype(int)
                    strategy = 'ä¼—æ•°å¡«å……+ç¼ºå¤±æŒ‡ç¤º'
                else:
                    strategy = 'ä¼—æ•°å¡«å……'

                # ä¼—æ•°å¡«å……
                if len(train_enhanced[feature].mode()) > 0:
                    mode_val = train_enhanced[feature].mode().iloc[0]
                else:
                    mode_val = 'unknown'

                train_enhanced[feature] = train_enhanced[feature].fillna(mode_val)
                if feature in test_enhanced.columns:
                    test_enhanced[feature] = test_enhanced[feature].fillna(mode_val)

                missing_report[feature] = {
                    'original_missing': missing_count,
                    'missing_rate': missing_rate,
                    'strategy': strategy
                }

    # ç­–ç•¥3: å…¶ä»–æ•°å€¼ç‰¹å¾çš„æ ‡å‡†å¤„ç†
    numeric_cols = train_enhanced.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != 'price' and col not in key_features_config and not col.endswith('_missing') and not col.endswith('_was_missing'):
            missing_count = train_enhanced[col].isnull().sum()
            if missing_count > 0:
                median_val = train_enhanced[col].median()
                train_enhanced[col] = train_enhanced[col].fillna(median_val)
                if col in test_enhanced.columns:
                    test_enhanced[col] = test_enhanced[col].fillna(median_val)
                
                missing_report[col] = {
                    'original_missing': missing_count,
                    'strategy': 'ä¸­ä½æ•°å¡«å……'
                }
    
    # è¾“å‡ºå¤„ç†æŠ¥å‘Š
    print("\n=== ç¼ºå¤±å€¼å¤„ç†æŠ¥å‘Š ===")
    for feature, report in missing_report.items():
        print(f"{feature}: {report['original_missing']} ä¸ªç¼ºå¤±å€¼ - {report['strategy']}")
    
    print(f"\nå¤„ç†å®Œæˆ! åˆ›å»ºäº† {sum(1 for col in train_enhanced.columns if col.endswith('_was_missing'))} ä¸ªç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡")
    
    return train_enhanced, test_enhanced

def load_and_optimize_data():
    """åŸºäºåˆ†æç»“æœä¼˜åŒ–æ•°æ®åŠ è½½ - æ·»åŠ é«˜çº§ç¼ºå¤±å€¼å¤„ç†"""
    print("æ­£åœ¨åŠ è½½å¹¶ä¼˜åŒ–æ•°æ®...")

    # åŠ è½½åŸå§‹æ•°æ® - ä½¿ç”¨ç»å¯¹è·¯å¾„
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')

    print(f"è®­ç»ƒæ•°æ®è·¯å¾„: {train_path}")
    print(f"æµ‹è¯•æ•°æ®è·¯å¾„: {test_path}")

    try:
        train_raw = pd.read_csv(train_path, sep=' ')
        test_raw = pd.read_csv(test_path, sep=' ')
    except Exception as e:
        print(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
        print("å°è¯•ä½¿ç”¨é€—å·åˆ†éš”ç¬¦...")
        train_raw = pd.read_csv(train_path)
        test_raw = pd.read_csv(test_path)

    print(f"åŸå§‹è®­ç»ƒé›†: {train_raw.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_raw.shape}")

    # ç¡®ä¿ç‰¹å¾å®Œå…¨ä¸€è‡´
    common_features = set(train_raw.columns) & set(test_raw.columns)
    feature_cols = [col for col in common_features if col != 'price']

    train_data = train_raw[feature_cols + ['price']].copy()
    test_data = test_raw[feature_cols].copy()

    # 1. æŒ‰è§„èŒƒå¤„ç†powerå¼‚å¸¸å€¼ (å‘ç°143ä¸ª>600çš„è®°å½•)
    print("å¤„ç†powerå¼‚å¸¸å€¼...")
    power_outliers_train = 0
    power_outliers_test = 0

    if 'power' in train_data.columns:
        power_outliers_train = (train_data['power'] > 600).sum()
        train_data.loc[train_data['power'] > 600, 'power'] = 600
        print(f"è®­ç»ƒé›†ä¿®æ­£äº† {power_outliers_train} ä¸ªpowerå¼‚å¸¸å€¼")

    if 'power' in test_data.columns:
        power_outliers_test = (test_data['power'] > 600).sum()
        test_data.loc[test_data['power'] > 600, 'power'] = 600
        print(f"æµ‹è¯•é›†ä¿®æ­£äº† {power_outliers_test} ä¸ªpowerå¼‚å¸¸å€¼")

    # 2. é«˜çº§ç¼ºå¤±å€¼å¤„ç† - æ›¿æ¢åŸæœ‰çš„ç®€å•å¤„ç†
    train_data, test_data = advanced_missing_value_handler(train_data, test_data)

    # 2. ä¼˜åŒ–ç¼ºå¤±å€¼å¤„ç† - åˆ†ç»„å¡«å…… + ç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡
    print("ä¼˜åŒ–ç¼ºå¤±å€¼å¤„ç†...")

    # åˆ›å»ºç¼ºå¤±å€¼ç»Ÿè®¡
    missing_stats = {}

    # ä¼˜å…ˆå¤„ç†å…³é”®æ•°å€¼ç‰¹å¾ - ä½¿ç”¨åˆ†ç»„å¡«å……
    key_numeric_features = ['power', 'kilometer']
    for col in key_numeric_features:
        if col in train_data.columns:
            missing_count_train = train_data[col].isnull().sum()
            missing_count_test = test_data[col].isnull().sum() if col in test_data.columns else 0

            if missing_count_train > 0 or missing_count_test > 0:
                print(f"  å¤„ç† {col} çš„ {missing_count_train + missing_count_test} ä¸ªç¼ºå¤±å€¼...")

                # åˆ›å»ºç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡
                train_data[f'{col}_missing'] = train_data[col].isnull().astype(int)
                if col in test_data.columns:
                    test_data[f'{col}_missing'] = test_data[col].isnull().astype(int)

                # æŒ‰å“ç‰Œåˆ†ç»„å¡«å……ï¼ˆå¦‚æœbrandå¯ç”¨ï¼‰
                if 'brand' in train_data.columns and train_data[col].isnull().sum() > 0:
                    # è®¡ç®—æ¯ä¸ªå“ç‰Œçš„ä¸­ä½æ•°
                    brand_medians = train_data.groupby('brand')[col].median().to_dict()

                    # å¡«å……è®­ç»ƒé›†
                    for brand, median_val in brand_medians.items():
                        mask = (train_data['brand'] == brand) & train_data[col].isnull()
                        train_data.loc[mask, col] = median_val

                    # å¤„ç†å‰©ä½™çš„ç¼ºå¤±å€¼ï¼ˆç”¨å…¨å±€ä¸­ä½æ•°ï¼‰
                    global_median = train_data[col].median()
                    train_data[col] = train_data[col].fillna(global_median)

                    # å¡«å……æµ‹è¯•é›†
                    if col in test_data.columns:
                        for brand, median_val in brand_medians.items():
                            mask = (test_data['brand'] == brand) & test_data[col].isnull()
                            test_data.loc[mask, col] = median_val
                        test_data[col] = test_data[col].fillna(global_median)
                else:
                    # å›é€€åˆ°ç®€å•ä¸­ä½æ•°å¡«å……
                    median_val = train_data[col].median()
                    train_data[col] = train_data[col].fillna(median_val)
                    if col in test_data.columns:
                        test_data[col] = test_data[col].fillna(median_val)

                missing_stats[col] = {'train': missing_count_train, 'test': missing_count_test, 'method': 'åˆ†ç»„ä¸­ä½æ•°+æŒ‡ç¤ºå˜é‡'}

    # å¤„ç†å…¶ä»–æ•°å€¼ç‰¹å¾ - ç®€å•ä¸­ä½æ•°å¡«å……
    numeric_cols = train_data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col != 'price' and col not in key_numeric_features and not col.endswith('_missing'):
            missing_count_train = train_data[col].isnull().sum()
            missing_count_test = test_data[col].isnull().sum() if col in test_data.columns else 0

            if missing_count_train > 0 or missing_count_test > 0:
                median_val = train_data[col].median()
                train_data[col] = train_data[col].fillna(median_val)
                if col in test_data.columns:
                    test_data[col] = test_data[col].fillna(median_val)
                missing_stats[col] = {'train': missing_count_train, 'test': missing_count_test, 'method': 'ä¸­ä½æ•°å¡«å……'}

    # ä¼˜åŒ–åˆ†ç±»ç‰¹å¾å¤„ç† - åˆ›å»ºé«˜ç¼ºå¤±ç‡ç‰¹å¾çš„æŒ‡ç¤ºå˜é‡
    categorical_cols = train_data.select_dtypes(exclude=[np.number]).columns
    high_missing_threshold = 0.05  # 5%ä»¥ä¸Šç¼ºå¤±ç‡åˆ›å»ºæŒ‡ç¤ºå˜é‡

    for col in categorical_cols:
        if col != 'price':
            missing_count_train = train_data[col].isnull().sum()
            missing_count_test = test_data[col].isnull().sum() if col in test_data.columns else 0
            missing_rate = missing_count_train / len(train_data)

            if missing_count_train > 0 or missing_count_test > 0:
                # é«˜ç¼ºå¤±ç‡ç‰¹å¾åˆ›å»ºæŒ‡ç¤ºå˜é‡
                if missing_rate >= high_missing_threshold:
                    print(f"  ä¸º {col} åˆ›å»ºç¼ºå¤±å€¼æŒ‡ç¤ºå˜é‡ (ç¼ºå¤±ç‡: {missing_rate:.2%})")
                    train_data[f'{col}_missing'] = train_data[col].isnull().astype(int)
                    if col in test_data.columns:
                        test_data[f'{col}_missing'] = test_data[col].isnull().astype(int)
                    method = 'ä¼—æ•°å¡«å……+æŒ‡ç¤ºå˜é‡'
                else:
                    method = 'ä¼—æ•°å¡«å……'

                # ä¼—æ•°å¡«å……
                if len(train_data[col].mode()) > 0:
                    mode_val = train_data[col].mode().iloc[0]
                else:
                    mode_val = 'unknown'

                train_data[col] = train_data[col].fillna(mode_val)
                if col in test_data.columns:
                    test_data[col] = test_data[col].fillna(mode_val)

                missing_stats[col] = {'train': missing_count_train, 'test': missing_count_test, 'method': method}

    # è¾“å‡ºç¼ºå¤±å€¼å¤„ç†ç»Ÿè®¡
    if missing_stats:
        print("\nç¼ºå¤±å€¼å¤„ç†ç»Ÿè®¡:")
        for col, stats in missing_stats.items():
            print(f"  {col}: è®­ç»ƒé›†{stats['train']}ä¸ª, æµ‹è¯•é›†{stats['test']}ä¸ª - {stats['method']}")
    else:
        print("  æ— ç¼ºå¤±å€¼éœ€è¦å¤„ç†")
    # 3. åˆ†ç±»ç‰¹å¾ç¼–ç ä¼˜åŒ–
    categorical_features = ['brand', 'model', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage']

    for col in categorical_features:
        if col in train_data.columns and col in test_data.columns:
            # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•é›†è¿›è¡Œç»Ÿä¸€ç¼–ç ï¼Œå¤„ç†è®­ç»ƒé›†ç‹¬æœ‰å€¼çš„é—®é¢˜
            all_values = pd.concat([
                train_data[col].astype(str), 
                test_data[col].astype(str)
            ]).unique()

            le = LabelEncoder()
            le.fit(all_values)

            train_data[col] = le.transform(train_data[col].astype(str))
            test_data[col] = le.transform(test_data[col].astype(str))

    # 4. ä¿å®ˆçš„ä»·æ ¼å¼‚å¸¸å€¼å¤„ç†ï¼ˆé¿å…è¿‡åº¦åˆ é™¤é«˜ä»·æ ·æœ¬ï¼‰
    if 'price' in train_data.columns:
        # ä½¿ç”¨æ›´å®½æ¾çš„0.5%-99.5%èŒƒå›´
        price_q005 = train_data['price'].quantile(0.005)
        price_q995 = train_data['price'].quantile(0.995)

        valid_idx = (train_data['price'] >= price_q005) & (train_data['price'] <= price_q995)
        removed_count = len(train_data) - valid_idx.sum()
        train_data = train_data[valid_idx].reset_index(drop=True)

        print(f"ä»·æ ¼èŒƒå›´: {train_data['price'].min():.2f} - {train_data['price'].max():.2f}")
        print(f"ç§»é™¤äº† {removed_count} ä¸ªä»·æ ¼å¼‚å¸¸æ ·æœ¬ ({removed_count/len(train_raw)*100:.2f}%)")

    print(f"ä¼˜åŒ–åè®­ç»ƒé›†: {train_data.shape}")
    print(f"ä¼˜åŒ–åæµ‹è¯•é›†: {test_data.shape}")

    return train_data, test_data

def create_targeted_features(train_data, test_data):
    """åŸºäºç‰¹å¾é‡è¦æ€§åˆ†æåˆ›å»ºé’ˆå¯¹æ€§ç‰¹å¾"""
    print("åˆ›å»ºé’ˆå¯¹æ€§ç‰¹å¾...")
    
    # 5. æ­£ç¡®å¤„ç†regDate - æŒ‰è§„èŒƒæå–å¹´ä»½è®¡ç®—è½¦é¾„
    if 'regDate' in train_data.columns:
        print("æ­£ç¡®å¤„ç†regDateæ—¶é—´ç‰¹å¾...")

        current_year = datetime.now().year
        
        # æå–æ³¨å†Œå¹´ä»½
        train_data['reg_year'] = train_data['regDate'] // 10000
        test_data['reg_year'] = test_data['regDate'] // 10000

        # æå–ä¸Šçº¿å¹´ä»½
        train_data['create_year'] = train_data['creatDate'] // 10000
        test_data['create_year'] = test_data['creatDate'] // 10000
        
        # è®¡ç®—è½¦é¾„
        train_data['car_age'] = current_year - train_data['reg_year']
        test_data['car_age'] = current_year - test_data['reg_year']
        
        # ç¡®ä¿è½¦é¾„ä¸ºæ­£æ•°
        train_data['car_age'] = np.maximum(train_data['car_age'], 1)
        test_data['car_age'] = np.maximum(test_data['car_age'], 1)
        
        # æå–æœˆä»½ï¼ˆå­£èŠ‚æ€§ç‰¹å¾ï¼‰
        train_data['reg_month'] = (train_data['regDate'] % 10000) // 100
        test_data['reg_month'] = (test_data['regDate'] % 10000) // 100
        
        # è½¦é¾„åˆ†æ¡£ï¼ˆåŸºäºä¸šåŠ¡ç†è§£ï¼‰
        age_bins = [0, 3, 7, 12, 20, 50]
        train_data['age_group'] = pd.Series(pd.cut(train_data['car_age'], bins=age_bins, labels=False)).fillna(4).astype(int)
        test_data['age_group'] = pd.Series(pd.cut(test_data['car_age'], bins=age_bins, labels=False)).fillna(4).astype(int)

        train_data.drop(columns=['name', 'offerType', 'seller', 'regDate', 'creatDate'], inplace=True)
        test_data.drop(columns=['name', 'offerType', 'seller', 'regDate', 'creatDate'], inplace=True)
    
    # 6. åŸºäºé‡è¦ç‰¹å¾åˆ›å»ºäº¤äº’ç‰¹å¾
    if 'kilometer' in train_data.columns and 'car_age' in train_data.columns:
        # å¹´å‡é‡Œç¨‹æ•° (åˆ†æä¸­å‘ç°è¿™ä¸ªç‰¹å¾æœ‰æ•ˆ)
        train_data['km_per_year'] = train_data['kilometer'] / train_data['car_age']
        test_data['km_per_year'] = test_data['kilometer'] / test_data['car_age']
        
        # é‡Œç¨‹ä½¿ç”¨å¼ºåº¦åˆ†ç±»
        km_year_bins = [0, 8000, 18000, 35000, np.inf]
        train_data['usage_intensity'] = pd.Series(pd.cut(train_data['km_per_year'], bins=km_year_bins, labels=False)).fillna(0).astype(int)
        test_data['usage_intensity'] = pd.Series(pd.cut(test_data['km_per_year'], bins=km_year_bins, labels=False)).fillna(0).astype(int)

    # 7. åŠŸç‡æ•ˆç‡ç‰¹å¾ (åˆ†æä¸­æ’åç¬¬5)
    if 'power' in train_data.columns and 'kilometer' in train_data.columns:
        train_data['power_efficiency'] = train_data['power'] / (train_data['kilometer'] + 1)
        test_data['power_efficiency'] = test_data['power'] / (test_data['kilometer'] + 1)
        
        # åŠŸç‡åˆ†æ¡£
        power_bins = [0, 75, 110, 150, 200, 600]
        train_data['power_level'] = pd.Series(pd.cut(train_data['power'], bins=power_bins, labels=False)).fillna(0).astype(int)
        test_data['power_level'] = pd.Series(pd.cut(test_data['power'], bins=power_bins, labels=False)).fillna(0).astype(int)
    
    # 8. åŸºäºTopç‰¹å¾çš„äº¤äº’ (v_0, v_12, v_3æ˜¯æœ€é‡è¦çš„)
    important_features = ['v_0', 'v_12', 'v_3']
    for feat in important_features:
        if feat in train_data.columns:
            # ä¸è½¦é¾„çš„äº¤äº’
            if 'car_age' in train_data.columns:
                train_data[f'{feat}_age_ratio'] = train_data[feat] / (train_data['car_age'] + 1)
                test_data[f'{feat}_age_ratio'] = test_data[feat] / (test_data['car_age'] + 1)
    
    # 9. ç»„åˆç‰¹å¾ - åŸºäºä¸šåŠ¡ç†è§£
    if 'v_0' in train_data.columns and 'v_12' in train_data.columns:
        train_data['v0_v12_combo'] = train_data['v_0'] * train_data['v_12']
        test_data['v0_v12_combo'] = test_data['v_0'] * test_data['v_12']
    
    # 10. æ–°å¢é‡è¦ç‰¹å¾ç»„åˆï¼ˆåŸºäº727åˆ†æ•°è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
    if 'v_0' in train_data.columns and 'v_3' in train_data.columns:
        train_data['v0_v3_interaction'] = train_data['v_0'] * train_data['v_3']
        test_data['v0_v3_interaction'] = test_data['v_0'] * test_data['v_3']
    
    # 11. åŸºäºè½¦è¾†ä½¿ç”¨çŠ¶å†µçš„ç»¼åˆè¯„åˆ†
    if 'kilometer' in train_data.columns and 'car_age' in train_data.columns and 'power' in train_data.columns:
        # ç»¼åˆè½¦è¾†çŠ¶å†µè¯„åˆ†ï¼šé‡Œç¨‹ + è½¦é¾„ + åŠŸç‡çš„ç»¼åˆè€ƒè™‘
        train_data['vehicle_condition'] = (
            (train_data['power'] / 100) / 
            (np.log1p(train_data['kilometer'] / 1000) * np.log1p(train_data['car_age']))
        )
        test_data['vehicle_condition'] = (
            (test_data['power'] / 100) / 
            (np.log1p(test_data['kilometer'] / 1000) * np.log1p(test_data['car_age']))
        )
    
    # 12. å“ç‰Œ-è½¦å‹ç»„åˆç‰¹å¾ï¼ˆå¦‚æœæ•°æ®æ”¯æŒï¼‰
    if 'brand' in train_data.columns and 'model' in train_data.columns:
        train_data['brand_model'] = train_data['brand'].astype(str) + '_' + train_data['model'].astype(str)
        test_data['brand_model'] = test_data['brand'].astype(str) + '_' + test_data['model'].astype(str)
        
        # å¯¹ç»„åˆç‰¹å¾è¿›è¡Œç¼–ç 
        from sklearn.preprocessing import LabelEncoder
        le_brand_model = LabelEncoder()
        
        # åˆå¹¶æ‰€æœ‰å¯èƒ½çš„ç»„åˆå€¼
        all_brand_model = pd.concat([
            train_data['brand_model'],
            test_data['brand_model']
        ]).unique()
        
        le_brand_model.fit(all_brand_model)
        train_data['brand_model_encoded'] = le_brand_model.transform(train_data['brand_model'])
        test_data['brand_model_encoded'] = le_brand_model.transform(test_data['brand_model'])
        
        # åˆ é™¤åŸå§‹å­—ç¬¦ä¸²åˆ—
        train_data = train_data.drop(['brand_model'], axis=1)
        test_data = test_data.drop(['brand_model'], axis=1)
    
    print(f"ç‰¹å¾å·¥ç¨‹åè®­ç»ƒé›†: {train_data.shape}")
    print(f"ç‰¹å¾å·¥ç¨‹åæµ‹è¯•é›†: {test_data.shape}")
    
    return train_data, test_data

def plot_feature_importance(model, feature_names, top_n=20, save_path=None):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(top_n)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')
    plt.title(f'å‰{top_n}ä¸ªé‡è¦ç‰¹å¾çš„é‡è¦æ€§åˆ†æ', fontsize=14, fontweight='bold')
    plt.xlabel('ç‰¹å¾é‡è¦æ€§', fontsize=12)
    plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    return feature_importance

def plot_learning_curve(model, X, y, cv=2, save_path=None):  # å‡å°‘CVæŠ˜æ•°
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿åˆ†ææ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ"""
    # ä½¿ç”¨é‡‡æ ·æ•°æ®åŠ é€Ÿè®¡ç®—
    sample_size = min(12000, len(X))  # é™åˆ¶æ ·æœ¬å¤§å°
    if len(X) > sample_size:
        X_sample = X.sample(n=sample_size, random_state=42)
        # ä½¿ç”¨ç´¢å¼•å¯¹é½é€‰æ‹©å¯¹åº”æ ‡ç­¾ï¼Œé¿å…å°†ç´¢å¼•æ ‡ç­¾è¯¯ç”¨ä¸ºæ•´æ•°ä½ç½®
        y_sample = y.iloc[X_sample.index]
    else:
        X_sample, y_sample = X, y
    
    train_sizes = np.linspace(0.2, 1.0, 6)  # å‡å°‘è®­ç»ƒå¤§å°ç‚¹æ•°
    train_sizes, train_scores, val_scores = learning_curve(  # type: ignore
        model, X_sample, y_sample, cv=cv, train_sizes=train_sizes, 
        scoring='neg_mean_absolute_error', n_jobs=1  # å‡å°‘å¹¶è¡Œåº¦
    )
    
    train_mae = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mae = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mae, 'o-', color='blue', label='è®­ç»ƒé›†MAE')
    plt.fill_between(train_sizes, train_mae - train_std, train_mae + train_std, alpha=0.1, color='blue')
    plt.plot(train_sizes, val_mae, 'o-', color='red', label='éªŒè¯é›†MAE')
    plt.fill_between(train_sizes, val_mae - val_std, val_mae + val_std, alpha=0.1, color='red')
    
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°é‡', fontsize=12)
    plt.ylabel('MAE', fontsize=12)
    plt.title('å­¦ä¹ æ›²çº¿åˆ†æ - åˆ¤æ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    # åˆ†æç»“æœ
    final_gap = val_mae[-1] - train_mae[-1]
    print(f"\nå­¦ä¹ æ›²çº¿åˆ†æç»“æœ:")
    print(f"æœ€ç»ˆè®­ç»ƒMAE: {train_mae[-1]:.4f}")
    print(f"æœ€ç»ˆéªŒè¯MAE: {val_mae[-1]:.4f}")
    print(f"Gap: {final_gap:.4f}")
    
    if final_gap > 50:
        print("âš ï¸  æ¨¡å‹å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    elif final_gap < 20:
        print("ğŸš€ æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¾ƒå¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ æ¨¡å‹å¤æ‚åº¦")
    else:
        print("âœ… æ¨¡å‹å¤æ‚åº¦è¾ƒä¸ºåˆé€‚")
    
    return train_mae, val_mae

def plot_validation_curve_analysis(X, y, param_name='max_depth', param_range=None, save_path=None):
    """ç»˜åˆ¶éªŒè¯æ›²çº¿åˆ†æå‚æ•°ä¼˜åŒ–ç©ºé—´"""
    if param_range is None:
        if param_name == 'max_depth':
            param_range = [5, 10, 15, 20, 25, 30, 35]
        elif param_name == 'n_estimators':
            param_range = [50, 100, 150, 200, 250, 300]
        elif param_name == 'min_samples_split':
            param_range = [2, 5, 10, 15, 20, 25]
        else:
            param_range = [1, 2, 3, 4, 5]
    
    model = RandomForestRegressor(random_state=42, n_jobs=-1)
    
    # ä½¿ç”¨æ›´å¿«çš„äº¤å‰éªŒè¯é…ç½®
    train_scores, val_scores = validation_curve(
        model, X, y, param_name=param_name, param_range=param_range,
        cv=2, scoring='neg_mean_absolute_error', n_jobs=1  # å‡å°‘CVæŠ˜æ•°å’Œçº¿ç¨‹æ•°
    )
    
    train_mae = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mae = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mae, 'o-', color='blue', label='è®­ç»ƒé›†MAE')
    plt.fill_between(param_range, train_mae - train_std, train_mae + train_std, alpha=0.1, color='blue')
    plt.plot(param_range, val_mae, 'o-', color='red', label='éªŒè¯é›†MAE')
    plt.fill_between(param_range, val_mae - val_std, val_mae + val_std, alpha=0.1, color='red')
    
    plt.xlabel(f'{param_name} å‚æ•°å€¼', fontsize=12)
    plt.ylabel('MAE', fontsize=12)
    plt.title(f'{param_name} å‚æ•°éªŒè¯æ›²çº¿ - å¯»æ‰¾æœ€ä¼˜å‚æ•°', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"éªŒè¯æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    # æ‰¾åˆ°æœ€ä¼˜å‚æ•°
    best_idx = np.argmin(val_mae)
    best_param = param_range[best_idx]
    best_mae = val_mae[best_idx]
    
    print(f"\n{param_name} å‚æ•°ä¼˜åŒ–ç»“æœ:")
    print(f"æœ€ä¼˜å‚æ•°: {best_param}")
    print(f"æœ€ä¼˜MAE: {best_mae:.4f}")
    
    return best_param, best_mae

def plot_residual_analysis(y_true, y_pred, save_path=None):
    """ç»˜åˆ¶æ®‹å·®åˆ†æå›¾"""
    residuals = y_true - y_pred
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ®‹å·® vs é¢„æµ‹å€¼
    axes[0, 0].scatter(y_pred, residuals, alpha=0.6, s=20)
    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)
    axes[0, 0].set_xlabel('é¢„æµ‹å€¼', fontsize=12)
    axes[0, 0].set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)', fontsize=12)
    axes[0, 0].set_title('æ®‹å·® vs é¢„æµ‹å€¼', fontsize=12, fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾
    axes[0, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
    axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.8)
    axes[0, 1].set_xlabel('æ®‹å·®', fontsize=12)
    axes[0, 1].set_ylabel('é¢‘æ¬¡', fontsize=12)
    axes[0, 1].set_title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾', fontsize=12, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. QQå›¾ - æ£€éªŒæ®‹å·®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1, 0])
    axes[1, 0].set_title('QQå›¾ - æ®‹å·®æ­£æ€æ€§æ£€éªŒ', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. çœŸå®å€¼ vs é¢„æµ‹å€¼
    axes[1, 1].scatter(y_true, y_pred, alpha=0.6, s=20)
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
    axes[1, 1].set_xlabel('çœŸå®å€¼', fontsize=12)
    axes[1, 1].set_ylabel('é¢„æµ‹å€¼', fontsize=12)
    axes[1, 1].set_title('çœŸå®å€¼ vs é¢„æµ‹å€¼', fontsize=12, fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ®‹å·®åˆ†æå›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    
    print(f"\næ®‹å·®åˆ†æç»“æœ:")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"RÂ²: {r2:.4f}")
    print(f"æ®‹å·®å¹³å‡å€¼: {residuals.mean():.4f}")
    print(f"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}")
    
    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'residuals': residuals}

def plot_price_distribution_comparison(y_train, ensemble_pred, save_path=None):
    """å¯¹æ¯”çœŸå®ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼çš„åˆ†å¸ƒ"""
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.hist(y_train, bins=50, alpha=0.7, label='è®­ç»ƒé›†çœŸå®ä»·æ ¼', color='blue', edgecolor='black')
    plt.hist(ensemble_pred, bins=50, alpha=0.7, label='æµ‹è¯•é›†é¢„æµ‹ä»·æ ¼', color='red', edgecolor='black')
    plt.xlabel('ä»·æ ¼', fontsize=12)
    plt.ylabel('é¢‘æ¬¡', fontsize=12)
    plt.title('ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.boxplot([y_train, ensemble_pred], label=['è®­ç»ƒé›†çœŸå®ä»·æ ¼', 'æµ‹è¯•é›†é¢„æµ‹ä»·æ ¼'])
    plt.ylabel('ä»·æ ¼', fontsize=12)
    plt.title('ä»·æ ¼åˆ†å¸ƒç®±çº¿å›¾', fontsize=12, fontweight='bold')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    print(f"\nä»·æ ¼åˆ†å¸ƒç»Ÿè®¡å¯¹æ¯”:")
    print(f"è®­ç»ƒé›† - å¹³å‡: {y_train.mean():.2f}, ä¸­ä½: {y_train.median():.2f}, æ ‡å‡†å·®: {y_train.std():.2f}")
    print(f"é¢„æµ‹é›† - å¹³å‡: {ensemble_pred.mean():.2f}, ä¸­ä½: {np.median(ensemble_pred):.2f}, æ ‡å‡†å·®: {ensemble_pred.std():.2f}")

def create_optimized_rf_ensemble(X_train, y_train, X_test, enable_analysis=True):
    """åˆ›å»ºä¼˜åŒ–çš„éšæœºæ£®æ—é›†æˆ - é‡ç‚¹è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜"""
    print("åˆ›å»ºæŠ—è¿‡æ‹Ÿåˆéšæœºæ£®æ—é›†æˆ...")
    
    # åŸºäºç‰¹å¾æ•°é‡è°ƒæ•´å‚æ•°
    n_features = X_train.shape[1]
    
    # è¶…ä¿å®ˆçš„é˜²è¿‡æ‹Ÿåˆé…ç½® - é’ˆå¯¹727åˆ†æ•°è¿›ä¸€æ­¥ä¼˜åŒ–
    rf_models = [
        # æ¨¡å‹1ï¼šæåº¦ä¿å®ˆ - æœ€å¼ºæ­£åˆ™åŒ–
        RandomForestRegressor(
            n_estimators=120,      # å‡å°‘æ ‘æ•°é‡
            max_depth=8,           # å¤§å¹…å‡å°‘æ·±åº¦
            min_samples_split=30,  # æ˜¾è‘—å¢åŠ æœ€å°åˆ†è£‚æ ·æœ¬
            min_samples_leaf=20,   # æ˜¾è‘—å¢åŠ æœ€å°å¶å­æ ·æœ¬
            max_features=0.4,      # å¤§å¹…å‡å°‘ç‰¹å¾é‡‡æ ·
            bootstrap=True,
            random_state=42,
            n_jobs=-1,
            min_impurity_decrease=0.001  # æ·»åŠ çº¯åº¦é˜ˆå€¼
        ),
        # æ¨¡å‹2ï¼šä¸­ç­‰ä¿å®ˆ
        RandomForestRegressor(
            n_estimators=150,
            max_depth=10,          # ä»15å‡åˆ°10
            min_samples_split=25,  # ä»20å¢åŠ åˆ°25
            min_samples_leaf=15,   # ä»12å¢åŠ åˆ°15
            max_features=0.5,      # ä»sqrtæ”¹ä¸ºå›ºå®šæ¯”ä¾‹
            bootstrap=True,
            random_state=123,
            n_jobs=-1,
            min_impurity_decrease=0.0005
        ),
        # æ¨¡å‹3ï¼šé€‚åº¦ä¿å®ˆ
        RandomForestRegressor(
            n_estimators=180,
            max_depth=12,          # ä»18å‡åˆ°12
            min_samples_split=20,
            min_samples_leaf=12,
            max_features=0.6,      # æ§åˆ¶ç‰¹å¾æ¯”ä¾‹
            bootstrap=True,
            random_state=456,
            n_jobs=-1
        ),
        # æ¨¡å‹4ï¼šæ–°å¢æç®€æ¨¡å‹
        RandomForestRegressor(
            n_estimators=100,
            max_depth=6,           # æœ€æµ…æ·±åº¦
            min_samples_split=40,  # æœ€å¤§åˆ†è£‚è¦æ±‚
            min_samples_leaf=25,   # æœ€å¤§å¶å­è¦æ±‚
            max_features=0.3,      # æœ€å°‘ç‰¹å¾
            bootstrap=True,
            random_state=789,
            n_jobs=-1,
            min_impurity_decrease=0.002
        ),
        # æ¨¡å‹5ï¼šBaggingé£æ ¼çš„ç®€åŒ–æ¨¡å‹
        RandomForestRegressor(
            n_estimators=200,
            max_depth=9,
            min_samples_split=35,
            min_samples_leaf=18,
            max_features=0.3,
            bootstrap=True,
            max_samples=0.8,       # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
            random_state=999,
            n_jobs=-1
        )
    ]
    
    # è®­ç»ƒé›†æˆæ¨¡å‹å¹¶è®¡ç®—OOBåˆ†æ•°
    predictions = []
    trained_models = []
    oob_scores = []
    
    for i, model in enumerate(rf_models):
        print(f"è®­ç»ƒæŠ—è¿‡æ‹Ÿåˆæ¨¡å‹ {i+1}/{len(rf_models)}...")
        
        # è®¾ç½®OOBè¯„åˆ†
        model.oob_score = True
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        pred = np.maximum(pred, 0)  # ç¡®ä¿éè´Ÿ
        
        predictions.append(pred)
        trained_models.append(model)
        oob_scores.append(model.oob_score_)
        
        print(f"  æ¨¡å‹{i+1} - é¢„æµ‹èŒƒå›´: {pred.min():.2f} - {pred.max():.2f}, OOB Score: {model.oob_score_:.4f}")
    
    # åŸºäºOOBåˆ†æ•°çš„æ™ºèƒ½åŠ æƒé›†æˆ
    oob_weights = np.array(oob_scores)
    # å¯¹è´Ÿçš„OOBåˆ†æ•°è¿›è¡Œå¤„ç†ï¼ˆè½¬æ¢ä¸ºæ­£æ•°ï¼‰
    if np.any(oob_weights < 0):
        oob_weights = oob_weights - np.min(oob_weights) + 0.01
    
    # å½’ä¸€åŒ–æƒé‡
    oob_weights = oob_weights / oob_weights.sum()
    
    # é¢å¤–ç»™ç®€å•æ¨¡å‹ï¼ˆç¬¬1å’Œç¬¬4ä¸ªï¼‰åŠ æƒ
    simple_model_boost = [1.2, 1.0, 1.0, 1.3, 1.1]  # ç®€å•æ¨¡å‹æƒé‡æå‡
    final_weights = oob_weights * np.array(simple_model_boost)
    final_weights = final_weights / final_weights.sum()
    
    ensemble_pred = np.average(predictions, axis=0, weights=final_weights)
    
    print(f"\né›†æˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"  OOBåˆ†æ•°: {[f'{score:.4f}' for score in oob_scores]}")
    print(f"  æœ€ç»ˆæƒé‡: {[f'{w:.3f}' for w in final_weights]}")
    print(f"  å‡å€¼: {ensemble_pred.mean():.2f}")
    print(f"  ä¸­ä½æ•°: {np.median(ensemble_pred):.2f}")
    print(f"  èŒƒå›´: {ensemble_pred.min():.2f} - {ensemble_pred.max():.2f}")
    # å¦‚æœå¯ç”¨åˆ†æï¼Œè¿›è¡Œè¯¦ç»†åˆ†æ
    if enable_analysis:
        print("\nå¼€å§‹æ¨¡å‹åˆ†æ...")
        
        # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
        results_dir = get_project_path('prediction_result')
        os.makedirs(results_dir, exist_ok=True)
        
        # ä½¿ç”¨æœ€ä¿å®ˆçš„æ¨¡å‹è¿›è¡Œåˆ†æ
        main_model = trained_models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ€ä¿å®ˆçš„æ¨¡å‹
        feature_names = X_train.columns.tolist()
        
        # 1. ç‰¹å¾é‡è¦æ€§åˆ†æ
        print("1. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾...")
        importance_path = get_user_data_path('feature_importance.png')
        feature_importance = plot_feature_importance(
            main_model, feature_names, top_n=20, save_path=importance_path
        )
        
        # 2. å­¦ä¹ æ›²çº¿åˆ†æ - é‡ç‚¹å…³æ³¨è¿‡æ‹Ÿåˆ
        print("2. ç»˜åˆ¶å­¦ä¹ æ›²çº¿...")
        learning_path = get_user_data_path('learning_curve.png')
        train_mae, val_mae = plot_learning_curve(
            main_model, X_train, y_train, cv=3, save_path=learning_path
        )
        
        # 3. å‚æ•°éªŒè¯æ›²çº¿ - éªŒè¯å½“å‰å‚æ•°æ˜¯å¦åˆé€‚
        print("3. ç»˜åˆ¶å‚æ•°éªŒè¯æ›²çº¿...")
        validation_path = get_user_data_path('validation_curve.png')
        # éªŒè¯max_depthå‚æ•°
        best_depth, best_mae = plot_validation_curve_analysis(
            X_train.sample(n=min(8000, len(X_train)), random_state=42),
            y_train.iloc[:min(8000, len(y_train))],
            param_name='max_depth', 
            param_range=[6, 8, 10, 12, 15],  # éªŒè¯æ›´ä¿å®ˆçš„æ·±åº¦èŒƒå›´
            save_path=validation_path
        )
        
        # 4. æ®‹å·®åˆ†æï¼ˆä½¿ç”¨äº¤å‰éªŒè¯é¢„æµ‹ï¼‰
        print("4. è¿›è¡Œæ®‹å·®åˆ†æ...")
        from sklearn.model_selection import cross_val_predict
        cv_pred = cross_val_predict(main_model, X_train, y_train, cv=3)
        residual_path = get_user_data_path('residual_analysis.png')
        residual_stats = plot_residual_analysis(
            y_train, cv_pred, save_path=residual_path
        )
        
        # 5. ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”
        print("5. ç»˜åˆ¶ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”...")
        distribution_path = get_user_data_path('price_distribution.png')
        plot_price_distribution_comparison(
            y_train, ensemble_pred, save_path=distribution_path
        )
        
        print(f"\nğŸ“Š æ‰€æœ‰åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {get_user_data_path()}")
        
        # è¿”å›åˆ†æç»“æœ
        analysis_results = {
            'feature_importance': feature_importance,
            'best_depth': best_depth,
            'residual_stats': residual_stats,
            'final_cv_mae': val_mae[-1] if len(val_mae) > 0 else None,
            'oob_scores': oob_scores,
            'model_weights': final_weights.tolist()
        }
        
        return ensemble_pred, analysis_results
    
    return ensemble_pred

def quick_cv_evaluation(X_train, y_train):
    """å¿«é€Ÿäº¤å‰éªŒè¯è¯„ä¼° - ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°"""
    print("å¿«é€Ÿäº¤å‰éªŒè¯è¯„ä¼°...")
    
    # ä½¿ç”¨æåº¦ä¿å®ˆçš„å‚æ•° - ä¸é›†æˆä¸­æœ€ä¿å®ˆæ¨¡å‹ä¿æŒä¸€è‡´
    rf = RandomForestRegressor(
        n_estimators=100,      # å‡å°‘æ ‘æ•°é‡
        max_depth=8,           # ä¸æœ€ä¿å®ˆæ¨¡å‹ä¸€è‡´
        min_samples_split=30,  # å¤§å¹…å¢åŠ æœ€å°åˆ†è£‚æ ·æœ¬
        min_samples_leaf=20,   # å¤§å¹…å¢åŠ æœ€å°å¶å­æ ·æœ¬
        max_features=0.4,      # ä¸æœ€ä¿å®ˆæ¨¡å‹ä¸€è‡´
        random_state=42,
        n_jobs=-1,
        bootstrap=True,
        min_impurity_decrease=0.001  # æ·»åŠ çº¯åº¦é˜ˆå€¼é˜²è¿‡æ‹Ÿåˆ
    )
    
    kfold = KFold(n_splits=3, shuffle=True, random_state=42)  # ä½¿ç”¨3æŠ˜ä¿è¯ç¨³å®šæ€§
    cv_scores = cross_val_score(rf, X_train, y_train, cv=kfold, scoring='neg_mean_absolute_error', n_jobs=1)
    cv_mae = -cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"äº¤å‰éªŒè¯MAE: {cv_mae:.4f} Â± {cv_std:.4f}")
    print(f"ä½¿ç”¨æåº¦ä¿å®ˆå‚æ•°ï¼šmax_depth=8, min_samples_split=30, min_samples_leaf=20")
    return cv_mae

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹é’ˆå¯¹æ€§ä¼˜åŒ–...")
    print("ç›®æ ‡: MAEä»698é™åˆ°500ä»¥å†…")
    
    # 1. ä¼˜åŒ–æ•°æ®åŠ è½½
    train_data, test_data = load_and_optimize_data()
    
    # 2. åˆ›å»ºé’ˆå¯¹æ€§ç‰¹å¾
    train_data, test_data = create_targeted_features(train_data, test_data)
    
    # 3. å‡†å¤‡å»ºæ¨¡æ•°æ®
    feature_cols = [col for col in train_data.columns if col != 'price']
    feature_cols = [col for col in feature_cols if col in test_data.columns]
    
    X_train = train_data[feature_cols]
    y_train = train_data['price']
    X_test = test_data[feature_cols]
    
    print(f"\næœ€ç»ˆç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(X_train)}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}")
    
    # 4. å¿«é€Ÿäº¤å‰éªŒè¯è¯„ä¼° - ä½¿ç”¨é‡‡æ ·åŠ é€Ÿ
    print("4. å¿«é€Ÿäº¤å‰éªŒè¯è¯„ä¼°...")
    sample_size = min(15000, len(X_train))  # é‡‡æ ·å‡å°‘è®¡ç®—é‡
    X_sample = X_train.sample(n=sample_size, random_state=42)
    y_sample = y_train.iloc[X_sample.index]
    cv_mae = quick_cv_evaluation(X_sample, y_sample)
    
    # 5. åˆ›å»ºä¼˜åŒ–é›†æˆï¼ˆå¯ç”¨åˆ†æåŠŸèƒ½ï¼‰
    result = create_optimized_rf_ensemble(X_train, y_train, X_test, enable_analysis=True)
    
    # æ£€æŸ¥è¿”å›å€¼ç±»å‹
    if isinstance(result, tuple):
        ensemble_pred, analysis_results = result
        print(f"\nğŸ“Š æ¨¡å‹åˆ†æå®Œæˆï¼")
        print(f"æœ€ä¼˜æ·±åº¦å»ºè®®: {analysis_results.get('best_depth', 'N/A')}")
        if analysis_results.get('final_cv_mae'):
            print(f"å­¦ä¹ æ›²çº¿æœ€ç»ˆMAE: {analysis_results['final_cv_mae']:.4f}")
    else:
        ensemble_pred = result
        print(f"\nâš ï¸ è·³è¿‡äº†æ¨¡å‹åˆ†æ")
    
    # 6. ä¿å­˜ç»“æœ - æŒ‰è§„èŒƒä¿å­˜
    submission = pd.DataFrame({
        'SaleID': range(len(ensemble_pred)),
        'price': ensemble_pred
    })
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æŒ‰è§„èŒƒä¿å­˜åˆ°resultsç›®å½•
    results_dir = get_project_path('prediction_result')
    os.makedirs(results_dir, exist_ok=True)
    
    filename = os.path.join(results_dir, f'rf_result_{timestamp}.csv')
    submission.to_csv(filename, index=False)
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    models_to_save = {}
    
    # ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„RFæ¨¡å‹
    for i, model in enumerate(trained_models):
        models_to_save[f'rf_{i+1}'] = model
    
    if models_to_save:
        save_models(models_to_save, 'rf_baseline')
    
    print(f"\n=== é’ˆå¯¹æ€§ä¼˜åŒ–å®Œæˆ ===")
    print(f"äº¤å‰éªŒè¯MAE: {cv_mae:.4f}")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    print(f"é¢„æµ‹å‡å€¼: {ensemble_pred.mean():.2f}")
    print(f"é¢„æµ‹èŒƒå›´: {ensemble_pred.min():.2f} - {ensemble_pred.max():.2f}")
    
    # é¢„æµ‹æ€§èƒ½è¯„ä¼°
    if cv_mae < 500:
        print("ğŸ‰ ç›®æ ‡è¾¾æˆï¼äº¤å‰éªŒè¯MAE < 500")
    else:
        print(f"âš ï¸  è¿˜éœ€ä¼˜åŒ–ï¼Œå½“å‰MAE {cv_mae:.0f}ï¼Œç›®æ ‡ < 500")
    
    return filename

if __name__ == "__main__":
    result_file = main()
    print("é’ˆå¯¹æ€§ä¼˜åŒ–å®Œæˆï¼")