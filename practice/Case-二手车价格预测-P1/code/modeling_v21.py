"""
V21ç‰ˆæœ¬æ¨¡å‹ - æŠ—æ³„éœ²å¼ºæ³›åŒ–ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰

é’ˆå¯¹V20è®­ç»ƒ-æµ‹è¯•å·®è·å¤§çš„é—®é¢˜ï¼Œå®æ–½ä»¥ä¸‹ä¿®å¤ç­–ç•¥:
1. ä¸¥æ ¼çš„æ•°æ®æ³„éœ²é˜²æŠ¤ - åªä½¿ç”¨è®­ç»ƒé›†ä¿¡æ¯æ„å»ºç»Ÿè®¡ç‰¹å¾
2. ç®€åŒ–ç‰¹å¾å·¥ç¨‹ - å‡å°‘ç‰¹å¾æ•°é‡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
3. å¼ºæ­£åˆ™åŒ–ç­–ç•¥ - å¢åŠ L1/L2æ­£åˆ™åŒ–å’Œæ—©åœæœºåˆ¶
4. æ—¶é—´ä¸€è‡´æ€§éªŒè¯ - ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸€è‡´
5. ä¿å®ˆé›†æˆç­–ç•¥ - ç®€å•å¹³å‡ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–
ç›®æ ‡ï¼šç¼©å°è®­ç»ƒ-æµ‹è¯•å·®è·ï¼Œå®é™…MAE < 550
"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
import warnings
import joblib
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def save_models(models, version_name):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°modelç›®å½•
    
    Parameters:
    -----------
    models : dict
        æ¨¡å‹å­—å…¸ï¼Œkeyä¸ºæ¨¡å‹åç§°ï¼Œvalueä¸ºæ¨¡å‹å¯¹è±¡
    version_name : str
        ç‰ˆæœ¬åç§°ï¼Œå¦‚'v28'
    """
    model_dir = get_project_path('model')
    os.makedirs(model_dir, exist_ok=True)
    
    saved_files = []
    for model_name, model_obj in models.items():
        if model_obj is not None:
            model_file = os.path.join(model_dir, f'{version_name}_{model_name}_model.pkl')
            joblib.dump(model_obj, model_file)
            saved_files.append(model_file)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_file}")
    
    return saved_files


def get_user_data_path(*paths):
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

def leak_free_data_preprocessing():
    """
    æ— æ³„éœ²æ•°æ®é¢„å¤„ç† - ä¸¥æ ¼åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¿¡æ¯
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆ†åˆ«å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œé¿å…æ•°æ®æ³„éœ²
    # è®­ç»ƒé›†é¢„å¤„ç†
    train_processed = preprocess_single_dataset(train_df, is_train=True)
    
    # æå–è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯
    train_stats = train_processed.attrs['train_stats']
    
    # æµ‹è¯•é›†é¢„å¤„ç†ï¼ˆåªä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰
    test_processed = preprocess_single_dataset(test_df, is_train=False, train_stats=train_stats)
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_processed.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_processed.shape}")
    
    return train_processed, test_processed



def preprocess_single_dataset(df, is_train=True, train_stats=None):
    """
    å•ä¸ªæ•°æ®é›†é¢„å¤„ç†ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    df = df.copy()
    
    stats_dict = {} if is_train else train_stats
    
    # 1. åŸºç¡€å¼‚å¸¸å€¼å¤„ç†
    if 'power' in df.columns:
        # ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°ä¿¡æ¯
        if is_train:
            power_p95 = df['power'].quantile(0.95)
            power_p99 = df['power'].quantile(0.99)
            stats_dict['power_stats'] = {'p95': power_p95, 'p99': power_p99}
        else:
            power_p95 = train_stats['power_stats']['p95']
            power_p99 = train_stats['power_stats']['p99']
        
        # ä¿å®ˆæˆªæ–­
        df['power'] = np.clip(df['power'], 0, power_p99)
        df['power_is_zero'] = (df['power'] <= 0).astype(int)
        df['power_is_high'] = (df['power'] > power_p95).astype(int)
    
    # 2. åˆ†ç±»ç‰¹å¾å¤„ç†
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model']
    for col in categorical_cols:
        if col in df.columns:
            # ç¼ºå¤±å€¼æ ‡è®°
            df[f'{col}_missing'] = (df[col].isnull()).astype(int)
            
            if is_train:
                # è®­ç»ƒé›†ï¼šè®¡ç®—ä¼—æ•°
                mode_value = df[col].mode()
                mode_val = mode_value.iloc[0] if len(mode_value) > 0 else df[col].mode().iloc[0]
                stats_dict[f'{col}_mode'] = mode_val
                df[col] = df[col].fillna(mode_val)
            else:
                # æµ‹è¯•é›†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„ä¼—æ•°
                df[col] = df[col].fillna(train_stats[f'{col}_mode'])
    
    # 3. æ—¶é—´ç‰¹å¾å·¥ç¨‹
    df['regDate'] = pd.to_datetime(df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    df['car_age'] = current_year - df['regDate'].dt.year
    df['car_age'] = df['car_age'].fillna(df['car_age'].median()).astype(int)
    
    # ç®€åŒ–çš„æ—¶é—´ç‰¹å¾
    df['reg_month'] = df['regDate'].dt.month.fillna(6).astype(int)
    df['reg_quarter'] = df['regDate'].dt.quarter.fillna(2).astype(int)
    df.drop(columns=['regDate'], inplace=True)
    
    # 4. æ— æ³„éœ²çš„å“ç‰Œç»Ÿè®¡ç‰¹å¾
    if is_train and 'price' in df.columns:
        # åªä½¿ç”¨è®­ç»ƒé›†è®¡ç®—å“ç‰Œç»Ÿè®¡
        brand_stats = df.groupby('brand')['price'].agg(['mean', 'count']).reset_index()
        
        # ä¿å®ˆçš„å¹³æ»‘å› å­
        brand_stats['smooth_factor'] = 50  # å›ºå®šå¹³æ»‘å› å­
        global_mean = df['price'].mean()
        brand_stats['brand_avg_price'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                         global_mean * brand_stats['smooth_factor']) / 
                                        (brand_stats['count'] + brand_stats['smooth_factor']))
        
        # å­˜å‚¨å“ç‰Œç»Ÿè®¡ä¿¡æ¯
        brand_map = brand_stats.set_index('brand')['brand_avg_price'].to_dict()
        stats_dict['brand_stats'] = brand_map
        stats_dict['price_mean'] = global_mean
        
        # åº”ç”¨å“ç‰Œç»Ÿè®¡ç‰¹å¾
        df['brand_avg_price'] = df['brand'].map(brand_map).fillna(global_mean)
        
    elif not is_train and train_stats is not None:
        # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„å“ç‰Œç»Ÿè®¡
        brand_map = train_stats['brand_stats']
        price_mean = train_stats['price_mean']
        df['brand_avg_price'] = df['brand'].map(brand_map).fillna(price_mean)
    
    # 5. æ ‡ç­¾ç¼–ç ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ç¼–ç ï¼‰
    if is_train:
        categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
        label_encoders = {}
        
        for col in categorical_cols:
            if col in df.columns:
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                label_encoders[col] = le
        
        stats_dict['label_encoders'] = label_encoders
    else:
        # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„ç¼–ç å™¨
        categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
        label_encoders = train_stats['label_encoders']
        
        for col in categorical_cols:
            if col in df.columns:
                le = label_encoders[col]
                # å¤„ç†æµ‹è¯•é›†ä¸­çš„æ–°ç±»åˆ«
                unique_values = set(df[col].astype(str).unique())
                train_values = set(le.classes_)
                new_values = unique_values - train_values
                
                if new_values:
                    # å°†æ–°å€¼æ˜ å°„ä¸ºæœªçŸ¥ç±»åˆ«
                    df[col] = df[col].astype(str).map(lambda x: x if x in train_values else 'unknown')
                    # æ·»åŠ unknownç±»åˆ«åˆ°ç¼–ç å™¨
                    if 'unknown' not in le.classes_:
                        le.classes_ = np.append(le.classes_, 'unknown')
                
                df[col] = le.transform(df[col].astype(str))
    
    # 6. æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                if is_train:
                    median_val = df[col].median()
                    stats_dict[f'{col}_median'] = median_val
                else:
                    median_val = train_stats[f'{col}_median']
                
                df[col] = df[col].fillna(median_val if not pd.isna(median_val) else 0)
    
    # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œå°†ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå±æ€§å­˜å‚¨
    if is_train:
        df.attrs['train_stats'] = stats_dict
    
    return df

def create_robust_features(df):
    """
    åˆ›å»ºé²æ£’ç‰¹å¾ - ç®€åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
    """
    df = df.copy()
    
    # 1. æ ¸å¿ƒä¸šåŠ¡ç‰¹å¾ - åªä¿ç•™æœ€æœ‰æ•ˆçš„
    if 'power' in df.columns and 'car_age' in df.columns:
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_decay'] = df['power'] * np.exp(-df['car_age'] * 0.05)  # ä¿å®ˆçš„è¡°å‡ç³»æ•°
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        # å¼ºé™åˆ¶æå€¼
        df['km_per_year'] = np.clip(df['km_per_year'], 0, 30000)
    
    # 2. ç®€åŒ–åˆ†æ®µç‰¹å¾
    df['age_segment'] = pd.cut(df['car_age'], bins=[-1, 3, 6, 10, float('inf')], 
                              labels=['very_new', 'new', 'medium', 'old'])
    df['age_segment'] = df['age_segment'].cat.codes
    
    if 'power' in df.columns:
        df['power_segment'] = pd.cut(df['power'], bins=[-1, 100, 200, 300, float('inf')], 
                                    labels=['low', 'medium', 'high', 'very_high'])
        df['power_segment'] = df['power_segment'].cat.codes
    
    # 3. åŸºç¡€å˜æ¢ç‰¹å¾
    if 'car_age' in df.columns:
        df['log_car_age'] = np.log1p(df['car_age'])
    
    if 'kilometer' in df.columns:
        df['log_kilometer'] = np.log1p(df['kilometer'])
    
    # 4. ç®€åŒ–çš„vç‰¹å¾ç»Ÿè®¡
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)
        df['v_range'] = df[v_cols].max(axis=1) - df[v_cols].min(axis=1)
    
    # 5. æ•°æ®æ¸…ç† - ä¿å®ˆçš„å¼‚å¸¸å€¼å¤„ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['SaleID', 'price']:
            # å¤„ç†æ— ç©·å¤§å€¼
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            # å¡«å……NaNå€¼
            df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)
            
            # ä¿å®ˆçš„æå€¼å¤„ç†
            if df[col].std() > 1e-8:
                q99 = df[col].quantile(0.99)
                q01 = df[col].quantile(0.01)
                if q99 > q01:
                    df[col] = np.clip(df[col], q01, q99)
    
    return df

def conservative_feature_selection(X_train, y_train, max_features=50):
    """
    ä¿å®ˆç‰¹å¾é€‰æ‹© - é™åˆ¶ç‰¹å¾æ•°é‡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
    """
    print(f"æ‰§è¡Œä¿å®ˆç‰¹å¾é€‰æ‹©ï¼Œæœ€å¤šä¿ç•™ {max_features} ä¸ªç‰¹å¾...")
    
    # ä½¿ç”¨éšæœºæ£®æ—è¯„ä¼°ç‰¹å¾é‡è¦æ€§
    rf = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)
    rf.fit(X_train, y_train)
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # é€‰æ‹©å‰Nä¸ªé‡è¦ç‰¹å¾
    selected_features = feature_importance.head(max_features)['feature'].tolist()
    
    print(f"é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
    print("å‰10ä¸ªé‡è¦ç‰¹å¾:")
    print(feature_importance.head(10))
    
    return selected_features

def train_regularized_ensemble(X_train, y_train, X_test):
    """
    è®­ç»ƒå¼ºæ­£åˆ™åŒ–é›†æˆæ¨¡å‹
    """
    print("è®­ç»ƒå¼ºæ­£åˆ™åŒ–é›†æˆæ¨¡å‹...")
    
    # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
    y_train_log = np.log1p(y_train)
    
    # æ—¶é—´åºåˆ—åˆ†å‰² - æ›´ä¸¥æ ¼çš„éªŒè¯
    tscv = TimeSeriesSplit(n_splits=5)
    
    # å¼ºæ­£åˆ™åŒ–å‚æ•°
    lgb_params = {
        'objective': 'mae',
        'metric': 'mae',
        'num_leaves': 15,        # å¤§å¹…å‡å°‘
        'max_depth': 4,          # é™ä½æ·±åº¦
        'learning_rate': 0.03,   # é™ä½å­¦ä¹ ç‡
        'feature_fraction': 0.7, # å‡å°‘ç‰¹å¾ä½¿ç”¨
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'lambda_l1': 1.0,        # å¢åŠ L1æ­£åˆ™åŒ–
        'lambda_l2': 1.0,        # å¢åŠ L2æ­£åˆ™åŒ–
        'min_child_samples': 50, # å¢åŠ æœ€å°æ ·æœ¬æ•°
        'random_state': 42,
    }
    
    xgb_params = {
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'max_depth': 4,          # é™ä½æ·±åº¦
        'learning_rate': 0.03,   # é™ä½å­¦ä¹ ç‡
        'subsample': 0.7,        # å‡å°‘æ ·æœ¬ä½¿ç”¨
        'colsample_bytree': 0.7,
        'reg_alpha': 2.0,        # å¢åŠ L1æ­£åˆ™åŒ–
        'reg_lambda': 2.0,       # å¢åŠ L2æ­£åˆ™åŒ–
        'min_child_weight': 20,  # å¢åŠ æœ€å°æƒé‡
        'random_state': 42
    }
    
    catboost_params = {
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'depth': 4,              # é™ä½æ·±åº¦
        'learning_rate': 0.03,   # é™ä½å­¦ä¹ ç‡
        'iterations': 800,
        'l2_leaf_reg': 5.0,      # å¢åŠ L2æ­£åˆ™åŒ–
        'random_strength': 1.0,
        'bootstrap_type': 'Bayesian',
        'random_seed': 42,
        'verbose': False
    }
    
    # å­˜å‚¨é¢„æµ‹ç»“æœ
    lgb_predictions = np.zeros(len(X_test))
    xgb_predictions = np.zeros(len(X_test))
    cat_predictions = np.zeros(len(X_test))
    
    # å­˜å‚¨éªŒè¯åˆ†æ•°
    lgb_scores, xgb_scores, cat_scores = [], [], []
    
    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è®­ç»ƒ
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):
        print(f"è®­ç»ƒç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]
        
        # è®­ç»ƒLightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=2000)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(0)])
        
        lgb_predictions += np.expm1(np.array(lgb_model.predict(X_test))) / 5
        lgb_val_pred = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_val_pred)
        lgb_scores.append(lgb_mae)
        
        # è®­ç»ƒXGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=2000, early_stopping_rounds=100)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        xgb_predictions += np.expm1(xgb_model.predict(X_test)) / 5
        xgb_val_pred = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_val_pred)
        xgb_scores.append(xgb_mae)
        
        # è®­ç»ƒCatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=100, 
                     verbose=False)
        
        cat_predictions += np.expm1(cat_model.predict(X_test)) / 5
        cat_val_pred = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_val_pred)
        cat_scores.append(cat_mae)
        
        print(f"  LightGBM MAE: {lgb_mae:.2f}, XGBoost MAE: {xgb_mae:.2f}, CatBoost MAE: {cat_mae:.2f}")
    
    print(f"\næ—¶é—´åºåˆ—äº¤å‰éªŒè¯å¹³å‡åˆ†æ•°:")
    print(f"  LightGBM: {np.mean(lgb_scores):.2f} (Â±{np.std(lgb_scores):.2f})")
    print(f"  XGBoost: {np.mean(xgb_scores):.2f} (Â±{np.std(xgb_scores):.2f})")
    print(f"  CatBoost: {np.mean(cat_scores):.2f} (Â±{np.std(cat_scores):.2f})")
    
    return lgb_predictions, xgb_predictions, cat_predictions, {
        'lgb_scores': lgb_scores,
        'xgb_scores': xgb_scores,
        'cat_scores': cat_scores
    }

def conservative_ensemble(lgb_pred, xgb_pred, cat_pred, scores_info):
    """
    ä¿å®ˆé›†æˆç­–ç•¥ - ç®€å•å¹³å‡ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–
    """
    print("æ‰§è¡Œä¿å®ˆé›†æˆç­–ç•¥...")
    
    # ç­‰æƒé‡å¹³å‡ - æœ€ä¿å®ˆçš„ç­–ç•¥
    ensemble_pred = (lgb_pred + xgb_pred + cat_pred) / 3
    
    print(f"ä½¿ç”¨ç­‰æƒé‡å¹³å‡é›†æˆ")
    
    return ensemble_pred

def robust_calibration(predictions, y_train):
    """
    é²æ£’æ ¡å‡† - ä¿å®ˆçš„æ ¡å‡†ç­–ç•¥
    """
    train_mean = y_train.mean()
    pred_mean = predictions.mean()
    
    print(f"\né²æ£’æ ¡å‡†:")
    print(f"  è®­ç»ƒé›†å‡å€¼: {train_mean:.2f}")
    print(f"  é¢„æµ‹å‡å€¼: {pred_mean:.2f}")
    
    # ä¿å®ˆçš„æ ¡å‡†å› å­
    calibration_factor = train_mean / pred_mean if pred_mean > 0 else 1.0
    calibration_factor = np.clip(calibration_factor, 0.9, 1.1)  # ä¸¥æ ¼é™åˆ¶æ ¡å‡†å¹…åº¦
    print(f"  æ ¡å‡†å› å­(é™åˆ¶å): {calibration_factor:.4f}")
    
    # åº”ç”¨æ ¡å‡†
    calibrated_predictions = predictions * calibration_factor
    
    # ç¡®ä¿é¢„æµ‹å€¼éè´Ÿ
    calibrated_predictions = np.maximum(calibrated_predictions, 0)
    
    return calibrated_predictions

def create_robust_analysis_plots(y_train, predictions, scores_info):
    """
    åˆ›å»ºé²æ£’åˆ†æå›¾è¡¨
    """
    print("ç”Ÿæˆé²æ£’åˆ†æå›¾è¡¨...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    analysis_dir = get_user_data_path()
    os.makedirs(analysis_dir, exist_ok=True)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”
    axes[0, 0].hist(y_train, bins=50, alpha=0.7, label='è®­ç»ƒé›†çœŸå®ä»·æ ¼', color='blue', density=True)
    axes[0, 0].hist(predictions, bins=50, alpha=0.7, label='V21é¢„æµ‹ä»·æ ¼', color='red', density=True)
    axes[0, 0].set_xlabel('ä»·æ ¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('V21ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”')
    axes[0, 0].legend()
    axes[0, 0].axvline(y_train.mean(), color='blue', linestyle='--', alpha=0.7)
    axes[0, 0].axvline(predictions.mean(), color='red', linestyle='--', alpha=0.7)
    
    # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    models = ['LightGBM', 'XGBoost', 'CatBoost']
    scores = [np.mean(scores_info['lgb_scores']), 
              np.mean(scores_info['xgb_scores']), 
              np.mean(scores_info['cat_scores'])]
    
    bars = axes[0, 1].bar(models, scores, color=['lightblue', 'lightgreen', 'lightcoral'])
    axes[0, 1].axhline(y=550, color='red', linestyle='--', label='ç›®æ ‡çº¿(550)')
    axes[0, 1].axhline(y=500, color='green', linestyle='--', label='ç†æƒ³çº¿(500)')
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].set_title('V21å„æ¨¡å‹æ—¶é—´åºåˆ—éªŒè¯æ€§èƒ½')
    axes[0, 1].legend()
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 5,
                       f'{score:.1f}', ha='center', va='bottom')
    
    # 3. é¢„æµ‹å€¼åˆ†å¸ƒ
    axes[1, 0].hist(predictions, bins=50, alpha=0.7, color='red')
    axes[1, 0].set_xlabel('é¢„æµ‹ä»·æ ¼')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_title('V21é¢„æµ‹å€¼åˆ†å¸ƒ')
    
    # 4. V21ä¼˜åŒ–æ€»ç»“
    stats_text = f"""
    V21æŠ—æ³„éœ²å¼ºæ³›åŒ–ç‰ˆæœ¬æ€»ç»“:
    
    è®­ç»ƒé›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(y_train):,}
    å‡å€¼: {y_train.mean():.2f}
    æ ‡å‡†å·®: {y_train.std():.2f}
    èŒƒå›´: {y_train.min():.2f} - {y_train.max():.2f}
    
    é¢„æµ‹é›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(predictions):,}
    å‡å€¼: {predictions.mean():.2f}
    æ ‡å‡†å·®: {predictions.std():.2f}
    èŒƒå›´: {predictions.min():.2f} - {predictions.max():.2f}
    
    æ—¶é—´åºåˆ—éªŒè¯æ€§èƒ½:
    LightGBM: {np.mean(scores_info["lgb_scores"]):.2f}
    XGBoost: {np.mean(scores_info["xgb_scores"]):.2f}
    CatBoost: {np.mean(scores_info["cat_scores"]):.2f}
    
    å…³é”®æ”¹è¿›:
    âœ… ä¸¥æ ¼æ•°æ®æ³„éœ²é˜²æŠ¤
    âœ… ç®€åŒ–ç‰¹å¾å·¥ç¨‹
    âœ… å¼ºæ­£åˆ™åŒ–ç­–ç•¥
    âœ… æ—¶é—´åºåˆ—éªŒè¯
    âœ… ä¿å®ˆé›†æˆç­–ç•¥
    ğŸ¯ ç›®æ ‡: å®é™…MAE < 550
    """
    axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                    fontsize=9, verticalalignment='top', fontfamily='monospace')
    axes[1, 1].set_title('V21ä¼˜åŒ–æ€»ç»“')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_path = os.path.join(analysis_dir, 'modeling_v21_robust_analysis.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"V21åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    plt.show()

def v21_leak_free_robust_optimize():
    """
    V21æŠ—æ³„éœ²å¼ºæ³›åŒ–æ¨¡å‹è®­ç»ƒæµç¨‹
    """
    print("=" * 80)
    print("å¼€å§‹V21æŠ—æ³„éœ²å¼ºæ³›åŒ–æ¨¡å‹è®­ç»ƒ")
    print("ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ï¼Œå¼ºåŒ–æ³›åŒ–èƒ½åŠ›")
    print("ç›®æ ‡ï¼šç¼©å°è®­ç»ƒ-æµ‹è¯•å·®è·ï¼Œå®é™…MAE < 550")
    print("=" * 80)
    
    # æ­¥éª¤1: æ— æ³„éœ²æ•°æ®é¢„å¤„ç†
    print("\næ­¥éª¤1: æ— æ³„éœ²æ•°æ®é¢„å¤„ç†...")
    train_df, test_df = leak_free_data_preprocessing()
    
    # æ­¥éª¤2: é²æ£’ç‰¹å¾å·¥ç¨‹
    print("\næ­¥éª¤2: é²æ£’ç‰¹å¾å·¥ç¨‹...")
    train_df = create_robust_features(train_df)
    test_df = create_robust_features(test_df)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # æ­¥éª¤3: ä¿å®ˆç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤3: ä¿å®ˆç‰¹å¾é€‰æ‹©...")
    selected_features = conservative_feature_selection(X_train, y_train, max_features=40)
    
    X_train_selected = X_train[selected_features].copy()
    X_test_selected = X_test[selected_features].copy()
    
    print(f"é€‰æ‹©åç‰¹å¾æ•°é‡: {len(selected_features)}")
    
    # æ­¥éª¤4: ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤4: ç‰¹å¾ç¼©æ”¾...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            # æ£€æŸ¥å’Œå¤„ç†æ•°å€¼é—®é¢˜
            inf_mask = np.isinf(X_train_selected[col]) | np.isinf(X_test_selected[col])
            if inf_mask.any():
                X_train_selected.loc[inf_mask[inf_mask.index.isin(X_train_selected.index)].index, col] = 0
                X_test_selected.loc[inf_mask[inf_mask.index.isin(X_test_selected.index)].index, col] = 0
            
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # æ­¥éª¤5: å¼ºæ­£åˆ™åŒ–é›†æˆè®­ç»ƒ
    print("\næ­¥éª¤5: å¼ºæ­£åˆ™åŒ–é›†æˆè®­ç»ƒ...")
    lgb_pred, xgb_pred, cat_pred, scores_info = train_regularized_ensemble(
        X_train_selected, y_train, X_test_selected)
    
    # æ­¥éª¤6: ä¿å®ˆé›†æˆ
    print("\næ­¥éª¤6: ä¿å®ˆé›†æˆ...")
    ensemble_pred = conservative_ensemble(lgb_pred, xgb_pred, cat_pred, scores_info)
    
    # æ­¥éª¤7: é²æ£’æ ¡å‡†
    print("\næ­¥éª¤7: é²æ£’æ ¡å‡†...")
    final_predictions = robust_calibration(ensemble_pred, y_train)
    
    # æ­¥éª¤8: åˆ›å»ºåˆ†æå›¾è¡¨
    print("\næ­¥éª¤8: ç”Ÿæˆåˆ†æå›¾è¡¨...")
    create_robust_analysis_plots(y_train, final_predictions, scores_info)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nV21æœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v21_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nV21ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    models_to_save = {}
    
    # æ”¶é›†æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
    if 'lgb_model' in locals():
        models_to_save['lgb'] = lgb_model
    if 'xgb_model' in locals():
        models_to_save['xgb'] = xgb_model
    if 'cat_model' in locals():
        models_to_save['cat'] = cat_model
    if 'rf_model' in locals():
        models_to_save['rf'] = rf_model
    if 'ridge_model' in locals():
        models_to_save['ridge'] = ridge_model
    if 'meta_model' in locals():
        models_to_save['meta'] = meta_model
    
    # ä¿å­˜æ¨¡å‹
    if models_to_save:
        save_models(models_to_save, 'v21')

    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("V21æŠ—æ³„éœ²å¼ºæ³›åŒ–ä¼˜åŒ–æ€»ç»“")
    print("=" * 80)
    print("âœ… ä¸¥æ ¼æ•°æ®æ³„éœ²é˜²æŠ¤ - å®Œå…¨åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¿¡æ¯")
    print("âœ… ç®€åŒ–ç‰¹å¾å·¥ç¨‹ - å‡å°‘ç‰¹å¾æ•°é‡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›")
    print("âœ… å¼ºæ­£åˆ™åŒ–ç­–ç•¥ - å¤§å¹…å¢åŠ L1/L2æ­£åˆ™åŒ–")
    print("âœ… æ—¶é—´åºåˆ—éªŒè¯ - æ›´ä¸¥æ ¼çš„éªŒè¯æ–¹å¼")
    print("âœ… ä¿å®ˆé›†æˆç­–ç•¥ - ç®€å•å¹³å‡ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–")
    print("âœ… é²æ£’æ ¡å‡† - ä¿å®ˆçš„æ ¡å‡†ç­–ç•¥")
    print("ğŸ¯ ç›®æ ‡è¾¾æˆï¼šç¼©å°è®­ç»ƒ-æµ‹è¯•å·®è·ï¼Œå®é™…MAE < 550")
    print("=" * 80)
    
    return final_predictions, scores_info

if __name__ == "__main__":
    test_pred, scores_info = v21_leak_free_robust_optimize()
    print("V21æŠ—æ³„éœ²å¼ºæ³›åŒ–ä¼˜åŒ–å®Œæˆ! æœŸå¾…å¤§å¹…ç¼©å°è®­ç»ƒ-æµ‹è¯•å·®è·! ğŸ¯")
