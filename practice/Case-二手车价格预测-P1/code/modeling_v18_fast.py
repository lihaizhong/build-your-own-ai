"""
V18 Fastç‰ˆæœ¬æ¨¡å‹ - å¿«é€ŸéªŒè¯é©å‘½æ€§ä¼˜åŒ–

åŸºäºV18çš„é©å‘½æ€§ç­–ç•¥ï¼Œå®æ–½ä»¥ä¸‹å¿«é€ŸéªŒè¯ä¼˜åŒ–:
1. æ ¸å¿ƒé©å‘½æ€§ç‰¹å¾å·¥ç¨‹ - ç›®æ ‡ç¼–ç ã€å…³é”®å¤šé¡¹å¼ç‰¹å¾
2. æ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ– - åŸºäºV17æœ€ä½³å‚æ•°çš„å¿«é€Ÿæœç´¢
3. ç®€åŒ–å¤šå±‚Stacking - 3ä¸ªæ ¸å¿ƒæ¨¡å‹çš„å¿«é€Ÿé›†æˆ
4. é«˜çº§æ ¡å‡†èåˆ - åˆ†å¸ƒæ ¡å‡†å’Œè‡ªé€‚åº”æƒé‡
ç›®æ ‡ï¼šå¿«é€ŸéªŒè¯V18ç­–ç•¥çš„æœ‰æ•ˆæ€§
"""

import os
from typing import Tuple, Dict, Any, List, Union, Optional
import pandas as pd  # type: ignore
import numpy as np  # type: ignore
from sklearn.model_selection import KFold, train_test_split, cross_val_score  # type: ignore
from sklearn.ensemble import RandomForestRegressor, IsolationForest  # type: ignore
from sklearn.linear_model import Ridge, LinearRegression, ElasticNet  # type: ignore
from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler  # type: ignore
from sklearn.metrics import mean_absolute_error  # type: ignore
from sklearn.feature_selection import SelectFromModel  # type: ignore
from sklearn.cluster import KMeans  # type: ignore
import lightgbm as lgb  # type: ignore
import xgboost as xgb  # type: ignore
from catboost import CatBoostRegressor  # type: ignore
import matplotlib.pyplot as plt  # type: ignore
from scipy import stats  # type: ignore
import warnings
from sklearn.base import BaseEstimator, TransformerMixin  # type: ignore
import time
import joblib  # type: ignore
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths: str) -> str:
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths: str) -> str:
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

class TargetEncoder(BaseEstimator, TransformerMixin):
    """ç®€åŒ–ç‰ˆç›®æ ‡ç¼–ç å™¨"""
    def __init__(self, smoothing: float = 5.0):
        self.smoothing = smoothing
        self.encodings = {}
        self.global_mean = 0
    
    def fit(self, X: pd.DataFrame, y: pd.Series):
        self.global_mean = y.mean()
        for col in X.columns:
            if X[col].nunique() < 50:  # åªå¯¹ä½åŸºæ•°ç‰¹å¾ç¼–ç 
                temp = pd.concat([X[col], y], axis=1)
                averages = temp.groupby(col)[y.name].agg(['mean', 'count'])
                smooth = (averages['count'] * averages['mean'] + 
                         self.smoothing * self.global_mean) / (averages['count'] + self.smoothing)
                self.encodings[col] = smooth
        return self
    
    def transform(self, X: pd.DataFrame):
        X_encoded = X.copy()
        for col in X.columns:
            if col in self.encodings:
                X_encoded[f'{col}_target_enc'] = X[col].map(self.encodings[col]).fillna(self.global_mean)
        return X_encoded

def load_and_preprocess_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å¿«é€Ÿæ™ºèƒ½æ•°æ®é¢„å¤„ç†
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆå¹¶æ•°æ®è¿›è¡Œç»Ÿä¸€é¢„å¤„ç†
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # 1. å¿«é€Ÿå¼‚å¸¸å€¼æ£€æµ‹
    print("æ‰§è¡Œå¿«é€Ÿå¼‚å¸¸å€¼æ£€æµ‹...")
    numeric_cols = ['power', 'kilometer']
    numeric_cols = [col for col in numeric_cols if col in all_df.columns]
    
    for col in numeric_cols:
        if col in all_df.columns:
            # ä½¿ç”¨åˆ†ä½æ•°æˆªæ–­
            Q1, Q3 = all_df[col].quantile([0.01, 0.99])
            all_df[col] = np.clip(all_df[col], Q1, Q3)
    
    # 2. ç¼ºå¤±å€¼å¤„ç†
    print("å¤„ç†ç¼ºå¤±å€¼...")
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model']
    
    for col in categorical_cols:
        if col in all_df.columns:
            # ç¼ºå¤±å€¼æ ‡è®°
            all_df[f'{col}_missing'] = all_df[col].isnull().astype(int)
            
            # æ™ºèƒ½å¡«å……
            if col == 'model' and 'brand' in all_df.columns:
                for brand in all_df['brand'].unique():
                    brand_mask = all_df['brand'] == brand
                    brand_mode = all_df[brand_mask][col].mode()
                    if len(brand_mode) > 0:
                        all_df.loc[brand_mask & all_df[col].isnull(), col] = brand_mode.iloc[0]
            
            # æœ€ç»ˆä¼—æ•°å¡«å……
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])
    
    # 3. æ—¶é—´ç‰¹å¾å·¥ç¨‹
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(all_df['car_age'].median()).astype(int)
    
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    all_df['reg_quarter'] = all_df['regDate'].dt.quarter.fillna(2).astype(int)
    
    # å­£èŠ‚æ€§ç‰¹å¾
    all_df['reg_season'] = all_df['reg_month'].map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 7:3, 8:3, 9:4, 10:4, 11:4})
    
    all_df.drop(columns=['regDate'], inplace=True)
    
    # 4. å“ç‰Œç»Ÿè®¡ç‰¹å¾
    if 'price' in all_df.columns:
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'std', 'count']).reset_index()
        
        brand_stats['smooth_factor'] = np.where(brand_stats['count'] < 10, 100, 
                                              np.where(brand_stats['count'] < 50, 50, 30))
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     all_df['price'].mean() * brand_stats['smooth_factor']) / 
                                    (brand_stats['count'] + brand_stats['smooth_factor']))
        
        brand_maps = {
            'brand_avg_price': brand_stats.set_index('brand')['smooth_mean'],
            'brand_price_std': brand_stats.set_index('brand')['std'].fillna(0),
            'brand_count': brand_stats.set_index('brand')['count']
        }
        
        for feature_name, brand_map in brand_maps.items():
            all_df[feature_name] = all_df['brand'].map(brand_map).fillna(all_df['price'].mean())
    
    # 5. æ ‡ç­¾ç¼–ç 
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
    
    # 6. æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                median_val = all_df[col].median()
                if not pd.isna(median_val):
                    all_df[col] = all_df[col].fillna(median_val)
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # é‡æ–°åˆ†ç¦»
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_df.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_df.shape}")
    
    return train_df, test_df

def create_core_revolutionary_features(df: pd.DataFrame, is_train: bool = True, target_encoder: Optional[TargetEncoder] = None) -> Tuple[pd.DataFrame, Optional[TargetEncoder]]:
    """
    æ ¸å¿ƒé©å‘½æ€§ç‰¹å¾å·¥ç¨‹ - å¿«é€Ÿç‰ˆ
    """
    df = df.copy()
    
    # 1. ç›®æ ‡ç¼–ç ç‰¹å¾
    if is_train and 'price' in df.columns:
        categorical_for_te = ['brand', 'fuelType', 'gearbox']
        cat_cols_te = [col for col in categorical_for_te if col in df.columns]
        
        if cat_cols_te:
            target_encoder = TargetEncoder(smoothing=5.0)
            target_encoder.fit(df[cat_cols_te], df['price'])
            df = target_encoder.transform(df)
    elif target_encoder is not None:
        categorical_for_te = ['brand', 'fuelType', 'gearbox']
        cat_cols_te = [col for col in categorical_for_te if col in df.columns]
        if cat_cols_te:
            df = target_encoder.transform(df)
    
    # 2. æ ¸å¿ƒå¤šé¡¹å¼ç‰¹å¾
    core_features = ['car_age', 'power', 'kilometer']
    core_features = [col for col in core_features if col in df.columns]
    
    if len(core_features) >= 2:
        for i, col1 in enumerate(core_features):
            for col2 in core_features[i+1:]:
                df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
                df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)
    
    # 3. èšç±»ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(core_features) >= 2:
        scaler_for_clustering = StandardScaler()
        clustering_data = scaler_for_clustering.fit_transform(df[core_features].fillna(df[core_features].median()))
        
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=5)
        cluster_labels = kmeans.fit_predict(clustering_data)
        df['cluster'] = cluster_labels
    
    # 4. ä¸šåŠ¡é€»è¾‘ç‰¹å¾
    if 'power' in df.columns and 'car_age' in df.columns:
        df['power_decay'] = df['power'] / (df['car_age'] + 1)
        df['power_per_year'] = df['power'] / np.maximum(df['car_age'], 1)
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        
        # ä½¿ç”¨å¼ºåº¦åˆ†ç±»
        df['usage_intensity'] = pd.cut(df['km_per_year'], 
                                     bins=[0, 10000, 20000, 30000, float('inf')],
                                     labels=['low', 'medium', 'high', 'very_high'])
        df['usage_intensity'] = df['usage_intensity'].cat.codes
    
    # 5. vç‰¹å¾ç»Ÿè®¡
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)
        df['v_max'] = df[v_cols].max(axis=1)
        df['v_min'] = df[v_cols].min(axis=1)
        df['v_range'] = df['v_max'] - df['v_min']
    
    # 6. æ—¶é—´ç‰¹å¾å¢å¼º
    if 'reg_month' in df.columns:
        df['month_sin'] = np.sin(2 * np.pi * df['reg_month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['reg_month'] / 12)
    
    # 7. å¼‚å¸¸å€¼æ ‡è®°
    if 'power' in df.columns:
        df['power_zero'] = (df['power'] <= 0).astype(int)
        df['power_very_high'] = (df['power'] > df['power'].quantile(0.95)).astype(int)
    
    if 'kilometer' in df.columns:
        df['km_very_high'] = (df['kilometer'] > df['kilometer'].quantile(0.95)).astype(int)
    
    # 8. æ•°æ®æ¸…ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        if col not in ['price', 'SaleID']:
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val if not pd.isna(median_val) else 0)
            
            if df[col].std() > 1e-8:
                mean_val = df[col].mean()
                std_val = df[col].std()
                df[col] = np.clip(df[col], mean_val - 4*std_val, mean_val + 4*std_val)
    
    return df, target_encoder

def fast_hyperparameter_optimization(X_train: pd.DataFrame, y_train: pd.Series) -> Dict[str, Dict[str, Any]]:
    """
    å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ– - åŸºäºV17æœ€ä½³å‚æ•°
    """
    print("å¼€å§‹å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–...")
    
    y_train_log = np.log1p(y_train)
    
    # åŸºäºV17æœ€ä½³å‚æ•°çš„æœç´¢ç©ºé—´
    lgb_param_grid = [
        {'num_leaves': 31, 'max_depth': 8, 'learning_rate': 0.1, 'feature_fraction': 0.8},
        {'num_leaves': 50, 'max_depth': 10, 'learning_rate': 0.05, 'feature_fraction': 0.9},
        {'num_leaves': 40, 'max_depth': 9, 'learning_rate': 0.08, 'feature_fraction': 0.85}
    ]
    
    xgb_param_grid = [
        {'max_depth': 8, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8},
        {'max_depth': 10, 'learning_rate': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.9},
        {'max_depth': 9, 'learning_rate': 0.08, 'subsample': 0.85, 'colsample_bytree': 0.85}
    ]
    
    cat_param_grid = [
        {'depth': 8, 'learning_rate': 0.1, 'l2_leaf_reg': 1.0},
        {'depth': 10, 'learning_rate': 0.05, 'l2_leaf_reg': 3.0},
        {'depth': 9, 'learning_rate': 0.08, 'l2_leaf_reg': 2.0}
    ]
    
    best_params = {}
    
    # LightGBMä¼˜åŒ–
    best_lgb_score = float('inf')
    for params in lgb_param_grid:
        lgb_model = lgb.LGBMRegressor(objective='mae', metric='mae', random_state=42, 
                                     n_estimators=100, **params)
        scores = cross_val_score(lgb_model, X_train, y_train_log, cv=3, 
                               scoring='neg_mean_absolute_error', n_jobs=-1)
        avg_score = -scores.mean()
        
        if avg_score < best_lgb_score:
            best_lgb_score = avg_score
            best_params['lgb'] = params
    
    # XGBoostä¼˜åŒ–
    best_xgb_score = float('inf')
    for params in xgb_param_grid:
        xgb_model = xgb.XGBRegressor(objective='reg:absoluteerror', eval_metric='mae', 
                                    random_state=42, n_estimators=100, **params)
        scores = cross_val_score(xgb_model, X_train, y_train_log, cv=3, 
                               scoring='neg_mean_absolute_error', n_jobs=-1)
        avg_score = -scores.mean()
        
        if avg_score < best_xgb_score:
            best_xgb_score = avg_score
            best_params['xgb'] = params
    
    # CatBoostä¼˜åŒ–
    best_cat_score = float('inf')
    for params in cat_param_grid:
        cat_model = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', 
                                     random_seed=42, iterations=100, verbose=False, **params)
        scores = cross_val_score(cat_model, X_train, y_train_log, cv=3, 
                               scoring='neg_mean_absolute_error', n_jobs=-1)
        avg_score = -scores.mean()
        
        if avg_score < best_cat_score:
            best_cat_score = avg_score
            best_params['cat'] = params
    
    print(f"å¿«é€Ÿä¼˜åŒ–å®Œæˆ - LGB: {best_lgb_score:.4f}, XGB: {best_xgb_score:.4f}, CAT: {best_cat_score:.4f}")
    
    return best_params

def fast_multi_stacking(X_train: pd.DataFrame, y_train: Union[pd.Series, pd.DataFrame], 
                       X_test: pd.DataFrame, best_params: Dict[str, Dict[str, Any]]) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
    """
    å¿«é€Ÿå¤šå±‚Stackingé›†æˆ
    """
    print("æ‰§è¡Œå¿«é€Ÿå¤šå±‚Stackingé›†æˆ...")
    
    y_train_log = np.log1p(y_train)
    
    # åŸºç¡€æ¨¡å‹å‚æ•°
    lgb_params = best_params['lgb']
    lgb_params.update({'objective': 'mae', 'metric': 'mae', 'random_state': 42})
    
    xgb_params = best_params['xgb']
    xgb_params.update({'objective': 'reg:absoluteerror', 'eval_metric': 'mae', 'random_state': 42})
    
    cat_params = best_params['cat']
    cat_params.update({'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': 42, 'verbose': False})
    
    # 3æŠ˜äº¤å‰éªŒè¯
    kfold = KFold(n_splits=3, shuffle=True, random_state=42)
    
    # å…ƒç‰¹å¾
    meta_features_train = np.zeros((len(X_train), 3))
    meta_features_test = np.zeros((len(X_test), 3))
    
    base_models = [
        ('lgb', lgb.LGBMRegressor(**lgb_params, n_estimators=500)),
        ('xgb', xgb.XGBRegressor(**xgb_params, n_estimators=500)),
        ('cat', CatBoostRegressor(**{**cat_params, 'iterations': 500}))
    ]
    
    test_predictions = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):
        print(f"å¿«é€ŸStackingç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]
        
        fold_test_preds = []
        
        for i, (name, model) in enumerate(base_models):
            if name == 'lgb':
                model.fit(X_tr, y_tr_log, eval_set=[(X_val, y_val_log)], 
                         callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(0)])
            elif name == 'xgb':
                model.fit(X_tr, y_tr_log, eval_set=[(X_val, y_val_log)], verbose=False)
            else:  # catboost
                model.fit(X_tr, y_tr_log, eval_set=[(X_val, y_val_log)], early_stopping_rounds=30, verbose=False)
            
            val_pred = np.expm1(model.predict(X_val))
            meta_features_train[val_idx, i] = val_pred
            
            test_pred = np.expm1(model.predict(X_test))
            fold_test_preds.append(test_pred)
        
        test_predictions.append(fold_test_preds)
    
    # å¹³å‡æµ‹è¯•é›†é¢„æµ‹
    for i in range(len(base_models)):
        meta_features_test[:, i] = np.mean([fold[i] for fold in test_predictions], axis=0)
    
    # å…ƒå­¦ä¹ å™¨
    meta_learners = {
        'linear': LinearRegression(),
        'ridge': Ridge(alpha=1.0),
        'lgb_meta': lgb.LGBMRegressor(objective='mae', random_state=42, n_estimators=300)
    }
    
    meta_predictions = {}
    for name, meta_model in meta_learners.items():
        meta_model.fit(meta_features_train, y_train)
        meta_predictions[name] = meta_model.predict(meta_features_test)
    
    # åŠ¨æ€æƒé‡
    meta_scores = {}
    for name, meta_model in meta_learners.items():
        meta_pred_val = meta_model.predict(meta_features_train)
        meta_mae = mean_absolute_error(y_train, meta_pred_val)
        meta_scores[name] = meta_mae
    
    total_inv_score = sum(1/score for score in meta_scores.values())
    dynamic_weights = {name: (1/score) / total_inv_score for name, score in meta_scores.items()}
    
    # åŠ æƒèåˆ
    final_stacking_pred = sum(dynamic_weights[name] * pred for name, pred in meta_predictions.items())
    
    return final_stacking_pred, meta_predictions

def fast_calibration_and_fusion(meta_predictions: Dict[str, np.ndarray], y_train: pd.Series) -> np.ndarray:
    """
    å¿«é€Ÿæ ¡å‡†å’Œèåˆ
    """
    print("æ‰§è¡Œå¿«é€Ÿæ ¡å‡†å’Œèåˆ...")
    
    # åˆ†å¸ƒæ ¡å‡†
    train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])
    
    # ä½¿ç”¨æœ€ä½³é¢„æµ‹è¿›è¡Œæ ¡å‡†
    best_pred_name = list(meta_predictions.keys())[0]  # ç®€åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ª
    base_pred = meta_predictions[best_pred_name]
    
    pred_quantiles = np.percentile(base_pred, [10, 25, 50, 75, 90])
    
    # ç®€åŒ–çš„åˆ†ä½æ•°æ˜ å°„
    calibrated_pred = np.copy(base_pred)
    for i in range(len(train_quantiles)):
        if pred_quantiles[i] > 0:
            scale_factor = train_quantiles[i] / pred_quantiles[i]
            mask = np.abs(base_pred - pred_quantiles[i]) < (pred_quantiles[min(i+1, len(pred_quantiles)-1)] - pred_quantiles[max(i-1, 0)]) / 2
            calibrated_pred[mask] *= scale_factor
    
    # è‡ªé€‚åº”èåˆ
    pred_matrix = np.column_stack(list(meta_predictions.values()))
    pred_weights = np.ones(len(meta_predictions)) / len(meta_predictions)
    
    # åŸºäºæ–¹å·®çš„æƒé‡è°ƒæ•´
    pred_variances = np.var(pred_matrix, axis=0)
    pred_weights = pred_weights / (pred_variances + 1e-6)
    pred_weights = pred_weights / np.sum(pred_weights)
    
    final_pred = sum(weight * pred for weight, pred in zip(pred_weights, meta_predictions.values()))
    
    # æœ€ç»ˆå¤„ç†
    final_pred = np.maximum(final_pred, 0)
    
    train_mean, train_std = y_train.mean(), y_train.std()
    pred_mean, pred_std = final_pred.mean(), final_pred.std()
    
    if pred_std > 0:
        final_pred = (final_pred - pred_mean) * (train_std / pred_std) + train_mean
    
    final_pred = np.maximum(final_pred, 0)
    final_pred = np.clip(final_pred, y_train.quantile(0.01), y_train.quantile(0.99))
    
    print(f"æ ¡å‡†å®Œæˆ - å‡å€¼: {final_pred.mean():.2f}, æ ‡å‡†å·®: {final_pred.std():.2f}")
    
    return final_pred

def v18_fast_optimize() -> Tuple[np.ndarray, Dict[str, Any]]:
    """
    V18 Fastç‰ˆæœ¬ä¼˜åŒ–æµç¨‹
    """
    print("=" * 80)
    print("å¼€å§‹V18 Fastç‰ˆæœ¬ä¼˜åŒ– - å¿«é€ŸéªŒè¯é©å‘½æ€§ç­–ç•¥")
    print("=" * 80)
    
    # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†
    print("æ­¥éª¤1: å¿«é€Ÿæ•°æ®é¢„å¤„ç†...")
    train_df, test_df = load_and_preprocess_data()
    
    # æ­¥éª¤2: æ ¸å¿ƒé©å‘½æ€§ç‰¹å¾å·¥ç¨‹
    print("æ­¥éª¤2: æ ¸å¿ƒé©å‘½æ€§ç‰¹å¾å·¥ç¨‹...")
    train_df, target_encoder = create_core_revolutionary_features(train_df, is_train=True)
    test_df, _ = create_core_revolutionary_features(test_df, is_train=False, target_encoder=target_encoder)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # æ­¥éª¤3: ç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤3: ç‰¹å¾é€‰æ‹©...")
    selector = SelectFromModel(estimator=RandomForestRegressor(n_estimators=50, random_state=42), 
                             threshold='median')
    selector.fit(X_train, y_train)
    
    selected_features = X_train.columns[selector.get_support()].tolist()
    print(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}")
    
    X_train_selected = X_train[selected_features].copy()
    X_test_selected = X_test[selected_features].copy()
    
    # æ­¥éª¤4: ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤4: ç‰¹å¾ç¼©æ”¾...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # æ­¥éª¤5: å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–
    print("\næ­¥éª¤5: å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–...")
    best_params = fast_hyperparameter_optimization(X_train_selected, y_train)
    
    # æ­¥éª¤6: å¿«é€Ÿå¤šå±‚Stacking
    print("\næ­¥éª¤6: å¿«é€Ÿå¤šå±‚Stacking...")
    stacking_pred, meta_predictions = fast_multi_stacking(
        X_train_selected, y_train, X_test_selected, best_params)
    
    # æ­¥éª¤7: å¿«é€Ÿæ ¡å‡†å’Œèåˆ
    print("\næ­¥éª¤7: å¿«é€Ÿæ ¡å‡†å’Œèåˆ...")
    final_predictions = fast_calibration_and_fusion(meta_predictions, y_train)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nV18 Fastæœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    print(f"ä¸­ä½æ•°: {np.median(final_predictions):.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v18_fast_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nV18 Fastç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    models_to_save = {}
    
    # æ”¶é›†æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
    if 'lgb_model' in locals():
        models_to_save['lgb'] = lgb_model
    if 'xgb_model' in locals():
        models_to_save['xgb'] = xgb_model
    if 'cat_model' in locals():
        models_to_save['cat'] = cat_model
    if 'rf_model' in locals():
        models_to_save['rf'] = rf_model
    if 'ridge_model' in locals():
        models_to_save['ridge'] = ridge_model
    if 'meta_model' in locals():
        models_to_save['meta'] = meta_model
    
    # ä¿å­˜æ¨¡å‹
    if models_to_save:
        save_models(models_to_save, 'v18_fast')

    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("V18 Fastç‰ˆæœ¬ä¼˜åŒ–æ€»ç»“")
    print("=" * 80)
    print("âœ… å¿«é€Ÿæ™ºèƒ½æ•°æ®é¢„å¤„ç† - å¼‚å¸¸å€¼æ£€æµ‹+ç¼ºå¤±å€¼å¤„ç†")
    print("âœ… æ ¸å¿ƒé©å‘½æ€§ç‰¹å¾å·¥ç¨‹ - ç›®æ ‡ç¼–ç +å…³é”®å¤šé¡¹å¼+èšç±»ç‰¹å¾")
    print("âœ… å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ– - åŸºäºV17æœ€ä½³å‚æ•°çš„æ™ºèƒ½æœç´¢")
    print("âœ… å¿«é€Ÿå¤šå±‚Stacking - 3ä¸ªæ ¸å¿ƒæ¨¡å‹+åŠ¨æ€æƒé‡")
    print("âœ… å¿«é€Ÿæ ¡å‡†å’Œèåˆ - åˆ†å¸ƒæ ¡å‡†+è‡ªé€‚åº”æƒé‡")
    print("ğŸ¯ å¿«é€ŸéªŒè¯V18é©å‘½æ€§ç­–ç•¥çš„æœ‰æ•ˆæ€§")
    print("=" * 80)
    
    return final_predictions, {
        'best_params': best_params,
        'meta_predictions': meta_predictions,
        'selected_features': selected_features
    }

if __name__ == "__main__":
    test_pred, model_info = v18_fast_optimize()
    print("V18 Fastç‰ˆæœ¬ä¼˜åŒ–å®Œæˆ! éªŒè¯é©å‘½æ€§ç­–ç•¥! ğŸš€")