"""
V27å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬æ¨¡å‹ - ç²¾å‡†çªç ´450åˆ†æ ¸å¿ƒç­–ç•¥æµ‹è¯•

åŸºäºV26çš„497.9590åˆ†åŸºç¡€ï¼Œå¿«é€Ÿæµ‹è¯•æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥:
1. å¢å¼ºç‰¹å¾å·¥ç¨‹ - å¢åŠ é«˜ä»·å€¼ç‰¹å¾åˆ°40ä¸ª
2. ä¼˜åŒ–æ¨¡å‹å‚æ•° - åœ¨V26åŸºç¡€ä¸Šå¾®è°ƒ
3. æ”¹è¿›é›†æˆç­–ç•¥ - åŠ¨æ€æƒé‡è°ƒæ•´
4. ç²¾ç»†åŒ–æ ¡å‡† - åˆ†ä½æ•°æ ¡å‡†

ç›®æ ‡ï¼šå¿«é€ŸéªŒè¯ä¼˜åŒ–æ•ˆæœï¼Œä¸ºå®Œæ•´ç‰ˆæä¾›å‚è€ƒ
"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths):
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

def enhanced_preprocessing():
    """
    å¢å¼ºçš„æ•°æ®é¢„å¤„ç† - åŸºäºV26ä½†å¢åŠ ç›®æ ‡ç¼–ç 
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆå¹¶æ•°æ®è¿›è¡Œé¢„å¤„ç†
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # å¢å¼ºçš„powerå¤„ç†
    if 'power' in all_df.columns:
        all_df['power'] = np.clip(all_df['power'], 0, 600)
        all_df['power_is_zero'] = (all_df['power'] <= 0).astype(int)
        all_df['power_is_low'] = (all_df['power'] <= 100).astype(int)
        all_df['power_is_high'] = (all_df['power'] > 200).astype(int)
        
        # å¤šç§å˜æ¢
        all_df['log_power'] = np.log1p(np.maximum(all_df['power'], 1))
        all_df['sqrt_power'] = np.sqrt(np.maximum(all_df['power'], 0))
    
    # å¢å¼ºçš„åˆ†ç±»ç‰¹å¾å¤„ç†
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model', 'brand']
    for col in categorical_cols:
        if col in all_df.columns:
            # åŸºç¡€å¤„ç†
            all_df[f'{col}_missing'] = (all_df[col].isnull()).astype(int)
            
            # æ™ºèƒ½å¡«å……
            if col == 'model' and 'brand' in all_df.columns:
                for brand in all_df['brand'].unique():
                    brand_mask = all_df['brand'] == brand
                    brand_mode = all_df[brand_mask][col].mode()
                    if len(brand_mode) > 0:
                        all_df.loc[brand_mask & all_df[col].isnull(), col] = brand_mode.iloc[0]
            
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])
            
            # é¢‘ç‡ç¼–ç 
            freq_map = all_df[col].value_counts().to_dict()
            all_df[f'{col}_freq'] = all_df[col].map(freq_map)
    
    # å¢å¼ºçš„æ—¶é—´ç‰¹å¾å·¥ç¨‹
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(0).astype(int)
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    
    # å­£èŠ‚ç‰¹å¾
    all_df['reg_season'] = all_df['reg_month'].map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 7:3, 8:3, 9:4, 10:4, 11:4})
    
    all_df.drop(columns=['regDate'], inplace=True)
    
    # å¢å¼ºçš„å“ç‰Œç»Ÿè®¡ç‰¹å¾
    if 'price' in all_df.columns:
        # å“ç‰Œç»Ÿè®¡
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'std', 'median', 'count']).reset_index()
        global_mean = all_df['price'].mean()
        global_std = all_df['price'].std()
        
        # å¹³æ»‘å¤„ç†
        smooth_factor = 50
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     global_mean * smooth_factor) / (brand_stats['count'] + smooth_factor))
        
        # æ˜ å°„ç‰¹å¾
        brand_mean_map = brand_stats.set_index('brand')['smooth_mean']
        brand_median_map = brand_stats.set_index('brand')['median']
        
        all_df['brand_avg_price'] = all_df['brand'].map(brand_mean_map).fillna(global_mean)
        all_df['brand_median_price'] = all_df['brand'].map(brand_median_map).fillna(global_mean)
        
        # ä»·æ ¼åå·®ç‰¹å¾
        if 'model' in all_df.columns:
            model_stats = all_df.groupby('model')['price'].agg(['mean', 'count']).reset_index()
            smooth_factor = 30
            model_stats['smooth_mean'] = ((model_stats['mean'] * model_stats['count'] + 
                                         global_mean * smooth_factor) / (model_stats['count'] + smooth_factor))
            model_mean_map = model_stats.set_index('model')['smooth_mean']
            all_df['model_avg_price'] = all_df['model'].map(model_mean_map).fillna(global_mean)
            all_df['price_vs_brand'] = all_df['model_avg_price'] - all_df['brand_avg_price']
    
    # æ ‡ç­¾ç¼–ç 
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
    
    # æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                median_val = all_df[col].median()
                if not pd.isna(median_val):
                    all_df[col] = all_df[col].fillna(median_val)
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # é‡æ–°åˆ†ç¦»
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_df.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_df.shape}")
    
    return train_df, test_df

def create_enhanced_features(df):
    """
    åˆ›å»ºå¢å¼ºç‰¹å¾ - å¢åŠ åˆ°40ä¸ªæ ¸å¿ƒç‰¹å¾
    """
    df = df.copy()
    
    # æ ¸å¿ƒä¸šåŠ¡ç‰¹å¾
    if 'power' in df.columns and 'car_age' in df.columns:
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_age_diff'] = df['power'] - df['car_age'] * 10
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        df['km_per_year'] = np.clip(df['km_per_year'], 0, 40000)
        df['km_age_ratio'] = df['kilometer'] / (df['car_age'] + 1)
    
    # ä»·æ ¼ç›¸å…³ç‰¹å¾
    if 'brand_avg_price' in df.columns and 'model_avg_price' in df.columns:
        df['price_ratio_brand_model'] = df['model_avg_price'] / (df['brand_avg_price'] + 1)
        df['price_diff_brand_model'] = df['model_avg_price'] - df['brand_avg_price']
    
    # å¢å¼ºçš„åˆ†æ®µç‰¹å¾
    df['age_segment'] = pd.cut(df['car_age'], bins=[-1, 2, 5, 8, 12, float('inf')], 
                              labels=['new', 'young', 'medium', 'old', 'very_old'])
    df['age_segment'] = df['age_segment'].cat.codes
    
    if 'kilometer' in df.columns:
        df['km_segment'] = pd.cut(df['kilometer'], bins=[-1, 30000, 80000, 120000, 160000, float('inf')], 
                                 labels=['very_low', 'low', 'medium', 'high', 'very_high'])
        df['km_segment'] = df['km_segment'].cat.codes
    
    if 'power' in df.columns:
        df['power_segment'] = pd.cut(df['power'], bins=[-1, 50, 100, 150, 200, 300, float('inf')], 
                                    labels=['very_low', 'low', 'medium', 'high', 'very_high', 'extreme'])
        df['power_segment'] = df['power_segment'].cat.codes
    
    # å¤šç§å˜æ¢ç‰¹å¾
    for col in ['car_age', 'kilometer', 'power']:
        if col in df.columns:
            df[f'log_{col}'] = np.log1p(np.maximum(df[col], 0))
            df[f'sqrt_{col}'] = np.sqrt(np.maximum(df[col], 0))
    
    # vç‰¹å¾ç»Ÿè®¡å¢å¼º
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 5:
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)
        df['v_range'] = df[v_cols].max(axis=1) - df[v_cols].min(axis=1)
        df['v_skew'] = df[v_cols].skew(axis=1).fillna(0)
        df['v_sum'] = df[v_cols].sum(axis=1)
        
        # vç‰¹å¾åˆ†ç»„
        v_positive = [col for col in v_cols if df[col].mean() > 0]
        v_negative = [col for col in v_cols if df[col].mean() <= 0]
        
        if v_positive:
            df['v_pos_mean'] = df[v_positive].mean(axis=1)
        if v_negative:
            df['v_neg_mean'] = df[v_negative].mean(axis=1)
    
    # äº¤äº’ç‰¹å¾
    if 'brand' in df.columns and 'bodyType' in df.columns:
        df['brand_bodyType'] = df['brand'].astype(str) + '_' + df['bodyType'].astype(str)
        le = LabelEncoder()
        df['brand_bodyType'] = le.fit_transform(df['brand_bodyType'])
    
    # æ•°æ®æ¸…ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # å¤„ç†æ— ç©·å¤§å€¼
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # å¡«å……NaNå€¼
        null_count = df[col].isnull().sum()
        if null_count > 0:
            df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)
        
        # å¼‚å¸¸å€¼å¤„ç†
        if col not in ['SaleID', 'price'] and df[col].std() > 1e-8:
            q99 = df[col].quantile(0.99)
            q01 = df[col].quantile(0.01)
            df[col] = np.clip(df[col], q01, q99)
    
    return df

def train_enhanced_ensemble(X_train, y_train, X_test):
    """
    è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹ - ä¼˜åŒ–å‚æ•°ï¼ŒåŠ¨æ€æƒé‡
    """
    print("è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹...")
    
    # å¯¹æ•°å˜æ¢
    y_train_log = np.log1p(y_train)
    
    # 5æŠ˜äº¤å‰éªŒè¯ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰
    y_bins = pd.qcut(y_train, q=5, labels=False)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # V27ä¼˜åŒ–å‚æ•° - å¹³è¡¡æ­£åˆ™åŒ–ä¸æ‹Ÿåˆèƒ½åŠ›
    lgb_params = {
        'objective': 'mae',
        'metric': 'mae',
        'num_leaves': 43,        # é€‚åº¦å¢åŠ å¶å­èŠ‚ç‚¹
        'max_depth': 9,          # é€‚åº¦å¢åŠ æ·±åº¦
        'learning_rate': 0.06,   # é€‚åº¦æé«˜å­¦ä¹ ç‡
        'feature_fraction': 0.85, # é€‚åº¦æé«˜ç‰¹å¾é‡‡æ ·
        'bagging_fraction': 0.85,
        'bagging_freq': 5,
        'lambda_l1': 0.2,        # é€‚åº¦é™ä½L1æ­£åˆ™åŒ–
        'lambda_l2': 0.2,        # é€‚åº¦é™ä½L2æ­£åˆ™åŒ–
        'min_child_samples': 18, # é€‚åº¦é™ä½æœ€å°æ ·æœ¬æ•°
        'random_state': 42,
    }
    
    xgb_params = {
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'max_depth': 9,          # é€‚åº¦å¢åŠ æ·±åº¦
        'learning_rate': 0.06,   # é€‚åº¦æé«˜å­¦ä¹ ç‡
        'subsample': 0.85,       # é€‚åº¦æé«˜é‡‡æ ·
        'colsample_bytree': 0.85,
        'reg_alpha': 0.6,        # é€‚åº¦é™ä½L1æ­£åˆ™åŒ–
        'reg_lambda': 0.6,       # é€‚åº¦é™ä½L2æ­£åˆ™åŒ–
        'min_child_weight': 8,   # é€‚åº¦é™ä½æœ€å°æƒé‡
        'random_state': 42
    }
    
    catboost_params = {
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'depth': 9,              # é€‚åº¦å¢åŠ æ·±åº¦
        'learning_rate': 0.06,   # é€‚åº¦æé«˜å­¦ä¹ ç‡
        'iterations': 1000,      # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰
        'l2_leaf_reg': 2.5,      # é€‚åº¦é™ä½L2æ­£åˆ™åŒ–
        'random_strength': 0.3,  # é€‚åº¦é™ä½éšæœºæ€§
        'random_seed': 42,
        'verbose': False
    }
    
    # å­˜å‚¨é¢„æµ‹
    lgb_predictions = np.zeros(len(X_test))
    xgb_predictions = np.zeros(len(X_test))
    cat_predictions = np.zeros(len(X_test))
    
    lgb_cv_pred = np.zeros(len(X_train))
    xgb_cv_pred = np.zeros(len(X_train))
    cat_cv_pred = np.zeros(len(X_train))
    
    lgb_scores, xgb_scores, cat_scores = [], [], []
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_bins), 1):
        print(f"è®­ç»ƒç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]
        
        # LightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=2000)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(0)])
        
        lgb_predictions += np.expm1(np.array(lgb_model.predict(X_test))) / 5
        lgb_cv_pred[val_idx] = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_cv_pred[val_idx])
        lgb_scores.append(lgb_mae)
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=2000, early_stopping_rounds=100)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        xgb_predictions += np.expm1(xgb_model.predict(X_test)) / 5
        xgb_cv_pred[val_idx] = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_cv_pred[val_idx])
        xgb_scores.append(xgb_mae)
        
        # CatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=100, 
                     verbose=False)
        
        cat_predictions += np.expm1(cat_model.predict(X_test)) / 5
        cat_cv_pred[val_idx] = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_cv_pred[val_idx])
        cat_scores.append(cat_mae)
        
        print(f"  LGB: {lgb_mae:.2f}, XGB: {xgb_mae:.2f}, CAT: {cat_mae:.2f}")
    
    print(f"\néªŒè¯åˆ†æ•°:")
    print(f"  LightGBM: {np.mean(lgb_scores):.2f} (Â±{np.std(lgb_scores):.2f})")
    print(f"  XGBoost: {np.mean(xgb_scores):.2f} (Â±{np.std(xgb_scores):.2f})")
    print(f"  CatBoost: {np.mean(cat_scores):.2f} (Â±{np.std(cat_scores):.2f})")
    
    # åŠ¨æ€æƒé‡è°ƒæ•´
    model_scores = {
        'lgb': np.mean(lgb_scores),
        'xgb': np.mean(xgb_scores),
        'cat': np.mean(cat_scores)
    }
    
    # è®¡ç®—æƒé‡ï¼ˆåˆ†æ•°è¶Šä½æƒé‡è¶Šé«˜ï¼‰
    inv_scores = {model: 1/score for model, score in model_scores.items()}
    total_inv = sum(inv_scores.values())
    weights = {model: inv_score/total_inv for model, inv_score in inv_scores.items()}
    
    # è€ƒè™‘æ¨¡å‹ç¨³å®šæ€§
    stability = {
        'lgb': 1 / (1 + np.std(lgb_scores)),
        'xgb': 1 / (1 + np.std(xgb_scores)),
        'cat': 1 / (1 + np.std(cat_scores))
    }
    
    # ç»¼åˆæƒé‡
    final_weights = {}
    total_weight = 0
    for model in model_scores.keys():
        final_weights[model] = weights[model] * stability[model]
        total_weight += final_weights[model]
    
    for model in final_weights:
        final_weights[model] /= total_weight
    
    # æœ€ç»ˆé›†æˆ
    final_predictions = (
        final_weights['lgb'] * lgb_predictions +
        final_weights['xgb'] * xgb_predictions +
        final_weights['cat'] * cat_predictions
    )
    
    print(f"\næ¨¡å‹æƒé‡:")
    for model, weight in final_weights.items():
        print(f"  {model.upper()}: {weight:.3f}")
    
    return final_predictions, {
        'lgb_scores': lgb_scores,
        'xgb_scores': xgb_scores,
        'cat_scores': cat_scores,
        'weights': final_weights
    }

def enhanced_calibration(predictions, y_train):
    """
    å¢å¼ºæ ¡å‡† - åˆ†ä½æ•°æ ¡å‡†+åŠ¨æ€è°ƒæ•´
    """
    print("æ‰§è¡Œå¢å¼ºæ ¡å‡†...")
    
    # åŸºç¡€ç»Ÿè®¡
    train_mean = y_train.mean()
    train_std = y_train.std()
    pred_mean = predictions.mean()
    pred_std = predictions.std()
    
    print(f"\næ ¡å‡†å‰:")
    print(f"  è®­ç»ƒé›†å‡å€¼: {train_mean:.2f}, æ ‡å‡†å·®: {train_std:.2f}")
    print(f"  é¢„æµ‹å‡å€¼: {pred_mean:.2f}, æ ‡å‡†å·®: {pred_std:.2f}")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šå‡å€¼æ ¡å‡†
    if pred_mean > 0:
        mean_calibration = train_mean / pred_mean
        mean_calibration = np.clip(mean_calibration, 0.8, 1.2)
    else:
        mean_calibration = 1.0
    
    calibrated_predictions = predictions * mean_calibration
    
    # ç¬¬äºŒé˜¶æ®µï¼šæ ‡å‡†å·®æ ¡å‡†
    calib_std = calibrated_predictions.std()
    if calib_std > 0:
        std_calibration = train_std / calib_std
        std_calibration = np.clip(std_calibration, 0.8, 1.2)
    else:
        std_calibration = 1.0
    
    # åº”ç”¨æ ‡å‡†å·®æ ¡å‡†
    calibrated_predictions = (calibrated_predictions - calibrated_predictions.mean()) * std_calibration + train_mean
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†ä½æ•°æ ¡å‡†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    try:
        # ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°è¿›è¡Œè°ƒæ•´
        train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])
        pred_quantiles = np.percentile(calibrated_predictions, [10, 25, 50, 75, 90])
        
        # ç®€åŒ–çš„åˆ†æ®µæ˜ å°„
        def quantile_mapping(x):
            if x <= pred_quantiles[0]:
                return train_quantiles[0]
            elif x <= pred_quantiles[2]:
                ratio = (x - pred_quantiles[0]) / (pred_quantiles[2] - pred_quantiles[0])
                return train_quantiles[0] + ratio * (train_quantiles[2] - train_quantiles[0])
            elif x <= pred_quantiles[4]:
                ratio = (x - pred_quantiles[2]) / (pred_quantiles[4] - pred_quantiles[2])
                return train_quantiles[2] + ratio * (train_quantiles[4] - train_quantiles[2])
            else:
                return train_quantiles[4]
        
        # åº”ç”¨åˆ†ä½æ•°æ ¡å‡†
        quantile_calibrated = np.array([quantile_mapping(x) for x in calibrated_predictions])
        
        # æ··åˆåŸå§‹é¢„æµ‹å’Œåˆ†ä½æ•°æ ¡å‡†ç»“æœ
        calibrated_predictions = 0.8 * calibrated_predictions + 0.2 * quantile_calibrated
        
    except Exception as e:
        print(f"åˆ†ä½æ•°æ ¡å‡†å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ ¡å‡†: {e}")
    
    # ç¡®ä¿é¢„æµ‹å€¼ä¸ºæ­£
    calibrated_predictions = np.maximum(calibrated_predictions, 0)
    
    print(f"\næ ¡å‡†å:")
    print(f"  å‡å€¼æ ¡å‡†å› å­: {mean_calibration:.3f}")
    print(f"  æ ‡å‡†å·®æ ¡å‡†å› å­: {std_calibration:.3f}")
    print(f"  é¢„æµ‹å‡å€¼: {calibrated_predictions.mean():.2f}")
    print(f"  é¢„æµ‹æ ‡å‡†å·®: {calibrated_predictions.std():.2f}")
    
    return calibrated_predictions

def v27_fast_test():
    """
    V27å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬è®­ç»ƒæµç¨‹
    """
    print("=" * 80)
    print("å¼€å§‹V27å¿«é€Ÿæµ‹è¯•æ¨¡å‹è®­ç»ƒ")
    print("åŸºäºV26çš„497.96åˆ†åŸºç¡€ï¼Œå¿«é€Ÿæµ‹è¯•æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥")
    print("ç›®æ ‡ï¼šéªŒè¯ä¼˜åŒ–æ•ˆæœï¼Œä¸ºå®Œæ•´ç‰ˆæä¾›å‚è€ƒ")
    print("=" * 80)
    
    # æ­¥éª¤1: å¢å¼ºæ•°æ®é¢„å¤„ç†
    print("\næ­¥éª¤1: å¢å¼ºæ•°æ®é¢„å¤„ç†...")
    train_df, test_df = enhanced_preprocessing()
    
    # æ­¥éª¤2: åˆ›å»ºå¢å¼ºç‰¹å¾
    print("\næ­¥éª¤2: åˆ›å»ºå¢å¼ºç‰¹å¾...")
    train_df = create_enhanced_features(train_df)
    test_df = create_enhanced_features(test_df)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # æ­¥éª¤3: æ™ºèƒ½ç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤3: æ™ºèƒ½ç‰¹å¾é€‰æ‹©...")
    
    # ä½¿ç”¨ç›¸å…³æ€§ç­›é€‰
    correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)
    top_features = correlations.head(40).index.tolist()
    
    # ç¡®ä¿åŒ…å«é‡è¦çš„ä¸šåŠ¡ç‰¹å¾
    business_features = ['car_age', 'power', 'kilometer', 'brand', 'model']
    for feature in business_features:
        if feature in X_train.columns and feature not in top_features:
            top_features.append(feature)
    
    X_train = X_train[top_features]
    X_test = X_test[top_features]
    
    print(f"ç­›é€‰åç‰¹å¾æ•°é‡: {len(top_features)}")
    
    # æ­¥éª¤4: ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤4: ç‰¹å¾ç¼©æ”¾...")
    
    # å¯¹æ•°å€¼ç‰¹å¾è¿›è¡ŒRobustç¼©æ”¾
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if X_train[col].std() > 1e-8:
            # æ£€æŸ¥æ— ç©·å¤§å€¼
            inf_mask = np.isinf(X_train[col]) | np.isinf(X_test[col])
            if inf_mask.any():
                X_train.loc[inf_mask[inf_mask.index.isin(X_train.index)].index, col] = 0
                X_test.loc[inf_mask[inf_mask.index.isin(X_test.index)].index, col] = 0
            
            X_train[col] = X_train[col].fillna(X_train[col].median())
            X_test[col] = X_test[col].fillna(X_train[col].median())
            
            scaler = RobustScaler()
            X_train[col] = scaler.fit_transform(X_train[[col]])
            X_test[col] = scaler.transform(X_test[[col]])
    
    # æ­¥éª¤5: è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹
    print("\næ­¥éª¤5: è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹...")
    ensemble_pred, scores_info = train_enhanced_ensemble(X_train, y_train, X_test)
    
    # æ­¥éª¤6: å¢å¼ºæ ¡å‡†
    print("\næ­¥éª¤6: å¢å¼ºæ ¡å‡†...")
    final_predictions = enhanced_calibration(ensemble_pred, y_train)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nV27å¿«é€Ÿæµ‹è¯•æœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v27_fast_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nV27å¿«é€Ÿæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("V27å¿«é€Ÿæµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print("âœ… åŸºäºV26çš„497.96åˆ†åŸºç¡€ï¼Œå¿«é€Ÿæµ‹è¯•æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥")
    print("âœ… å¢å¼ºç‰¹å¾å·¥ç¨‹ - 40ä¸ªæ ¸å¿ƒç‰¹å¾ï¼Œç›®æ ‡ç¼–ç ")
    print("âœ… ä¼˜åŒ–æ¨¡å‹å‚æ•° - å¹³è¡¡æ­£åˆ™åŒ–ä¸æ‹Ÿåˆèƒ½åŠ›")
    print("âœ… æ”¹è¿›é›†æˆç­–ç•¥ - åŠ¨æ€æƒé‡è°ƒæ•´")
    print("âœ… ç²¾ç»†åŒ–æ ¡å‡† - åˆ†ä½æ•°æ ¡å‡†+åŠ¨æ€è°ƒæ•´")
    print("âœ… æ™ºèƒ½ç‰¹å¾é€‰æ‹© - åŸºäºç›¸å…³æ€§çš„åŠ¨æ€ç­›é€‰")
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•å®Œæˆï¼ŒæœŸå¾…ä¼˜åŒ–æ•ˆæœ!")
    print("=" * 80)
    
    return final_predictions, scores_info

if __name__ == "__main__":
    test_pred, scores_info = v27_fast_test()
    print("V27å¿«é€Ÿæµ‹è¯•å®Œæˆ! ğŸš€")
