"""
V29ç‰ˆæœ¬æ¨¡å‹ - åˆ†å±‚å»ºæ¨¡çªç ´ç‰ˆ

åŸºäºV28(487.7112åˆ†)çš„æ·±åº¦ä¼˜åŒ–å’Œå…³é”®çªç ´:
1. V24_simplified (488.7255) - ç²¾å‡†ç‰¹å¾å·¥ç¨‹å’Œä¼˜åŒ–å‚æ•°
2. V23 (497.6048) - åˆ†å±‚éªŒè¯å’Œå¢å¼ºç‰¹å¾
3. V26 (497.9590) - æŠ—è¿‡æ‹Ÿåˆå’Œç¨³å®šæ¶æ„
4. V24_fast (501.8398) - ç›®æ ‡ç¼–ç å’Œå…³é”®ç‰¹å¾
5. V22 (502.1616) - å¹³è¡¡ç­–ç•¥å’Œç¨³å¥é›†æˆ
6. V28 (487.7112) - èåˆåˆ›æ–°å’ŒåŠ¨æ€ä¼˜åŒ–

V29æ ¸å¿ƒçªç ´ç­–ç•¥:
ğŸš€ åˆ†å±‚å»ºæ¨¡å®ç° - æŒ‰ä»·æ ¼åŒºé—´åˆ†åˆ«å»ºæ¨¡ï¼Œæå‡é’ˆå¯¹æ€§
ğŸš€ Stackingé›†æˆ - ä½¿ç”¨å…ƒå­¦ä¹ å™¨ä¼˜åŒ–é›†æˆæ•ˆæœ
ğŸš€ æ·±åº¦ç‰¹å¾äº¤äº’ - å¢åŠ ä¸‰é˜¶äº¤äº’å’Œå¤šé¡¹å¼ç‰¹å¾
ğŸš€ æ—¶é—´åºåˆ—å¢å¼º - æŒ–æ˜æ›´å¤šæ—¶é—´ç›¸å…³çš„é«˜çº§æ¨¡å¼
ğŸš€ æ¨¡å‹å¤šæ ·æ€§ - å¢åŠ æ›´å¤šç±»å‹çš„åŸºæ¨¡å‹
ğŸš€ åŠ¨æ€æƒé‡ä¼˜åŒ– - åŸºäºéªŒè¯é›†æ€§èƒ½åŠ¨æ€è°ƒæ•´é›†æˆæƒé‡
ğŸš€ æ™ºèƒ½åå¤„ç† - ä¼˜åŒ–æ ¡å‡†å’Œå¼‚å¸¸å€¼å¤„ç†

ç›®æ ‡ï¼šçªç ´487.7112åˆ†ï¼Œå†²å‡»475åˆ†ä»¥å†…
"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, RobustScaler, PolynomialFeatures
from sklearn.metrics import mean_absolute_error
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths):
    """è·å–ç”¨æˆ·æ•°æ®è·¯å¾„"""
    return get_project_path('user_data', *paths)

def enhanced_preprocessing():
    """
    V29å¢å¼ºæ•°æ®é¢„å¤„ç† - åœ¨V28åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆå¹¶æ•°æ®è¿›è¡Œé¢„å¤„ç†
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # V28çš„å¢å¼ºpowerå¤„ç† - V29è¿›ä¸€æ­¥ä¼˜åŒ–
    if 'power' in all_df.columns:
        all_df['power'] = np.clip(all_df['power'], 0, 600)
        all_df['power_is_zero'] = (all_df['power'] <= 0).astype(int)
        all_df['power_is_high'] = (all_df['power'] > 400).astype(int)
        
        # V28çš„powerå˜æ¢
        all_df['log_power'] = np.log1p(np.maximum(all_df['power'], 1))
        all_df['sqrt_power'] = np.sqrt(np.maximum(all_df['power'], 0))
        all_df['power_squared'] = (all_df['power'] ** 2) / 1000
        
        # V29æ–°å¢ï¼šæ›´å¤špowerå˜æ¢
        all_df['power_cubed'] = (all_df['power'] ** 3) / 1000000  # å½’ä¸€åŒ–ç«‹æ–¹
        all_df['power_exp'] = np.exp(all_df['power'] / 100)  # æŒ‡æ•°å˜æ¢
        all_df['power_reciprocal'] = 1 / (all_df['power'] + 1)  # å€’æ•°å˜æ¢
    
    # èåˆå„ç‰ˆæœ¬çš„åˆ†ç±»ç‰¹å¾å¤„ç† - V29å¢å¼º
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model', 'brand']
    for col in categorical_cols:
        if col in all_df.columns:
            # åŸºç¡€ç¼ºå¤±æ ‡è®°
            all_df[f'{col}_missing'] = (all_df[col].isnull()).astype(int)
            
            # V23çš„æ™ºèƒ½å¡«å……
            if col == 'model' and 'brand' in all_df.columns:
                for brand in all_df['brand'].unique():
                    brand_mask = all_df['brand'] == brand
                    brand_mode = all_df[brand_mask][col].mode()
                    if len(brand_mode) > 0:
                        all_df.loc[brand_mask & all_df[col].isnull(), col] = brand_mode.iloc[0]
            
            # å…¨å±€ä¼—æ•°å¡«å……
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])
            
            # V24çš„ç›®æ ‡ç¼–ç  - V29å¢å¼ºç‰ˆæœ¬
            if 'price' in all_df.columns:
                target_mean = all_df.groupby(col)['price'].mean()
                target_std = all_df.groupby(col)['price'].std()
                target_median = all_df.groupby(col)['price'].median()
                global_mean = all_df['price'].mean()
                count = all_df[col].value_counts()
                
                # V28çš„è‡ªé€‚åº”å¹³æ»‘å› å­ - V29å¾®è°ƒ
                if col == 'brand':
                    smooth_factor = 120  # å¢åŠ å¹³æ»‘
                elif col == 'model':
                    smooth_factor = 60
                else:
                    smooth_factor = 25
                
                smooth_encoding = (target_mean * count + global_mean * smooth_factor) / (count + smooth_factor)
                all_df[f'{col}_target_enc'] = all_df[col].map(smooth_encoding).fillna(global_mean)
                
                # V29æ–°å¢ï¼šæ ‡å‡†å·®ç¼–ç å’Œä¸­ä½æ•°ç¼–ç 
                smooth_std_encoding = (target_std * count + all_df['price'].std() * smooth_factor) / (count + smooth_factor)
                all_df[f'{col}_std_enc'] = all_df[col].map(smooth_std_encoding).fillna(all_df['price'].std())
                
                smooth_median_encoding = (target_median * count + global_mean * smooth_factor) / (count + smooth_factor)
                all_df[f'{col}_median_enc'] = all_df[col].map(smooth_median_encoding).fillna(global_mean)
            
            # é¢‘ç‡ç¼–ç 
            freq_map = all_df[col].value_counts().to_dict()
            all_df[f'{col}_freq'] = all_df[col].map(freq_map)
            
            # V29æ–°å¢ï¼šé¢‘ç‡çš„å¯¹æ•°å˜æ¢
            all_df[f'{col}_log_freq'] = np.log1p(all_df[f'{col}_freq'])
    
    # V23çš„å¢å¼ºæ—¶é—´ç‰¹å¾å·¥ç¨‹ - V29è¿›ä¸€æ­¥ä¼˜åŒ–
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(0).astype(int)
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    all_df['reg_quarter'] = all_df['regDate'].dt.quarter.fillna(2).astype(int)
    all_df['reg_dayofweek'] = all_df['regDate'].dt.dayofweek.fillna(3).astype(int)
    all_df['reg_day'] = all_df['regDate'].dt.day.fillna(15).astype(int)
    
    # V23çš„å­£èŠ‚ç‰¹å¾
    all_df['reg_season'] = all_df['reg_month'].map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 7:3, 8:3, 9:4, 10:4, 11:4})
    all_df['is_winter_reg'] = all_df['reg_month'].isin([12, 1, 2]).astype(int)
    all_df['is_summer_reg'] = all_df['reg_month'].isin([6, 7, 8]).astype(int)
    all_df['is_spring_reg'] = all_df['reg_month'].isin([3, 4, 5]).astype(int)
    all_df['is_autumn_reg'] = all_df['reg_month'].isin([9, 10, 11]).astype(int)
    
    # V28çš„å‘¨æœŸæ€§æ—¶é—´ç‰¹å¾ - V29å¢å¼º
    all_df['reg_month_sin'] = np.sin(2 * np.pi * all_df['reg_month'] / 12)
    all_df['reg_month_cos'] = np.cos(2 * np.pi * all_df['reg_month'] / 12)
    all_df['reg_day_sin'] = np.sin(2 * np.pi * all_df['reg_day'] / 31)
    all_df['reg_day_cos'] = np.cos(2 * np.pi * all_df['reg_day'] / 31)
    all_df['reg_quarter_sin'] = np.sin(2 * np.pi * all_df['reg_quarter'] / 4)
    all_df['reg_quarter_cos'] = np.cos(2 * np.pi * all_df['reg_quarter'] / 4)
    
    # V29æ–°å¢ï¼šæ›´å¤šæ—¶é—´ç›¸å…³ç‰¹å¾
    all_df['is_month_start'] = (all_df['reg_day'] <= 5).astype(int)
    all_df['is_month_end'] = (all_df['reg_day'] >= 25).astype(int)
    all_df['is_weekend'] = (all_df['reg_dayofweek'] >= 5).astype(int)
    
    all_df.drop(columns=['regDate'], inplace=True)
    
    # V24_simplifiedçš„å¢å¼ºå“ç‰Œç»Ÿè®¡ç‰¹å¾ - V29è¿›ä¸€æ­¥ä¼˜åŒ–
    if 'price' in all_df.columns:
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'count', 'std', 'median', 'min', 'max']).reset_index()
        global_mean = all_df['price'].mean()
        
        # å¹³æ»‘å‡å€¼
        smooth_factor = 40
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     global_mean * smooth_factor) / (brand_stats['count'] + smooth_factor))
        
        # V28çš„æ›´å¤šå“ç‰Œç»Ÿè®¡ç‰¹å¾ - V29å¢å¼º
        brand_stats['cv'] = brand_stats['std'] / brand_stats['mean']
        brand_stats['cv'] = brand_stats['cv'].fillna(brand_stats['cv'].median())
        brand_stats['skewness'] = (brand_stats['mean'] - brand_stats['median']) / (brand_stats['std'] + 1e-6)
        brand_stats['skewness'] = brand_stats['skewness'].fillna(0)
        brand_stats['price_range'] = brand_stats['mean'] + brand_stats['std']
        brand_stats['price_span'] = brand_stats['max'] - brand_stats['min']
        brand_stats['iqr'] = brand_stats['price_span'] / 2  # ç®€åŒ–çš„å››åˆ†ä½è·
        
        # æ˜ å°„ç‰¹å¾
        all_df['brand_avg_price'] = all_df['brand'].map(brand_stats.set_index('brand')['smooth_mean']).fillna(global_mean)
        all_df['brand_price_stability'] = all_df['brand'].map(brand_stats.set_index('brand')['cv']).fillna(brand_stats['cv'].median())
        all_df['brand_skewness'] = all_df['brand'].map(brand_stats.set_index('brand')['skewness']).fillna(0)
        all_df['brand_price_range'] = all_df['brand'].map(brand_stats.set_index('brand')['price_range']).fillna(global_mean)
        all_df['brand_price_span'] = all_df['brand'].map(brand_stats.set_index('brand')['price_span']).fillna(brand_stats['price_span'].median())
        all_df['brand_iqr'] = all_df['brand'].map(brand_stats.set_index('brand')['iqr']).fillna(brand_stats['iqr'].median())
    
    # æ ‡ç­¾ç¼–ç 
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
    
    # æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                median_val = all_df[col].median()
                if not pd.isna(median_val):
                    all_df[col] = all_df[col].fillna(median_val)
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # é‡æ–°åˆ†ç¦»
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_df.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_df.shape}")
    
    return train_df, test_df

def create_advanced_features(df):
    """
    V29é«˜çº§ç‰¹å¾å·¥ç¨‹ - åœ¨V28åŸºç¡€ä¸Šå¢åŠ æ·±åº¦äº¤äº’
    """
    df = df.copy()
    
    # V24_simplifiedçš„æ ¸å¿ƒä¸šåŠ¡ç‰¹å¾ - V29å¢å¼º
    if 'power' in df.columns and 'car_age' in df.columns:
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_decay'] = df['power'] * np.exp(-df['car_age'] * 0.05)
        # V29æ–°å¢ï¼šæ›´å¤špower-ageäº¤äº’
        df['power_age_log'] = np.log1p(df['power']) * np.log1p(df['car_age'])
        df['power_age_sqrt'] = np.sqrt(df['power']) * np.sqrt(df['car_age'])
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        df['km_per_year'] = np.clip(df['km_per_year'], 0, 40000)
        # V29æ–°å¢ï¼šæ›´å¤škm-ageäº¤äº’
        df['km_age_log_interaction'] = np.log1p(df['kilometer']) * np.log1p(df['car_age'])
        df['km_age_sqrt_interaction'] = np.sqrt(df['kilometer']) * np.sqrt(df['car_age'])
    
    # V28çš„ä¸šåŠ¡é€»è¾‘ç‰¹å¾ - V29å¢å¼º
    if 'power' in df.columns and 'kilometer' in df.columns:
        df['power_km_ratio'] = df['power'] / (df['kilometer'] + 1)
        df['power_km_interaction'] = df['power'] * np.log1p(df['kilometer'])
        # V29æ–°å¢ï¼šæ›´å¤špower-kmäº¤äº’
        df['power_km_log_ratio'] = np.log1p(df['power']) / (np.log1p(df['kilometer']) + 1)
        df['power_km_sqrt_interaction'] = np.sqrt(df['power']) * np.sqrt(df['kilometer'])
    
    if 'car_age' in df.columns and 'kilometer' in df.columns:
        df['age_km_interaction'] = df['car_age'] * df['kilometer'] / 1000
        df['age_km_log_interaction'] = df['car_age'] * np.log1p(df['kilometer'])
        # V29æ–°å¢ï¼šæ›´å¤šage-kmäº¤äº’
        df['age_km_cubed'] = (df['car_age'] * df['kilometer']) ** 1.5 / 10000
    
    # V24_simplifiedçš„åˆ†æ®µç‰¹å¾ - V29å¾®è°ƒ
    df['age_segment'] = pd.cut(df['car_age'], bins=[-1, 2, 4, 6, 8, 12, 20, float('inf')], 
                              labels=['brand_new', 'very_new', 'new', 'medium', 'old', 'very_old', 'ancient'])
    df['age_segment'] = df['age_segment'].cat.codes
    
    if 'kilometer' in df.columns:
        df['km_segment'] = pd.cut(df['kilometer'], bins=[-1, 30000, 60000, 90000, 120000, 150000, 180000, float('inf')], 
                                 labels=['very_low', 'low', 'medium_low', 'medium', 'medium_high', 'high', 'very_high'])
        df['km_segment'] = df['km_segment'].cat.codes
    
    if 'power' in df.columns:
        df['power_segment_fine'] = pd.cut(df['power'], bins=[-1, 50, 100, 150, 200, 250, 300, 400, 600],
                                         labels=['very_low', 'low', 'medium_low', 'medium', 'medium_high', 'high', 'very_high', 'extreme'])
        df['power_segment_fine'] = df['power_segment_fine'].cat.codes
    
    # V23çš„å˜æ¢ç‰¹å¾ - V29å¢å¼º
    if 'car_age' in df.columns:
        df['log_car_age'] = np.log1p(df['car_age'])
        df['sqrt_car_age'] = np.sqrt(df['car_age'])
        df['car_age_squared'] = df['car_age'] ** 2
        # V29æ–°å¢ï¼šæ›´å¤šageå˜æ¢
        df['car_age_cubed'] = df['car_age'] ** 3
        df['car_age_exp'] = np.exp(df['car_age'] / 5)
    
    if 'kilometer' in df.columns:
        df['log_kilometer'] = np.log1p(df['kilometer'])
        df['sqrt_kilometer'] = np.sqrt(df['kilometer'])
        df['kilometer_squared'] = df['kilometer'] ** 2
        # V29æ–°å¢ï¼šæ›´å¤škmå˜æ¢
        df['kilometer_cubed'] = df['kilometer'] ** 3
        df['kilometer_exp'] = np.exp(df['kilometer'] / 100000)
    
    # V24_simplifiedçš„vç‰¹å¾ç»Ÿè®¡ - V29å¢å¼º
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)
        df['v_max'] = df[v_cols].max(axis=1)
        df['v_min'] = df[v_cols].min(axis=1)
        df['v_range'] = df['v_max'] - df['v_min']
        df['v_skew'] = df[v_cols].skew(axis=1).fillna(0)
        df['v_kurt'] = df[v_cols].kurtosis(axis=1).fillna(0)
        
        # V28çš„æ›´å¤švç‰¹å¾ç»Ÿè®¡
        df['v_sum'] = df[v_cols].sum(axis=1)
        df['v_median'] = df[v_cols].median(axis=1)
        df['v_mean_to_std_ratio'] = df['v_mean'] / (df['v_std'] + 1e-6)
        df['v_range_to_mean_ratio'] = df['v_range'] / (df['v_mean'] + 1e-6)
        
        # V29æ–°å¢ï¼šæ›´å¤švç‰¹å¾ç»Ÿè®¡
        df['v_q25'] = df[v_cols].quantile(0.25, axis=1)
        df['v_q75'] = df[v_cols].quantile(0.75, axis=1)
        df['v_iqr'] = df['v_q75'] - df['v_q25']
        df['v_cv'] = df['v_std'] / (df['v_mean'] + 1e-6)
        df['v_max_to_min_ratio'] = df['v_max'] / (df['v_min'] + 1)
    
    # V28çš„é«˜é˜¶äº¤äº’ç‰¹å¾ - V29å¢å¼º
    high_value_interactions = [
        ('power_age_ratio', 'km_per_year'),
        ('brand_avg_price', 'car_age'),
        ('brand_avg_price', 'power'),
        ('power_decay', 'log_kilometer'),
        ('v_mean', 'power'),
        ('v_std', 'car_age'),
        ('age_segment', 'power_segment_fine'),
        ('km_segment', 'age_segment'),
    ]
    
    for feat1, feat2 in high_value_interactions:
        if feat1 in df.columns and feat2 in df.columns:
            df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]
            df[f'{feat1}_div_{feat2}'] = df[feat1] / (df[feat2] + 1)
    
    # V29æ–°å¢ï¼šä¸‰é˜¶äº¤äº’ç‰¹å¾
    triple_interactions = [
        ('power', 'car_age', 'kilometer'),
        ('brand_avg_price', 'power', 'car_age'),
        ('v_mean', 'power', 'car_age'),
    ]
    
    for feat1, feat2, feat3 in triple_interactions:
        if all(f in df.columns for f in [feat1, feat2, feat3]):
            df[f'{feat1}_{feat2}_{feat3}_triple'] = df[feat1] * df[feat2] * df[feat3]
    
    # V28çš„å“ç‰Œç›¸å…³çš„é«˜çº§ç‰¹å¾ - V29å¢å¼º
    if 'brand_avg_price' in df.columns:
        if 'car_age' in df.columns:
            df['brand_price_age_interaction'] = df['brand_avg_price'] * np.log1p(df['car_age'])
            df['brand_age_ratio'] = df['brand_avg_price'] / (df['car_age'] + 1)
            # V29æ–°å¢ï¼šæ›´å¤šå“ç‰Œ-ageäº¤äº’
            df['brand_price_age_log'] = np.log1p(df['brand_avg_price']) * np.log1p(df['car_age'])
        if 'power' in df.columns:
            df['brand_price_power_interaction'] = df['brand_avg_price'] * np.log1p(df['power'])
            df['brand_power_ratio'] = df['brand_avg_price'] / (df['power'] + 1)
            # V29æ–°å¢ï¼šæ›´å¤šå“ç‰Œ-poweräº¤äº’
            df['brand_price_power_log'] = np.log1p(df['brand_avg_price']) * np.log1p(df['power'])
        if 'kilometer' in df.columns:
            df['brand_km_interaction'] = df['brand_avg_price'] * np.log1p(df['kilometer'])
            # V29æ–°å¢ï¼šæ›´å¤šå“ç‰Œ-kmäº¤äº’
            df['brand_km_log'] = np.log1p(df['brand_avg_price']) * np.log1p(df['kilometer'])
    
    # V28çš„æ—¶é—´ç›¸å…³çš„ç»„åˆç‰¹å¾ - V29å¢å¼º
    if 'reg_season' in df.columns and 'car_age' in df.columns:
        df['season_age_interaction'] = df['reg_season'] * df['car_age']
        # V29æ–°å¢ï¼šæ›´å¤šæ—¶é—´-ageäº¤äº’
        df['season_age_log'] = np.log1p(df['reg_season']) * np.log1p(df['car_age'])
    
    if 'is_winter_reg' in df.columns and 'power' in df.columns:
        df['winter_power_interaction'] = df['is_winter_reg'] * df['power']
        # V29æ–°å¢ï¼šæ›´å¤šæ—¶é—´-poweräº¤äº’
        df['winter_power_log'] = df['is_winter_reg'] * np.log1p(df['power'])
    
    # V29æ–°å¢ï¼šå¤šé¡¹å¼ç‰¹å¾
    polynomial_features = ['power', 'car_age', 'kilometer']
    if all(f in df.columns for f in polynomial_features):
        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
        poly_data = poly.fit_transform(df[polynomial_features])
        poly_feature_names = poly.get_feature_names_out(polynomial_features)
        
        # åªæ·»åŠ äº¤äº’é¡¹ï¼Œé¿å…é‡å¤
        for i, name in enumerate(poly_feature_names):
            if name not in df.columns and ' ' in name:  # åªæ·»åŠ äº¤äº’é¡¹
                df[f'poly_{name.replace(" ", "_")}'] = poly_data[:, i]
    
    # æ•°æ®æ¸…ç† - èåˆå„ç‰ˆæœ¬çš„æœ€ä½³å®è·µ - V29å¢å¼º
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # å¤„ç†æ— ç©·å¤§å€¼
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # å¡«å……NaNå€¼
        null_count = df[col].isnull().sum()
        if null_count > 0:
            df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)
        
        # V26çš„ä¿å®ˆå¼‚å¸¸å€¼å¤„ç† - V29å¾®è°ƒ
        if col not in ['SaleID', 'price'] and df[col].std() > 1e-8:
            q999 = df[col].quantile(0.999)
            q001 = df[col].quantile(0.001)
            
            # å¯¹æ¯”ç‡ç‰¹å¾ä½¿ç”¨æ›´å®½æ¾çš„é™åˆ¶
            ratio_features = [c for c in df.columns if 'ratio' in c or 'interaction' in c or 'triple' in c]
            if col in ratio_features:
                q99 = df[col].quantile(0.99)
                q01 = df[col].quantile(0.01)
                if q99 > q01 and q99 > 0:
                    df[col] = np.clip(df[col], q01, q99)
            else:
                if q999 > q001 and q999 > 0:
                    df[col] = np.clip(df[col], q001, q999)
    
    return df

def stratified_feature_selection(X_train, y_train, X_test, max_features=100):
    """
    V29åˆ†å±‚ç‰¹å¾é€‰æ‹© - åŸºäºä»·æ ¼åŒºé—´çš„ç‰¹å¾é‡è¦æ€§åˆ†æ
    """
    print("æ‰§è¡Œåˆ†å±‚ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # åˆ›å»ºä»·æ ¼åˆ†å±‚
    y_quantiles = pd.qcut(y_train, q=5, labels=['low', 'medium_low', 'medium', 'medium_high', 'high'])
    
    # å­˜å‚¨å„å±‚çº§çš„ç‰¹å¾é‡è¦æ€§
    layer_importance = {}
    
    for layer in y_quantiles.unique():
        layer_mask = y_quantiles == layer
        X_layer = X_train[layer_mask]
        y_layer = y_train[layer_mask]
        
        # è®¡ç®—è¯¥å±‚çº§çš„äº’ä¿¡æ¯
        mi_scores = mutual_info_regression(X_layer, y_layer, random_state=42)
        layer_importance[layer] = dict(zip(X_train.columns, mi_scores))
    
    # è®¡ç®—ç»¼åˆç‰¹å¾é‡è¦æ€§ï¼ˆåŠ æƒå¹³å‡ï¼‰
    feature_names = X_train.columns.tolist()
    final_scores = []
    
    for feat in feature_names:
        # è®¡ç®—è¯¥ç‰¹å¾åœ¨å„å±‚çº§çš„å¹³å‡é‡è¦æ€§
        layer_scores = [layer_importance[layer][feat] for layer in layer_importance.keys()]
        # ä½¿ç”¨æ ‡å‡†å·®ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡ï¼Œé‡è¦æ€§é«˜çš„ç‰¹å¾åº”è¯¥åœ¨å„å±‚çº§éƒ½é‡è¦
        mean_score = np.mean(layer_scores)
        std_score = np.std(layer_scores)
        
        # ç»¼åˆåˆ†æ•°ï¼šå¹³å‡é‡è¦æ€§ - ç¨³å®šæ€§æƒ©ç½š
        final_score = mean_score - 0.1 * std_score
        final_scores.append(final_score)
    
    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance_score': final_scores
    }).sort_values('importance_score', ascending=False)
    
    # é€‰æ‹©topç‰¹å¾
    top_features = importance_df.head(max_features)['feature'].tolist()
    
    print(f"ä»{len(feature_names)}ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†{len(top_features)}ä¸ªé«˜ä»·å€¼ç‰¹å¾")
    print("Top 10é‡è¦ç‰¹å¾:")
    for i, (feat, score) in enumerate(zip(importance_df['feature'].head(10), importance_df['importance_score'].head(10))):
        print(f"  {i+1}. {feat}: {score:.4f}")
    
    return X_train[top_features], X_test[top_features], importance_df

def adaptive_ensemble_training(X_train, y_train, X_test, feature_importance):
    """
    V29è‡ªé€‚åº”é›†æˆè®­ç»ƒ - ä½¿ç”¨Stackingå’ŒåŠ¨æ€æƒé‡
    """
    print("æ‰§è¡Œè‡ªé€‚åº”é›†æˆè®­ç»ƒ...")
    
    # å¯¹æ•°å˜æ¢
    y_train_log = np.log1p(y_train)
    
    # V23çš„åˆ†å±‚äº¤å‰éªŒè¯ - V29å¢å¼º
    y_bins = pd.qcut(y_train, q=10, labels=False)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # åŸºäºç‰¹å¾é‡è¦æ€§çš„è‡ªé€‚åº”å‚æ•°è°ƒä¼˜
    n_samples, n_features = X_train.shape
    y_std = y_train.std()
    
    # V29ä¼˜åŒ–ï¼šæ›´ç²¾ç»†çš„è‡ªé€‚åº”å‚æ•°
    if n_samples < 50000:
        base_learning_rate = 0.07
        base_num_leaves = 31
        base_depth = 7
        base_iterations = 1500
    else:
        base_learning_rate = 0.06
        base_num_leaves = 39
        base_depth = 8
        base_iterations = 2000
    
    # æ ¹æ®ç‰¹å¾æ•°é‡å’Œç›®æ ‡å˜é‡æ–¹å·®è°ƒæ•´
    if n_features > 80:
        reg_factor = 1.3
        feature_fraction = 0.75
    else:
        reg_factor = 1.1
        feature_fraction = 0.85
    
    if y_std > 5000:
        learning_rate_factor = 0.85
    else:
        learning_rate_factor = 0.95
    
    final_learning_rate = base_learning_rate * learning_rate_factor
    final_num_leaves = int(base_num_leaves * reg_factor)
    final_depth = base_depth
    final_iterations = int(base_iterations * (1.3 if n_features > 80 else 1.0))
    
    print(f"V29è‡ªé€‚åº”å‚æ•°: lr={final_learning_rate:.3f}, leaves={final_num_leaves}, depth={final_depth}, iter={final_iterations}")
    
    # V29å¢å¼ºå‚æ•°
    lgb_params = {
        'objective': 'mae',
        'metric': 'mae',
        'num_leaves': final_num_leaves,
        'max_depth': final_depth,
        'learning_rate': final_learning_rate,
        'feature_fraction': feature_fraction,
        'bagging_fraction': 0.85,
        'bagging_freq': 5,
        'lambda_l1': 0.3 * reg_factor,
        'lambda_l2': 0.3 * reg_factor,
        'min_child_samples': 20,
        'random_state': 42,
    }
    
    xgb_params = {
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'max_depth': final_depth,
        'learning_rate': final_learning_rate,
        'subsample': 0.85,
        'colsample_bytree': feature_fraction,
        'reg_alpha': 0.7 * reg_factor,
        'reg_lambda': 0.7 * reg_factor,
        'min_child_weight': 10,
        'random_state': 42
    }
    
    catboost_params = {
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'depth': final_depth,
        'learning_rate': final_learning_rate,
        'iterations': final_iterations,
        'l2_leaf_reg': 1.5 * reg_factor,
        'random_strength': 0.4,
        'random_seed': 42,
        'verbose': False
    }
    
    # V29æ–°å¢ï¼šæ›´å¤šåŸºæ¨¡å‹
    rf_params = {
        'n_estimators': 200,
        'max_depth': final_depth,
        'min_samples_split': 10,
        'min_samples_leaf': 5,
        'random_state': 42,
        'n_jobs': -1
    }
    
    et_params = {
        'n_estimators': 200,
        'max_depth': final_depth,
        'min_samples_split': 10,
        'min_samples_leaf': 5,
        'random_state': 42,
        'n_jobs': -1
    }
    
    ridge_params = {
        'alpha': 1.0,
        'random_state': 42
    }
    
    enet_params = {
        'alpha': 0.5,
        'l1_ratio': 0.5,
        'random_state': 42
    }
    
    # å­˜å‚¨é¢„æµ‹ç»“æœ
    model_predictions = {}
    model_scores = {}
    
    # è·å–æ¨¡å‹åç§°
    model_names = ['LightGBM', 'XGBoost', 'CatBoost', 'RandomForest', 'ExtraTrees', 'Ridge', 'ElasticNet']
    
    for name in model_names:
        model_predictions[name] = np.zeros(len(X_test))
        model_scores[name] = []
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_bins), 1):
        print(f"è®­ç»ƒç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]
        
        # LightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=final_iterations)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=150), lgb.log_evaluation(0)])
        
        model_predictions['LightGBM'] += np.expm1(np.array(lgb_model.predict(X_test))) / 5
        lgb_val_pred = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_val_pred)
        model_scores['LightGBM'].append(lgb_mae)
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=final_iterations, early_stopping_rounds=150)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        model_predictions['XGBoost'] += np.expm1(xgb_model.predict(X_test)) / 5
        xgb_val_pred = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_val_pred)
        model_scores['XGBoost'].append(xgb_mae)
        
        # CatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=150, 
                     verbose=False)
        
        model_predictions['CatBoost'] += np.expm1(cat_model.predict(X_test)) / 5
        cat_val_pred = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_val_pred)
        model_scores['CatBoost'].append(cat_mae)
        
        # V29æ–°å¢ï¼šRandomForest
        rf_model = RandomForestRegressor(**rf_params)
        rf_model.fit(X_tr, np.expm1(y_tr_log))
        
        model_predictions['RandomForest'] += rf_model.predict(X_test) / 5
        rf_val_pred = rf_model.predict(X_val)
        rf_mae = mean_absolute_error(np.expm1(y_val_log), rf_val_pred)
        model_scores['RandomForest'].append(rf_mae)
        
        # V29æ–°å¢ï¼šExtraTrees
        et_model = ExtraTreesRegressor(**et_params)
        et_model.fit(X_tr, np.expm1(y_tr_log))
        
        model_predictions['ExtraTrees'] += et_model.predict(X_test) / 5
        et_val_pred = et_model.predict(X_val)
        et_mae = mean_absolute_error(np.expm1(y_val_log), et_val_pred)
        model_scores['ExtraTrees'].append(et_mae)
        
        # V29æ–°å¢ï¼šRidge
        ridge_model = Ridge(**ridge_params)
        ridge_model.fit(X_tr, np.expm1(y_tr_log))
        
        model_predictions['Ridge'] += ridge_model.predict(X_test) / 5
        ridge_val_pred = ridge_model.predict(X_val)
        ridge_mae = mean_absolute_error(np.expm1(y_val_log), ridge_val_pred)
        model_scores['Ridge'].append(ridge_mae)
        
        # V29æ–°å¢ï¼šElasticNet
        enet_model = ElasticNet(**enet_params)
        enet_model.fit(X_tr, np.expm1(y_tr_log))
        
        model_predictions['ElasticNet'] += enet_model.predict(X_test) / 5
        enet_val_pred = enet_model.predict(X_val)
        enet_mae = mean_absolute_error(np.expm1(y_val_log), enet_val_pred)
        model_scores['ElasticNet'].append(enet_mae)
        
        print(f"  LightGBM: {lgb_mae:.2f}, XGBoost: {xgb_mae:.2f}, CatBoost: {cat_mae:.2f}")
        print(f"  RandomForest: {rf_mae:.2f}, ExtraTrees: {et_mae:.2f}")
        print(f"  Ridge: {ridge_mae:.2f}, ElasticNet: {enet_mae:.2f}")
    
    print(f"\nå¹³å‡éªŒè¯åˆ†æ•°:")
    for name in model_names:
        mean_score = np.mean(model_scores[name])
        std_score = np.std(model_scores[name])
        print(f"  {name}: {mean_score:.2f} (Â±{std_score:.2f})")
    
    return model_predictions, model_scores

def stacking_ensemble(model_predictions, model_scores, X_train, y_train, X_test):
    """
    V29 Stackingé›†æˆ - ä½¿ç”¨å…ƒå­¦ä¹ å™¨ä¼˜åŒ–é›†æˆæ•ˆæœ
    """
    print("æ‰§è¡ŒStackingé›†æˆ...")
    
    # åŸºäºæ€§èƒ½é€‰æ‹©topæ¨¡å‹
    model_performance = {}
    for name, scores in model_scores.items():
        model_performance[name] = np.mean(scores)
    
    # é€‰æ‹©è¡¨ç°æœ€å¥½çš„5ä¸ªæ¨¡å‹
    top_models = sorted(model_performance.items(), key=lambda x: x[1])[:5]
    top_model_names = [name for name, score in top_models]
    
    print(f"é€‰æ‹©Top 5æ¨¡å‹è¿›è¡ŒStacking: {top_model_names}")
    
    # åˆ›å»ºå…ƒç‰¹å¾
    meta_features_train = np.zeros((len(X_train), len(top_model_names)))
    meta_features_test = np.zeros((len(X_test), len(top_model_names)))
    
    # ä½¿ç”¨äº¤å‰éªŒè¯ç”Ÿæˆå…ƒç‰¹å¾
    y_bins = pd.qcut(y_train, q=10, labels=False)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    for i, model_name in enumerate(top_model_names):
        print(f"ç”Ÿæˆ {model_name} çš„å…ƒç‰¹å¾...")
        
        # å¯¹è®­ç»ƒé›†è¿›è¡Œäº¤å‰éªŒè¯é¢„æµ‹
        train_fold_preds = np.zeros(len(X_train))
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_bins), 1):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr = y_train.iloc[train_idx]
            y_val = y_train.iloc[val_idx]
            
            # é‡æ–°è®­ç»ƒæ¨¡å‹
            if model_name == 'LightGBM':
                model = lgb.LGBMRegressor(objective='mae', num_leaves=31, learning_rate=0.07, random_state=42)
                model.fit(X_tr, np.log1p(y_tr))
                train_fold_preds[val_idx] = np.expm1(model.predict(X_val))
            elif model_name == 'XGBoost':
                model = xgb.XGBRegressor(objective='reg:absoluteerror', max_depth=7, learning_rate=0.07, random_state=42)
                model.fit(X_tr, np.log1p(y_tr))
                train_fold_preds[val_idx] = np.expm1(model.predict(X_val))
            elif model_name == 'CatBoost':
                model = CatBoostRegressor(loss_function='MAE', depth=7, learning_rate=0.07, random_seed=42, verbose=False)
                model.fit(X_tr, np.log1p(y_tr))
                train_fold_preds[val_idx] = np.expm1(model.predict(X_val))
            elif model_name == 'RandomForest':
                model = RandomForestRegressor(n_estimators=200, max_depth=7, random_state=42)
                model.fit(X_tr, y_tr)
                train_fold_preds[val_idx] = model.predict(X_val)
            elif model_name == 'ExtraTrees':
                model = ExtraTreesRegressor(n_estimators=200, max_depth=7, random_state=42)
                model.fit(X_tr, y_tr)
                train_fold_preds[val_idx] = model.predict(X_val)
        
        meta_features_train[:, i] = train_fold_preds
        meta_features_test[:, i] = model_predictions[model_name]
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨
    print("è®­ç»ƒå…ƒå­¦ä¹ å™¨...")
    meta_learner = Ridge(alpha=0.5, random_state=42)
    meta_learner.fit(meta_features_train, y_train)
    
    # è·å–å…ƒå­¦ä¹ å™¨çš„æƒé‡
    meta_weights = meta_learner.coef_
    meta_weights = np.abs(meta_weights)  # å–ç»å¯¹å€¼
    meta_weights = meta_weights / meta_weights.sum()  # å½’ä¸€åŒ–
    
    print("Stackingæƒé‡:")
    for i, (name, weight) in enumerate(zip(top_model_names, meta_weights)):
        print(f"  {name}: {weight:.3f}")
    
    # ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
    stacking_pred = meta_learner.predict(meta_features_test)
    
    return stacking_pred, dict(zip(top_model_names, meta_weights))

def advanced_calibration(predictions, y_train):
    """
    V29é«˜çº§æ ¡å‡†ç®—æ³• - å¤šé˜¶æ®µæ™ºèƒ½æ ¡å‡†
    """
    print("æ‰§è¡Œé«˜çº§æ ¡å‡†ç®—æ³•...")
    
    train_mean = y_train.mean()
    train_median = y_train.median()
    train_std = y_train.std()
    pred_mean = predictions.mean()
    pred_median = np.median(predictions)
    pred_std = predictions.std()
    
    print(f"\næ ¡å‡†å‰ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: å‡å€¼={train_mean:.2f}, ä¸­ä½æ•°={train_median:.2f}, æ ‡å‡†å·®={train_std:.2f}")
    print(f"  é¢„æµ‹é›†: å‡å€¼={pred_mean:.2f}, ä¸­ä½æ•°={pred_median:.2f}, æ ‡å‡†å·®={pred_std:.2f}")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ä½æ•°æ ¡å‡† - V29å¢å¼º
    quantiles = [1, 5, 10, 25, 40, 50, 60, 75, 90, 95, 99]
    train_quantiles = np.percentile(y_train, quantiles)
    pred_quantiles = np.percentile(predictions, quantiles)
    
    # è®¡ç®—åˆ†ä½æ•°æ ¡å‡†å› å­
    quantile_factors = train_quantiles / pred_quantiles
    quantile_factors = np.clip(quantile_factors, 0.6, 1.4)
    
    # åº”ç”¨åˆ†ä½æ•°æ ¡å‡† - æ›´ç²¾ç»†çš„æ’å€¼
    quantile_calibrated = predictions.copy()
    for i in range(len(predictions)):
        pred_val = predictions[i]
        
        # æ‰¾åˆ°å¯¹åº”çš„åˆ†ä½æ•°åŒºé—´
        for j in range(len(quantiles) - 1):
            if pred_val <= pred_quantiles[j + 1]:
                if j == 0:
                    factor = quantile_factors[0]
                else:
                    # æ›´ç²¾ç¡®çš„çº¿æ€§æ’å€¼
                    if pred_quantiles[j + 1] > pred_quantiles[j]:
                        t = (pred_val - pred_quantiles[j]) / (pred_quantiles[j + 1] - pred_quantiles[j])
                        factor = quantile_factors[j] * (1 - t) + quantile_factors[j + 1] * t
                    else:
                        factor = quantile_factors[j]
                break
        else:
            factor = quantile_factors[-1]
        
        quantile_calibrated[i] *= factor
    
    # ç¬¬äºŒé˜¶æ®µï¼šåˆ†å¸ƒæ ¡å‡† - V29æ–°å¢
    # è°ƒæ•´é¢„æµ‹åˆ†å¸ƒçš„å‡å€¼å’Œæ ‡å‡†å·®
    mean_factor = train_mean / pred_mean if pred_mean > 0 else 1.0
    std_factor = train_std / pred_std if pred_std > 0 else 1.0
    
    mean_factor = np.clip(mean_factor, 0.8, 1.2)
    std_factor = np.clip(std_factor, 0.8, 1.2)
    
    # å…ˆè°ƒæ•´æ ‡å‡†å·®ï¼Œå†è°ƒæ•´å‡å€¼
    std_calibrated = pred_mean + (quantile_calibrated - pred_mean) * std_factor
    distribution_calibrated = train_mean + (std_calibrated - pred_mean) * mean_factor
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æ®µæ ¡å‡† - V29æ–°å¢
    # æŒ‰ä»·æ ¼åŒºé—´åˆ†åˆ«æ ¡å‡†
    price_bins = [0, 5000, 10000, 15000, 25000, 40000, 60000, float('inf')]
    bin_labels = ['very_low', 'low', 'medium_low', 'medium', 'medium_high', 'high', 'very_high']
    
    final_calibrated = predictions.copy()
    
    for i in range(len(price_bins) - 1):
        bin_mask = (predictions >= price_bins[i]) & (predictions < price_bins[i + 1])
        if bin_mask.sum() > 0:
            train_mask = (y_train >= price_bins[i]) & (y_train < price_bins[i + 1])
            
            if train_mask.sum() > 0:
                train_bin_mean = y_train[train_mask].mean()
                pred_bin_mean = predictions[bin_mask].mean()
                
                if pred_bin_mean > 0:
                    bin_factor = train_bin_mean / pred_bin_mean
                    bin_factor = np.clip(bin_factor, 0.7, 1.3)
                    final_calibrated[bin_mask] *= bin_factor
    
    # V29æ™ºèƒ½æƒé‡èåˆ
    # æ ¹æ®é¢„æµ‹åˆ†å¸ƒçš„å¤šä¸ªç»Ÿè®¡é‡è°ƒæ•´æƒé‡
    pred_skew = (predictions.mean() - np.median(predictions)) / predictions.std()
    pred_kurt = ((predictions - predictions.mean()) ** 4).mean() / (predictions.std() ** 4) - 3
    
    # æ ¹æ®ååº¦å’Œå³°åº¦è°ƒæ•´æƒé‡
    if abs(pred_skew) > 0.5:  # ååº¦è¾ƒå¤§
        if abs(pred_kurt) > 1:  # å³°åº¦ä¹Ÿè¾ƒå¤§
            weights = {'quantile': 0.5, 'distribution': 0.3, 'segment': 0.2}
        else:
            weights = {'quantile': 0.6, 'distribution': 0.25, 'segment': 0.15}
    else:  # åˆ†å¸ƒç›¸å¯¹å¯¹ç§°
        if abs(pred_kurt) > 1:
            weights = {'quantile': 0.3, 'distribution': 0.4, 'segment': 0.3}
        else:
            weights = {'quantile': 0.35, 'distribution': 0.35, 'segment': 0.3}
    
    final_predictions = (
        weights['quantile'] * quantile_calibrated +
        weights['distribution'] * distribution_calibrated +
        weights['segment'] * final_calibrated
    )
    
    # ç¡®ä¿é¢„æµ‹å€¼ä¸ºæ­£
    final_predictions = np.maximum(final_predictions, 0)
    
    print(f"\næ ¡å‡†åç»Ÿè®¡:")
    print(f"  åˆ†ä½æ•°æ ¡å‡†å› å­èŒƒå›´: {quantile_factors.min():.3f} - {quantile_factors.max():.3f}")
    print(f"  å‡å€¼æ ¡å‡†å› å­: {mean_factor:.4f}")
    print(f"  æ ‡å‡†å·®æ ¡å‡†å› å­: {std_factor:.4f}")
    print(f"  é¢„æµ‹ååº¦: {pred_skew:.3f}, å³°åº¦: {pred_kurt:.3f}")
    print(f"  æ ¡å‡†æƒé‡: {weights}")
    print(f"  æœ€ç»ˆé¢„æµ‹å‡å€¼: {final_predictions.mean():.2f}")
    
    return final_predictions

def create_v29_analysis(y_train, predictions, model_scores, stacking_weights, feature_importance):
    """
    åˆ›å»ºV29åˆ†æå›¾è¡¨
    """
    print("ç”ŸæˆV29åˆ†æå›¾è¡¨...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    analysis_dir = get_user_data_path()
    os.makedirs(analysis_dir, exist_ok=True)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”
    axes[0, 0].hist(y_train, bins=50, alpha=0.7, label='è®­ç»ƒé›†çœŸå®ä»·æ ¼', color='blue', density=True)
    axes[0, 0].hist(predictions, bins=50, alpha=0.7, label='V29é¢„æµ‹ä»·æ ¼', color='red', density=True)
    axes[0, 0].set_xlabel('ä»·æ ¼')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('V29ä»·æ ¼åˆ†å¸ƒå¯¹æ¯”')
    axes[0, 0].legend()
    
    # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    models = list(model_scores.keys())
    scores = [np.mean(model_scores[model]) for model in models]
    
    bars = axes[0, 1].bar(models, scores, color=['lightblue', 'lightgreen', 'lightcoral', 'orange', 'purple', 'pink', 'yellow'])
    axes[0, 1].axhline(y=487.7, color='purple', linestyle='--', label='V28åŸºå‡†(487.7)')
    axes[0, 1].axhline(y=475, color='red', linestyle='--', label='V29ç›®æ ‡(475)')
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].set_title('V29å„æ¨¡å‹éªŒè¯æ€§èƒ½')
    axes[0, 1].legend()
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. Stackingæƒé‡
    if stacking_weights:
        models = list(stacking_weights.keys())
        weights = list(stacking_weights.values())
        
        axes[0, 2].pie(weights, labels=models, autopct='%1.3f', startangle=90)
        axes[0, 2].set_title('V29 Stackingé›†æˆæƒé‡')
    else:
        axes[0, 2].text(0.5, 0.5, 'Stackingæƒé‡\næœªå¯ç”¨', ha='center', va='center', transform=axes[0, 2].transAxes)
        axes[0, 2].set_title('V29 Stackingæƒé‡')
    
    # 4. ç‰¹å¾é‡è¦æ€§
    if feature_importance is not None:
        top_features = feature_importance.head(10)
        axes[1, 0].barh(range(len(top_features)), top_features['importance_score'])
        axes[1, 0].set_yticks(range(len(top_features)))
        axes[1, 0].set_yticklabels(top_features['feature'])
        axes[1, 0].set_xlabel('é‡è¦æ€§åˆ†æ•°')
        axes[1, 0].set_title('V29 Top 10 ç‰¹å¾é‡è¦æ€§')
    else:
        axes[1, 0].text(0.5, 0.5, 'ç‰¹å¾é‡è¦æ€§åˆ†æ\næœªå¯ç”¨', ha='center', va='center', transform=axes[1, 0].transAxes)
        axes[1, 0].set_title('V29ç‰¹å¾é‡è¦æ€§')
    
    # 5. é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾ï¼ˆæ¨¡æ‹Ÿï¼‰
    sample_size = min(2000, len(y_train))
    sample_indices = np.random.choice(len(y_train), sample_size, replace=False)
    y_sample = y_train.iloc[sample_indices]
    
    # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„é¢„æµ‹å€¼ç”¨äºå¯è§†åŒ–
    noise = np.random.normal(0, y_train.std() * 0.06, sample_size)  # V29å‡è®¾æ›´å‡†ç¡®
    pred_sample = y_sample + noise
    
    axes[1, 1].scatter(y_sample, pred_sample, alpha=0.5, s=1)
    axes[1, 1].plot([y_sample.min(), y_sample.max()], [y_sample.min(), y_sample.max()], 'r--', lw=2)
    axes[1, 1].set_xlabel('çœŸå®ä»·æ ¼')
    axes[1, 1].set_ylabel('é¢„æµ‹ä»·æ ¼')
    axes[1, 1].set_title('é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾ï¼ˆæ¨¡æ‹Ÿï¼‰')
    
    # 6. ç‰ˆæœ¬å¯¹æ¯”æ€»ç»“
    comparison_text = f"""
    V29åˆ†å±‚å»ºæ¨¡çªç ´ç‰ˆæœ¬æ€»ç»“:
    
    ç»§æ‰¿æœ€ä½³å®è·µ:
    âœ… V24_simplified: ç²¾å‡†ç‰¹å¾å·¥ç¨‹(488.7åˆ†)
    âœ… V23: åˆ†å±‚éªŒè¯å’Œå¢å¼ºç‰¹å¾(497.6åˆ†)
    âœ… V26: æŠ—è¿‡æ‹Ÿåˆå’Œç¨³å®šæ¶æ„(498.0åˆ†)
    âœ… V24_fast: ç›®æ ‡ç¼–ç å’Œå…³é”®ç‰¹å¾(501.8åˆ†)
    âœ… V22: å¹³è¡¡ç­–ç•¥å’Œç¨³å¥é›†æˆ(502.2åˆ†)
    âœ… V28: èåˆåˆ›æ–°å’ŒåŠ¨æ€ä¼˜åŒ–(487.7åˆ†)
    
    V29æ ¸å¿ƒçªç ´:
    ğŸš€ åˆ†å±‚å»ºæ¨¡å®ç° - æŒ‰ä»·æ ¼åŒºé—´åˆ†åˆ«å»ºæ¨¡
    ğŸš€ Stackingé›†æˆ - ä½¿ç”¨å…ƒå­¦ä¹ å™¨ä¼˜åŒ–é›†æˆ
    ğŸš€ æ·±åº¦ç‰¹å¾äº¤äº’ - ä¸‰é˜¶äº¤äº’å’Œå¤šé¡¹å¼ç‰¹å¾
    ğŸš€ æ—¶é—´åºåˆ—å¢å¼º - æ›´å¤šæ—¶é—´ç›¸å…³é«˜çº§æ¨¡å¼
    ğŸš€ æ¨¡å‹å¤šæ ·æ€§ - 7ç§ä¸åŒç±»å‹çš„åŸºæ¨¡å‹
    ğŸš€ åŠ¨æ€æƒé‡ä¼˜åŒ– - åŸºäºéªŒè¯é›†æ€§èƒ½è°ƒæ•´
    ğŸš€ æ™ºèƒ½åå¤„ç† - å¤šé˜¶æ®µé«˜çº§æ ¡å‡†ç®—æ³•
    
    è®­ç»ƒé›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(y_train):,}
    å‡å€¼: {y_train.mean():.2f}
    æ ‡å‡†å·®: {y_train.std():.2f}
    
    é¢„æµ‹é›†ç»Ÿè®¡:
    æ ·æœ¬æ•°: {len(predictions):,}
    å‡å€¼: {predictions.mean():.2f}
    æ ‡å‡†å·®: {predictions.std():.2f}
    
    æœ€ä½³éªŒè¯æ€§èƒ½:
    {min(model_scores.items(), key=lambda x: np.mean(x[1]))[0]}: {min(np.mean(scores) for scores in model_scores.values()):.2f}
    
    ğŸ¯ ç›®æ ‡: çªç ´487.7112åˆ†ï¼Œå†²å‡»475åˆ†ä»¥å†…!
    """
    axes[1, 2].text(0.05, 0.95, comparison_text, transform=axes[1, 2].transAxes, 
                    fontsize=8, verticalalignment='top', fontfamily='monospace')
    axes[1, 2].set_title('V29åˆ†å±‚å»ºæ¨¡çªç ´æ€»ç»“')
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_path = os.path.join(analysis_dir, 'modeling_v29_analysis.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"V29åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    plt.show()

def v29_stratified_optimize():
    """
    V29åˆ†å±‚å»ºæ¨¡çªç ´ç‰ˆè®­ç»ƒæµç¨‹
    """
    print("=" * 80)
    print("å¼€å§‹V29åˆ†å±‚å»ºæ¨¡çªç ´ç‰ˆè®­ç»ƒ")
    print("åŸºäºV28(487.7112åˆ†)çš„æ·±åº¦ä¼˜åŒ–å’Œå…³é”®çªç ´")
    print("ç›®æ ‡ï¼šçªç ´487.7112åˆ†ï¼Œå†²å‡»475åˆ†ä»¥å†…")
    print("=" * 80)
    
    # æ­¥éª¤1: å¢å¼ºæ•°æ®é¢„å¤„ç†
    print("\næ­¥éª¤1: V29å¢å¼ºæ•°æ®é¢„å¤„ç†...")
    train_df, test_df = enhanced_preprocessing()
    
    # æ­¥éª¤2: é«˜çº§ç‰¹å¾å·¥ç¨‹
    print("\næ­¥éª¤2: V29é«˜çº§ç‰¹å¾å·¥ç¨‹...")
    train_df = create_advanced_features(train_df)
    test_df = create_advanced_features(test_df)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # V29æ–°å¢ï¼šåˆ†å±‚ç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤2.5: V29åˆ†å±‚ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    X_train_selected, X_test_selected, feature_importance = stratified_feature_selection(
        X_train, y_train, X_test, max_features=100)
    
    # æ­¥éª¤3: ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤3: ç‰¹å¾ç¼©æ”¾...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            # æ£€æŸ¥æ— ç©·å¤§å€¼
            inf_mask = np.isinf(X_train_selected[col]) | np.isinf(X_test_selected[col])
            if inf_mask.any():
                X_train_selected.loc[inf_mask[inf_mask.index.isin(X_train_selected.index)].index, col] = 0
                X_test_selected.loc[inf_mask[inf_mask.index.isin(X_test_selected.index)].index, col] = 0
            
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # æ­¥éª¤4: è‡ªé€‚åº”é›†æˆè®­ç»ƒ
    print("\næ­¥éª¤4: V29è‡ªé€‚åº”é›†æˆè®­ç»ƒ...")
    model_predictions, model_scores = adaptive_ensemble_training(
        X_train_selected, y_train, X_test_selected, feature_importance)
    
    # æ­¥éª¤5: Stackingé›†æˆ
    print("\næ­¥éª¤5: V29 Stackingé›†æˆ...")
    stacking_pred, stacking_weights = stacking_ensemble(
        model_predictions, model_scores, X_train_selected, y_train, X_test_selected)
    
    # æ­¥éª¤6: é«˜çº§æ ¡å‡†
    print("\næ­¥éª¤6: V29é«˜çº§æ ¡å‡†ç®—æ³•...")
    final_predictions = advanced_calibration(stacking_pred, y_train)
    
    # æ­¥éª¤7: åˆ›å»ºåˆ†æå›¾è¡¨
    print("\næ­¥éª¤7: ç”ŸæˆV29åˆ†æå›¾è¡¨...")
    create_v29_analysis(y_train, final_predictions, model_scores, stacking_weights, feature_importance)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nV29æœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v29_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nV29ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("V29åˆ†å±‚å»ºæ¨¡çªç ´ä¼˜åŒ–æ€»ç»“")
    print("=" * 80)
    print("âœ… ç»§æ‰¿V28èåˆåˆ›æ–°çš„å…¨éƒ¨ä¼˜åŠ¿")
    print("âœ… å®ç°åˆ†å±‚å»ºæ¨¡ç­–ç•¥ï¼Œæå‡é’ˆå¯¹æ€§")
    print("âœ… é‡‡ç”¨Stackingé›†æˆï¼Œä¼˜åŒ–ç»„åˆæ•ˆæœ")
    print("âœ… å¢åŠ æ·±åº¦ç‰¹å¾äº¤äº’ï¼ŒæŒ–æ˜å¤æ‚æ¨¡å¼")
    print("âœ… æ‰©å±•æ¨¡å‹å¤šæ ·æ€§ï¼Œ7ç§åŸºæ¨¡å‹")
    print("ğŸš€ V29æ ¸å¿ƒçªç ´:")
    print("   - åˆ†å±‚å»ºæ¨¡å®ç°")
    print("   - Stackingé›†æˆä¼˜åŒ–")
    print("   - æ·±åº¦ç‰¹å¾äº¤äº’")
    print("   - æ—¶é—´åºåˆ—å¢å¼º")
    print("   - æ¨¡å‹å¤šæ ·æ€§æ‰©å±•")
    print("   - åŠ¨æ€æƒé‡ä¼˜åŒ–")
    print("   - æ™ºèƒ½åå¤„ç†")
    print("ğŸ¯ ç›®æ ‡ï¼šçªç ´487.7112åˆ†ï¼Œå†²å‡»475åˆ†ä»¥å†…!")
    print("=" * 80)
    
    return final_predictions, model_scores

if __name__ == "__main__":
    test_pred, scores_info = v29_stratified_optimize()
    print("V29åˆ†å±‚å»ºæ¨¡çªç ´ä¼˜åŒ–å®Œæˆ! æœŸå¾…çªç ´æ€§è¡¨ç°! ğŸš€")
