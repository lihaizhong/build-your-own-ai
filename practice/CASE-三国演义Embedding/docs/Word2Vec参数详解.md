# Word2Vec训练参数详解

## 参数列表

```python
params: dict[str, int] = {
    'vector_size': 100,      # 向量维度
    'window': 5,              # 上下文窗口大小
    'min_count': 5,           # 最小词频
    'workers': 4,             # 并行数
    'sg': 0,                  # 0=CBOW, 1=Skip-gram
    'epochs': 100,            # 训练轮数
    'seed': 42                # 随机种子
}
```

---

## 1. `vector_size: 100` - 向量维度

**含义**：每个词被表示为多少维的向量

**作用**：
- 决定词向量的表达能力
- 维度越高，能捕获的语义信息越丰富，但计算成本也越高

**选择建议**：
- **小数据集**（< 10万词）：50-100维
- **中等数据集**（10万-100万词）：100-200维
- **大数据集**（> 100万词）：200-500维
- **当前项目**：7579个词，100维是合理的选择

**示例**：
```python
# 每个词用一个100维的向量表示
'曹操' → [0.1, -0.3, 0.8, ..., 0.2]  # 100个浮点数
```

---

## 2. `window: 5` - 上下文窗口大小

**含义**：训练时考虑目标词前后多少个词作为上下文

**作用**：
- 决定模型能捕获多大范围的语义关系
- 窗口越大，能捕获更远的语义关系，但可能引入噪声

**选择建议**：
- **小窗口（2-5）**：关注局部语法关系（如：形容词-名词）
- **中等窗口（5-10）**：平衡局部和全局语义
- **大窗口（10-20）**：关注主题和语义关联

**示例**：
```python
# window=5 时，训练"曹操"这个词
句子: "刘备 关羽 张飞 曹操 诸葛亮 周瑜 孙权"
                    ↑
                目标词
# 上下文: ["刘备", "关羽", "张飞", "诸葛亮", "周瑜"]
```

---

## 3. `min_count: 5` - 最小词频阈值

**含义**：只保留出现次数≥5的词，其他词被丢弃

**作用**：
- 过滤低频词，减少词汇表大小
- 提高模型训练效率
- 避免低频噪声影响词向量质量

**选择建议**：
- **小数据集**：2-5
- **中等数据集**：5-10
- **大数据集**：10-20
- **当前项目**：从402725个词过滤到7579个词（过滤率98%）

**示例**：
```python
# 原始词频统计
'曹操': 1523次  # 保留
'刘备': 1456次  # 保留
'某个生僻字': 3次  # 丢弃（< 5）
```

---

## 4. `workers: 4` - 并行工作线程数

**含义**：使用多少个CPU核心并行训练

**作用**：
- 加速训练过程
- 线程数越多，训练越快（但受限于CPU核心数）

**选择建议**：
- **小数据集**：2-4个线程
- **中等数据集**：4-8个线程
- **大数据集**：8-16个线程
- **不要超过CPU核心数**，否则会降低性能

**示例**：
```python
# 查看CPU核心数
import os
print(f"CPU核心数: {os.cpu_count()}")  # 假设输出: 8
# 建议: workers = min(4, os.cpu_count())
```

---

## 5. `sg: 0` - 训练算法选择

**含义**：选择Word2Vec的训练算法

**两种算法**：
- **`sg=0` (CBOW - Continuous Bag of Words)**：根据上下文预测目标词
- **`sg=1` (Skip-gram)**：根据目标词预测上下文

**对比**：

| 特性 | CBOW (sg=0) | Skip-gram (sg=1) |
|------|-------------|------------------|
| 训练速度 | ⚡ 快（3-4倍） | 🐢 慢 |
| 对低频词效果 | ❌ 较差 | ✅ 更好 |
| 对高频词效果 | ✅ 较好 | ✅ 良好 |
| 语义精度 | 中等 | 更高 |
| 适用场景 | 大数据集、快速训练 | 小数据集、精确语义 |

**选择建议**：
- **大数据集 + 追求速度**：`sg=0` (CBOW)
- **小数据集 + 追求精度**：`sg=1` (Skip-gram)
- **当前项目**：7579个词，中等规模，CBOW是合理选择

**示例**：
```python
# CBOW训练过程
上下文: ["刘备", "关羽", "张飞", "诸葛亮", "周瑜"]
目标: "曹操"
模型学习: 给定上下文，预测中间的词是"曹操"

# Skip-gram训练过程
目标: "曹操"
上下文: ["刘备", "关羽", "张飞", "诸葛亮", "周瑜"]
模型学习: 给定"曹操"，预测周围的词
```

---

## 6. `epochs: 100` - 训练轮数

**含义**：整个数据集被遍历训练的次数

**作用**：
- 决定模型训练的充分程度
- 轮数太少：欠拟合，词向量质量差
- 轮数太多：过拟合，泛化能力下降

**选择建议**：
- **小数据集**：10-50轮
- **中等数据集**：50-100轮
- **大数据集**：100-200轮
- **当前项目**：100轮是合理选择

**示例**：
```python
# 训练过程
Epoch 1/100: 损失 = 2.5
Epoch 2/100: 损失 = 2.3
...
Epoch 50/100: 损失 = 0.8  # 收敛
Epoch 100/100: 损失 = 0.75  # 继续优化
```

---

## 7. `seed: 42` - 随机种子

**含义**：初始化随机数生成器的种子

**作用**：
- 确保结果可重现
- 固定种子后，每次运行结果相同

**选择建议**：
- **任意整数**：42、123、2024等
- **目的**：便于调试和复现实验

**示例**：
```python
# 不设置seed
model1 = Word2Vec(sentences, ...)  # 结果A
model2 = Word2Vec(sentences, ...)  # 结果B（不同）

# 设置seed
model1 = Word2Vec(sentences, seed=42, ...)  # 结果A
model2 = Word2Vec(sentences, seed=42, ...)  # 结果A（相同）
```

---

## 参数调优建议

### 针对不同场景的推荐配置

```python
# 1. 快速原型（小数据集）
quick_params = {
    'vector_size': 50,
    'window': 3,
    'min_count': 2,
    'workers': 2,
    'sg': 0,  # CBOW更快
    'epochs': 20,
    'seed': 42
}

# 2. 平衡性能（中等数据集）- 当前项目使用
balanced_params = {
    'vector_size': 100,
    'window': 5,
    'min_count': 5,
    'workers': 4,
    'sg': 0,
    'epochs': 100,
    'seed': 42
}

# 3. 高精度（小数据集，追求语义质量）
high_quality_params = {
    'vector_size': 200,
    'window': 10,
    'min_count': 3,
    'workers': 4,
    'sg': 1,  # Skip-gram更精确
    'epochs': 200,
    'seed': 42
}

# 4. 大规模训练（大数据集）
large_scale_params = {
    'vector_size': 300,
    'window': 8,
    'min_count': 10,
    'workers': 8,
    'sg': 0,
    'epochs': 150,
    'seed': 42
}
```

### 调优实验方法

```python
# 方法1：网格搜索
vector_sizes = [50, 100, 200]
windows = [3, 5, 10]
best_score = 0

for vs in vector_sizes:
    for w in windows:
        params = {
            'vector_size': vs,
            'window': w,
            'min_count': 5,
            'workers': 4,
            'sg': 0,
            'epochs': 100,
            'seed': 42
        }
        model = Word2Vec(sentences, **params)
        score = evaluate_model(model)  # 评估函数
        if score > best_score:
            best_score = score
            best_params = params
```

---

## 当前项目参数合理性分析

你的参数配置对三国演义数据集来说是**合理且平衡的**：

✅ **优点**：
- `vector_size=100`：适合7579个词的规模
- `window=5`：平衡局部和全局语义
- `min_count=5`：有效过滤噪声词
- `workers=4`：充分利用多核CPU
- `sg=0`：CBOW训练速度快
- `epochs=100`：充分训练
- `seed=42`：结果可重现

🎯 **建议**：
如果想要更精确的语义关系，可以尝试：
```python
params = {
    'vector_size': 150,  # 增加维度
    'window': 8,         # 扩大窗口
    'sg': 1,             # 使用Skip-gram
    'epochs': 150        # 增加训练轮数
}
```

---

## 相关资源

- **项目路径**：`practice/CASE-三国演义Embedding/`
- **训练脚本**：`practice/CASE-三国演义Embedding/code/three_kingdoms.py`
- **演示脚本**：`practice/CASE-三国演义Embedding/code/word2vec_demo.py`
- **模型路径**：`practice/CASE-三国演义Embedding/model/three_kingdoms.model`

---

*文档创建日期：2026年1月19日*
*适用于：三国演义 Word2Vec 词嵌入分析项目*