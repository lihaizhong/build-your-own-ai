# DeepSeek使用与Prompt工程项目 - 技术架构

## 架构概览

DeepSeek使用与Prompt工程项目采用多层次架构设计，支持云端API调用、本地模型部署和服务封装三种主要模式，确保系统的灵活性、可扩展性和高可用性。

```
┌─────────────────────────────────────────────────────────────────┐
│                         应用层 (Application)                      │
├─────────────────────────────────────────────────────────────────┤
│  情感分析  │  Prompt工程  │  内容生成  │  代码辅助  │  客服系统    │
├─────────────────────────────────────────────────────────────────┤
│                         服务层 (Service)                          │
├──────────────┬──────────────┬──────────────┬────────────────────┤
│   FastAPI    │   流式响应    │   客户端封装  │    Prompt管理      │
├──────────────┴──────────────┴──────────────┴────────────────────┤
│                         接口层 (Interface)                        │
├──────────────┬──────────────┬────────────────────────────────────┤
│  DashScope   │   Ollama     │         ModelScope                 │
│   (云端API)   │  (本地运行)   │        (本地推理)                   │
├──────────────┴──────────────┴────────────────────────────────────┤
│                         模型层 (Model)                            │
├─────────────────────────────────────────────────────────────────┤
│     DeepSeek-V3    │    DeepSeek-R1    │   DeepSeek-R1-Distill   │
└─────────────────────────────────────────────────────────────────┘
```

## 核心组件架构

### 1. 多模式调用架构

```python
# 统一调用接口设计
class LLMClient:
    """大模型客户端统一接口"""
    
    def __init__(self, mode: str = "cloud"):
        """
        初始化客户端
        
        Args:
            mode: 调用模式 - cloud(云端)/ollama(本地)/local(本地推理)
        """
        self.mode = mode
        
    def chat(self, messages: list, **kwargs) -> str:
        """统一对话接口"""
        if self.mode == "cloud":
            return self._cloud_chat(messages, **kwargs)
        elif self.mode == "ollama":
            return self._ollama_chat(messages, **kwargs)
        else:
            return self._local_chat(messages, **kwargs)
```

### 2. 数据流架构

```
用户请求
    │
    ▼
┌─────────────┐
│  Prompt构建  │
└─────┬───────┘
      │
      ▼
┌─────────────┐     ┌─────────────┐
│   模式选择   │────▶│  云端API    │
└─────┬───────┘     └──────┬──────┘
      │                    │
      │              ┌─────▼──────┐
      │              │ DashScope  │
      │              └─────┬──────┘
      │                    │
      │              ┌─────▼──────┐
      │              │  Ollama    │
      │              └─────┬──────┘
      │                    │
      └──────────────┬─────┘
                     │
               ┌─────▼──────┐
               │  响应处理   │
               └─────┬──────┘
                     │
               ┌─────▼──────┐
               │  结果返回   │
               └────────────┘
```

## 技术栈详解

### 1. 云端API调用

#### 阿里云DashScope代理
```python
import dashscope
from dashscope.api_entities.dashscope_response import Role

# 配置API密钥
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# 调用模型
response = dashscope.Generation.call(
    model="deepseek-r1",        # 或 "deepseek-v3"
    messages=messages,
    result_format="message",     # 输出格式
    temperature=0.7,            # 随机性控制
)
```

**技术特点：**
- **稳定可靠**: 阿里云基础设施保障
- **多模型支持**: DeepSeek-V3、DeepSeek-R1
- **灵活配置**: 支持温度、最大token等参数
- **消息格式**: 支持多轮对话

### 2. Ollama本地部署

#### Ollama架构
```python
def query_ollama(prompt, model="deepseek-r1:8b"):
    """通过Ollama API调用本地模型"""
    url = f"{ollama_base_url}/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False  # 非流式响应
    }
    response = requests.post(url, json=data)
    return response.json()["response"]
```

**部署架构：**
```
┌─────────────────────────────────────────────────────┐
│                    Ollama 服务                       │
├─────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  │
│  │  API 接口   │  │  模型管理   │  │  推理引擎   │  │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  │
│         │                │                │         │
│         └────────────────┴────────────────┘         │
│                          │                          │
│                    ┌─────▼─────┐                    │
│                    │  GPU/CPU  │                    │
│                    └───────────┘                    │
└─────────────────────────────────────────────────────┘
```

**技术特点：**
- **本地运行**: 数据不出本地，隐私安全
- **多模型支持**: 支持多种开源模型
- **GPU加速**: 自动利用GPU加速推理
- **简单易用**: 一键安装和运行

### 3. 本地模型推理

#### ModelScope + Transformers架构
```python
from modelscope import AutoModelForCausalLM, AutoTokenizer

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 构建输入
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# 生成回复
generated_ids = model.generate(**model_inputs, max_new_tokens=2000)
```

**推理流程：**
```
输入文本
    │
    ▼
┌─────────────┐
│  Tokenizer  │  文本 → Token
└─────┬───────┘
      │
      ▼
┌─────────────┐
│   模型推理   │  Token → Logits
└─────┬───────┘
      │
      ▼
┌─────────────┐
│   解码输出   │  Token → 文本
└─────┬───────┘
      │
      ▼
输出文本
```

### 4. FastAPI服务封装

#### 服务架构
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class ChatRequest(BaseModel):
    prompt: str
    model: str = "deepseek-r1:8b"

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """对话API接口"""
    # 调用后端模型
    response = await call_model(request.prompt, request.model)
    return {"response": response}
```

**服务架构图：**
```
┌─────────────────────────────────────────────────────┐
│                    FastAPI 服务                      │
├─────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  │
│  │  路由处理   │  │  中间件     │  │  异常处理   │  │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  │
│         │                │                │         │
│         └────────────────┴────────────────┘         │
│                          │                          │
│                   ┌─────▼─────┐                     │
│                   │  后端模型  │                     │
│                   │  (Ollama)  │                     │
│                   └───────────┘                     │
└─────────────────────────────────────────────────────┘
```

## Prompt工程设计

### 1. 结构化Prompt设计

```python
# Prompt模板结构
prompt = f"""
# 目标
{instruction}

# 输出格式
{output_format}

# 上下文
{context}

# 用户输入
{input_text}
"""
```

**设计原则：**
1. **明确目标**: 清晰描述任务目标
2. **指定格式**: 明确输出格式要求
3. **提供上下文**: 给出必要的背景信息
4. **示例引导**: 使用Few-shot示例

### 2. Prompt优化技巧

#### 角色设定
```python
system_prompt = "你是一名专业的舆情分析师，擅长判断产品口碑的正负向"
```

#### 任务分解
```python
instruction = """
你的任务是识别用户对手机流量套餐产品的选择条件。
每种流量套餐产品包含三个属性：名称，月费价格，月流量。
根据用户输入，识别用户在上述三种属性上的需求是什么。
"""
```

#### 输出控制
```python
output_format = """
以 JSON 格式输出，包含以下字段：
- name: 套餐名称
- price: 月费价格
- data: 月流量
"""
```

### 3. 高级Prompt技术

#### Chain of Thought (思维链)
```python
cot_prompt = """
请一步一步分析：
1. 首先，理解用户的真实需求
2. 然后，分析问题的核心要点
3. 最后，给出详细的解决方案
"""
```

#### Few-shot Learning
```python
few_shot_prompt = """
示例1：
输入：办个100G的套餐
输出：{"data": "100G"}

示例2：
输入：有没有便宜点的套餐
输出：{"price": "便宜"}

现在请处理：
输入：{user_input}
输出：
"""
```

## 系统配置架构

### 1. 环境变量配置
```bash
# .env 配置文件
DASHSCOPE_API_KEY=your_api_key_here
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. 模型配置
```python
# 模型配置
MODEL_CONFIG = {
    "deepseek-v3": {
        "provider": "dashscope",
        "max_tokens": 4096,
        "temperature": 0.7,
    },
    "deepseek-r1": {
        "provider": "dashscope",
        "max_tokens": 8192,
        "temperature": 0.0,  # 推理模型建议低温度
    },
    "deepseek-r1:8b": {
        "provider": "ollama",
        "max_tokens": 2048,
        "temperature": 0.7,
    }
}
```

### 3. 服务配置
```python
# FastAPI 配置
app = FastAPI(
    title="DeepSeek API Service",
    description="大模型API服务",
    version="1.0.0"
)

# CORS 配置
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

## 安全架构

### 1. API密钥管理
```python
# 安全的密钥管理
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("DASHSCOPE_API_KEY")

# 不要在代码中硬编码密钥
# BAD: api_key = "sk-xxxxx"
```

### 2. 输入验证
```python
from pydantic import BaseModel, validator

class ChatRequest(BaseModel):
    prompt: str
    model: str = "deepseek-r1:8b"
    
    @validator('prompt')
    def validate_prompt(cls, v):
        if len(v) > 10000:
            raise ValueError('Prompt太长')
        return v
```

### 3. 错误处理
```python
from fastapi import HTTPException

@app.post("/api/chat")
async def chat(request: ChatRequest):
    try:
        response = await call_model(request.prompt)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## 扩展架构

### 1. 插件化设计
```python
class ModelPlugin:
    """模型插件基类"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    def chat(self, messages: list) -> str:
        raise NotImplementedError
```

### 2. 多模型支持
```python
class ModelManager:
    """多模型管理器"""
    
    def __init__(self):
        self.models = {
            "deepseek-v3": DeepSeekV3Plugin(),
            "deepseek-r1": DeepSeekR1Plugin(),
            "qwen": QwenPlugin(),
        }
    
    def get_model(self, name: str) -> ModelPlugin:
        return self.models.get(name)
```

---

*最后更新: 2026年2月21日*
*架构版本: v1.0*
*技术负责人: AI系统架构组*
