# DeepSeek使用与Prompt工程项目 - 使用指南

## 环境准备

### 1. 系统要求
- **操作系统**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **Python版本**: 3.11+
- **内存**: 最小8GB，推荐16GB+（本地模型需要更多）
- **GPU**: 本地推理推荐NVIDIA GPU（8GB+显存）

### 2. 依赖安装

#### 方式一：使用项目虚拟环境（推荐）
```bash
# 进入项目根目录
cd build-your-own-ai

# 安装依赖（使用uv包管理器）
uv sync

# 激活虚拟环境
source .venv/bin/activate  # Linux/Mac
# 或
.venv\Scripts\activate     # Windows
```

#### 方式二：独立环境安装
```bash
# 创建虚拟环境
python -m venv .venv

# 激活虚拟环境
source .venv/bin/activate  # Linux/Mac

# 安装依赖
pip install dashscope modelscope torch transformers fastapi uvicorn requests python-dotenv
```

### 3. 环境变量配置

```bash
# 复制环境变量模板
cp .env.example .env

# 编辑 .env 文件
# DASHSCOPE_API_KEY=your_api_key_here
# OLLAMA_BASE_URL=http://localhost:11434
```

## 快速开始

### 1. 云端API调用

#### 基础对话示例
```bash
# 激活虚拟环境
source .venv/bin/activate

# 运行示例
uv run python practice/02-CASE-DeepSeek使用与Prompt工程/code/1-情感分析-Deepseek-阿里代理.py
```

#### 预期输出
```
========================================
DeepSeek API 调用示例
========================================

加载环境变量...
配置API密钥...

发送对话请求...
模型: deepseek-r1

回复内容:
你好！我是DeepSeek开发的大型语言模型...

========================================
情感分析示例
========================================

评论内容: 这款音效特别好 给你意想不到的音质。

分析结果: 正向
```

### 2. Prompt工程实践

#### 运行Prompt工程示例
```bash
uv run python practice/02-CASE-DeepSeek使用与Prompt工程/code/2-提示词工程使用.py
```

#### 预期输出
```
========================================
Prompt工程实践示例
========================================

==== Prompt ====
# 目标
你的任务是识别用户对手机流量套餐产品的选择条件...

# 用户输入
办个100G的套餐。
================

模型输出:
根据用户输入，识别结果如下：
- 月流量需求: 100G

========================================
结构化输出示例
========================================

JSON输出:
{
  "data": "100G"
}
```

### 3. Ollama本地使用

#### 安装Ollama
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 访问 https://ollama.com/download 下载安装
```

#### 启动Ollama服务
```bash
# 启动服务
ollama serve

# 拉取模型
ollama pull deepseek-r1:8b

# 运行示例
uv run python practice/02-CASE-DeepSeek使用与Prompt工程/code/5-ollama使用.py
```

## 详细使用说明

### 1. 云端API使用

#### DashScope API调用
```python
import os
import dashscope
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# 构建消息
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "你好，请介绍一下你自己"}
]

# 调用API
response = dashscope.Generation.call(
    model="deepseek-r1",
    messages=messages,
    result_format="message"
)

# 获取回复
print(response.output.choices[0].message.content)
```

#### 参数配置
| 参数 | 说明 | 默认值 | 建议范围 |
|-----|------|-------|---------|
| model | 模型名称 | - | deepseek-v3/deepseek-r1 |
| temperature | 随机性 | 0.7 | 0.0-1.0 |
| max_tokens | 最大输出 | - | 1024-8192 |
| top_p | 核采样 | 0.9 | 0.5-1.0 |

### 2. Prompt工程技巧

#### 结构化Prompt模板
```python
def build_prompt(instruction, input_text, output_format=None):
    """构建结构化Prompt"""
    prompt = f"""
# 目标
{instruction}

"""
    
    if output_format:
        prompt += f"""
# 输出格式
{output_format}

"""
    
    prompt += f"""
# 用户输入
{input_text}
"""
    return prompt

# 使用示例
instruction = "识别用户对流量套餐的需求"
input_text = "办个100G的套餐"
output_format = "以JSON格式输出"

prompt = build_prompt(instruction, input_text, output_format)
```

#### 角色设定技巧
```python
# 好的角色设定示例
system_prompt = """
你是一名专业的手机流量套餐客服代表，名叫小瓜。

你的职责：
1. 帮助用户选择最合适的流量套餐
2. 提供准确的产品信息
3. 用礼貌、专业的语言与用户交流

可选套餐：
- 经济套餐：月费50元，10G流量
- 畅游套餐：月费180元，100G流量
- 无限套餐：月费300元，1000G流量
- 校园套餐：月费150元，200G流量（仅限在校生）
"""
```

#### 输出格式控制
```python
# JSON输出控制
output_format = """
请严格按照以下JSON格式输出：
{
  "name": "套餐名称",
  "price": "月费价格",
  "data": "月流量"
}

注意：只输出JSON，不要包含其他内容。
"""
```

### 3. 本地模型使用

#### 模型下载
```bash
# 运行模型下载脚本
uv run python practice/02-CASE-DeepSeek使用与Prompt工程/code/4-model-download.py
```

```python
# 模型下载代码
from modelscope import snapshot_download

# 下载模型到本地
snapshot_download(
    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',
    cache_dir="./models"
)
```

#### 本地推理
```python
from modelscope import AutoModelForCausalLM, AutoTokenizer

# 加载模型
model_name = "./models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 构建输入
messages = [
    {"role": "system", "content": "you are a helpful assistant"},
    {"role": "user", "content": "帮我写一个快速排序"}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# 生成回复
generated_ids = model.generate(**model_inputs, max_new_tokens=2000)
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print(response)
```

### 4. Ollama使用

#### 基础调用
```python
import requests

def query_ollama(prompt, model="deepseek-r1:8b"):
    """调用Ollama API"""
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    response = requests.post(url, json=data)
    return response.json()["response"]

# 使用示例
response = query_ollama("你好，请介绍一下你自己")
print(response)
```

#### 流式响应
```python
import json

def query_ollama_stream(prompt, model="deepseek-r1:8b"):
    """流式调用Ollama"""
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    with requests.post(url, json=data, stream=True) as response:
        for line in response.iter_lines(decode_unicode=True):
            if line:
                obj = json.loads(line)
                print(obj.get("response", ""), end="", flush=True)
```

### 5. FastAPI服务

#### 启动服务
```bash
# 启动FastAPI服务
uv run python practice/02-CASE-DeepSeek使用与Prompt工程/code/7-ollama-fastapi.py

# 服务地址: http://localhost:8000
# API文档: http://localhost:8000/docs
```

#### 客户端调用
```python
import requests

# 调用API
response = requests.post(
    "http://localhost:8000/api/chat",
    json={"prompt": "你好，请介绍一下你自己"}
)

print(response.json())
```

## 常见问题解决

### 1. API调用失败

#### 问题现象
```
dashscope.error.AuthenticationError: Invalid API-KEY
```

#### 解决方案
```bash
# 检查环境变量
echo $DASHSCOPE_API_KEY

# 确保.env文件配置正确
cat .env

# 重新加载环境变量
source .env
```

### 2. Ollama连接失败

#### 问题现象
```
requests.exceptions.ConnectionError: Connection refused
```

#### 解决方案
```bash
# 检查Ollama服务状态
ollama list

# 重启Ollama服务
ollama serve

# 检查端口占用
lsof -i :11434
```

### 3. 模型加载失败

#### 问题现象
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

#### 解决方案
```python
# 方案1：使用CPU
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu"
)

# 方案2：使用更小的模型
# 使用 DeepSeek-R1-Distill-Qwen-1.5B 替代 7B 模型

# 方案3：量化加载
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True
)
```

### 4. 输出格式不符合预期

#### 问题现象
模型输出不是期望的JSON格式

#### 解决方案
```python
# 更明确的输出格式描述
output_format = """
请严格按照以下JSON格式输出，不要包含任何其他内容：

{"name": "套餐名称", "price": "价格", "data": "流量"}

示例：
输入：办个100G的套餐
输出：{"name": null, "price": null, "data": "100G"}
"""

# 或者使用 temperature=0 降低随机性
response = dashscope.Generation.call(
    model="deepseek-v3",
    messages=messages,
    temperature=0,  # 降低随机性
    result_format="message"
)
```

## 性能调优

### 1. API调用优化

```python
# 批量请求处理
import asyncio
import aiohttp

async def batch_chat(prompts):
    """批量并发请求"""
    async with aiohttp.ClientSession() as session:
        tasks = [chat_single(session, prompt) for prompt in prompts]
        return await asyncio.gather(*tasks)
```

### 2. 本地推理优化

```python
# 使用GPU加速
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # 半精度
    device_map="auto"
)

# 批量推理
def batch_generate(prompts, batch_size=4):
    results = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        # 批量处理...
    return results
```

### 3. 缓存优化

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_chat(prompt_hash):
    """缓存相同prompt的结果"""
    return call_model(prompt_hash)

def chat_with_cache(prompt):
    """带缓存的对话"""
    prompt_hash = hash(prompt)
    return cached_chat(prompt_hash)
```

---

*最后更新: 2026年2月21日*
*使用指南版本: v1.0*
*技术支持: build-your-own-ai项目团队*
