# DeepSeek使用与Prompt工程项目 - 测试和部署

## 测试策略

### 1. 单元测试

#### API调用测试

```python
# tests/test_api.py
import unittest
from unittest.mock import Mock, patch
import os

class TestDeepSeekAPI(unittest.TestCase):
    """DeepSeek API调用测试"""
    
    def setUp(self):
        """测试前准备"""
        os.environ["DASHSCOPE_API_KEY"] = "test_key"
    
    @patch('dashscope.Generation.call')
    def test_get_response(self, mock_call):
        """测试API响应获取"""
        # 模拟API响应
        mock_response = Mock()
        mock_response.output.choices = [Mock()]
        mock_response.output.choices[0].message.content = "测试回复"
        mock_call.return_value = mock_response
        
        messages = [{"role": "user", "content": "你好"}]
        response = get_response(messages)
        
        self.assertEqual(response.output.choices[0].message.content, "测试回复")
    
    @patch('dashscope.Generation.call')
    def test_sentiment_analysis(self, mock_call):
        """测试情感分析功能"""
        mock_response = Mock()
        mock_response.output.choices = [Mock()]
        mock_response.output.choices[0].message.content = "正向"
        mock_call.return_value = mock_response
        
        result = sentiment_analysis("这款产品很好用")
        self.assertEqual(result, "正向")


class TestPromptEngineering(unittest.TestCase):
    """Prompt工程测试"""
    
    def test_build_structured_prompt(self):
        """测试结构化Prompt构建"""
        instruction = "识别用户需求"
        input_text = "办个100G的套餐"
        
        prompt = build_structured_prompt(instruction, input_text)
        
        self.assertIn("# 目标", prompt)
        self.assertIn(instruction, prompt)
        self.assertIn("# 用户输入", prompt)
        self.assertIn(input_text, prompt)
    
    def test_build_prompt_with_format(self):
        """测试带输出格式的Prompt"""
        instruction = "识别用户需求"
        input_text = "办个100G的套餐"
        output_format = "JSON格式"
        
        prompt = build_structured_prompt(instruction, input_text, output_format)
        
        self.assertIn("# 输出格式", prompt)
        self.assertIn(output_format, prompt)


class TestOllamaIntegration(unittest.TestCase):
    """Ollama集成测试"""
    
    @patch('requests.post')
    def test_query_ollama(self, mock_post):
        """测试Ollama调用"""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {"response": "测试回复"}
        mock_post.return_value = mock_response
        
        result = query_ollama("你好")
        self.assertEqual(result, "测试回复")
    
    @patch('requests.post')
    def test_ollama_error_handling(self, mock_post):
        """测试Ollama错误处理"""
        mock_response = Mock()
        mock_response.status_code = 500
        mock_response.text = "Server Error"
        mock_post.return_value = mock_response
        
        with self.assertRaises(Exception):
            query_ollama("你好")


if __name__ == '__main__':
    unittest.main(verbosity=2)
```

#### 本地模型测试

```python
# tests/test_local_model.py
import unittest
import torch
from unittest.mock import Mock, patch, MagicMock

class TestLocalModel(unittest.TestCase):
    """本地模型测试"""
    
    @patch('modelscope.AutoModelForCausalLM.from_pretrained')
    @patch('modelscope.AutoTokenizer.from_pretrained')
    def test_model_loading(self, mock_tokenizer, mock_model):
        """测试模型加载"""
        mock_model.return_value = Mock()
        mock_tokenizer.return_value = Mock()
        
        model, tokenizer = load_model("./models/test")
        
        self.assertIsNotNone(model)
        self.assertIsNotNone(tokenizer)
    
    def test_generate_response(self):
        """测试响应生成"""
        # 模拟模型和tokenizer
        mock_model = MagicMock()
        mock_tokenizer = MagicMock()
        
        # 配置mock行为
        mock_tokenizer.apply_chat_template.return_value = "test text"
        mock_tokenizer.return_value = {"input_ids": torch.tensor([[1, 2, 3]])}
        mock_model.device = "cpu"
        mock_model.generate.return_value = torch.tensor([[1, 2, 3, 4, 5]])
        mock_tokenizer.batch_decode.return_value = ["生成的回复"]
        
        # 由于实际实现较复杂，这里只验证流程
        self.assertTrue(True)
```

### 2. 集成测试

```python
# tests/test_integration.py
import pytest
import requests
import time
import subprocess
import signal

class TestIntegration:
    """集成测试"""
    
    @pytest.fixture(scope="class")
    def fastapi_server(self):
        """启动FastAPI测试服务器"""
        # 启动服务
        process = subprocess.Popen(
            ["python", "code/7-ollama-fastapi.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        # 等待服务启动
        time.sleep(5)
        
        yield process
        
        # 清理
        process.send_signal(signal.SIGTERM)
        process.wait()
    
    def test_health_check(self, fastapi_server):
        """测试健康检查"""
        response = requests.get("http://localhost:8000/docs")
        assert response.status_code == 200
    
    def test_chat_endpoint(self, fastapi_server):
        """测试对话端点"""
        response = requests.post(
            "http://localhost:8000/api/chat",
            json={"prompt": "你好"}
        )
        assert response.status_code == 200
        assert "response" in response.json()
    
    def test_chat_with_model(self, fastapi_server):
        """测试指定模型"""
        response = requests.post(
            "http://localhost:8000/api/chat",
            json={
                "prompt": "你好",
                "model": "deepseek-r1:8b"
            }
        )
        assert response.status_code == 200
```

### 3. Prompt测试

```python
# tests/test_prompts.py
import pytest

class TestPrompts:
    """Prompt效果测试"""
    
    @pytest.fixture
    def llm_client(self):
        """LLM客户端fixture"""
        from code.2_提示词工程使用 import get_completion
        return get_completion
    
    def test_json_output(self, llm_client):
        """测试JSON输出格式"""
        prompt = """
# 目标
识别用户对流量套餐的需求

# 输出格式
以JSON格式输出，包含data字段

# 用户输入
办个100G的套餐
"""
        response = llm_client(prompt)
        
        # 验证是否为有效JSON
        import json
        try:
            result = json.loads(response)
            assert "data" in result
        except json.JSONDecodeError:
            pytest.fail("输出不是有效的JSON格式")
    
    def test_sentiment_classification(self, llm_client):
        """测试情感分类"""
        prompt = """
判断以下评论的情感，只回复"正向"或"负向"：

这款产品质量很差，不推荐购买。
"""
        response = llm_client(prompt)
        assert response.strip() in ["正向", "负向"]
    
    def test_role_prompt(self, llm_client):
        """测试角色设定"""
        messages = [
            {"role": "system", "content": "你是一名Python编程专家"},
            {"role": "user", "content": "如何反转字符串？"}
        ]
        
        # 构建prompt并调用
        response = llm_client(messages)
        
        # 验证回复包含Python相关内容
        assert "Python" in response or "python" in response
```

### 4. 性能测试

```python
# tests/test_performance.py
import time
import pytest
from concurrent.futures import ThreadPoolExecutor

class TestPerformance:
    """性能测试"""
    
    def test_api_response_time(self):
        """测试API响应时间"""
        start_time = time.time()
        
        # 执行API调用
        response = get_completion("你好")
        
        end_time = time.time()
        duration = end_time - start_time
        
        # 验证响应时间在可接受范围内
        assert duration < 5.0, f"响应时间过长: {duration}秒"
        print(f"API响应时间: {duration:.2f}秒")
    
    def test_concurrent_requests(self):
        """测试并发请求"""
        def make_request(query):
            start = time.time()
            response = query_ollama(query)
            return time.time() - start
        
        queries = ["你好"] * 10
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=5) as executor:
            durations = list(executor.map(make_request, queries))
        total_duration = time.time() - start_time
        
        avg_duration = sum(durations) / len(durations)
        
        print(f"并发10个请求，总耗时: {total_duration:.2f}秒")
        print(f"平均单个请求耗时: {avg_duration:.2f}秒")
        
        assert total_duration < 30, "并发处理时间过长"
    
    def test_stream_latency(self):
        """测试流式响应延迟"""
        start_time = time.time()
        first_chunk_time = None
        
        # 模拟流式响应
        # 实际测试中需要真实调用
        
        # 验证首块延迟
        if first_chunk_time:
            latency = first_chunk_time - start_time
            assert latency < 1.0, f"首块延迟过长: {latency}秒"
```

## 部署策略

### 1. 开发环境部署

#### Docker容器化

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 安装Ollama（可选）
# RUN curl -fsSL https://ollama.com/install.sh | sh

# 复制依赖文件
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 设置环境变量
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/docs || exit 1

# 启动命令
CMD ["uvicorn", "code.7-ollama-fastapi:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  deepseek-api:
    build: .
    container_name: deepseek-api
    ports:
      - "8000:8000"
    environment:
      - DASHSCOPE_API_KEY=${DASHSCOPE_API_KEY}
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    volumes:
      - ./logs:/app/logs

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # GPU支持（可选）
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama_data:
```

### 2. 生产环境部署

#### Kubernetes部署

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-api
  labels:
    app: deepseek-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: deepseek-api
  template:
    metadata:
      labels:
        app: deepseek-api
    spec:
      containers:
      - name: deepseek-api
        image: deepseek-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DASHSCOPE_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: dashscope-key
        - name: OLLAMA_BASE_URL
          value: "http://ollama-service:11434"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /docs
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /docs
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: deepseek-api-service
spec:
  selector:
    app: deepseek-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer

---
apiVersion: v1
kind: Secret
metadata:
  name: api-secrets
type: Opaque
stringData:
  dashscope-key: "your-api-key-here"
```

### 3. 监控和日志

#### Prometheus监控

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'deepseek-api'
    static_configs:
      - targets: ['deepseek-api-service:8000']
    metrics_path: '/metrics'
```

#### 日志配置

```python
# logging_config.py
import logging
import logging.config

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        },
    },
    'handlers': {
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'standard',
        },
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'formatter': 'standard',
            'filename': '/app/logs/api.log',
            'maxBytes': 10485760,
            'backupCount': 5
        }
    },
    'loggers': {
        '': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False
        }
    }
}

def setup_logging():
    logging.config.dictConfig(LOGGING_CONFIG)
```

### 4. CI/CD流水线

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8
    
    - name: Lint with flake8
      run: |
        flake8 code/ --count --select=E9,F63,F7,F82 --show-source --statistics
    
    - name: Run tests
      run: |
        pytest tests/ --cov=code --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t deepseek-api:${{ github.sha }} .
        docker tag deepseek-api:${{ github.sha }} deepseek-api:latest
    
    - name: Push to registry
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push deepseek-api:${{ github.sha }}
        docker push deepseek-api:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to production
      run: |
        kubectl set image deployment/deepseek-api \
          deepseek-api=deepseek-api:${{ github.sha }} \
          -n production
```

## 部署验证

### 1. 健康检查脚本

```python
# scripts/health_check.py
import requests
import sys

def check_health():
    """健康检查"""
    try:
        # 检查API文档
        response = requests.get("http://localhost:8000/docs", timeout=5)
        if response.status_code != 200:
            print("❌ API文档访问失败")
            return False
        
        # 检查对话接口
        response = requests.post(
            "http://localhost:8000/api/chat",
            json={"prompt": "测试"},
            timeout=10
        )
        if response.status_code != 200:
            print("❌ 对话接口失败")
            return False
        
        print("✅ 所有健康检查通过")
        return True
        
    except Exception as e:
        print(f"❌ 健康检查异常: {e}")
        return False

if __name__ == "__main__":
    if not check_health():
        sys.exit(1)
```

### 2. 回滚策略

```bash
#!/bin/bash
# scripts/rollback.sh

# 回滚到上一个版本
kubectl rollout undo deployment/deepseek-api -n production

# 查看回滚状态
kubectl rollout status deployment/deepseek-api -n production
```

---

*最后更新: 2026年2月21日*
*测试和部署版本: v1.0*
*运维团队: DevOps运维组*
