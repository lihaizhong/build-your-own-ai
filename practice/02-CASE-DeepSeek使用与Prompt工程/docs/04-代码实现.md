# DeepSeek使用与Prompt工程项目 - 代码实现详解

## 核心代码结构

### 1. 项目文件概览

```
code/
├── 1-情感分析-Deepseek-阿里代理.py   # 云端API基础调用
├── 2-提示词工程使用.py               # Prompt工程实践
├── 3-deepseek-r1-7b使用.py          # 本地模型推理
├── 4-model-download.py             # 模型下载脚本
├── 5-ollama使用.py                 # Ollama基础调用
├── 6-ollama-stream.py              # 流式响应处理
├── 7-ollama-fastapi.py             # FastAPI服务端
└── 7-ollama-fastapi-python客户端.py # FastAPI客户端
```

## 云端API调用实现

### 1. 基础对话功能

#### 代码实现
```python
# 1-情感分析-Deepseek-阿里代理.py
import os
import dashscope
from dotenv import load_dotenv
from dashscope.api_entities.dashscope_response import Role

# 加载环境变量
load_dotenv(verbose=True)
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

def get_response(messages):
    """
    调用DeepSeek API获取回复
    
    Args:
        messages: 对话消息列表
        
    Returns:
        API响应对象
    """
    response = dashscope.Generation.call(
        model="deepseek-r1",
        messages=messages,
        result_format="message"
    )
    return response
```

**实现要点：**
- **环境变量管理**: 使用`python-dotenv`安全管理API密钥
- **消息格式**: 采用标准的`messages`格式，支持多轮对话
- **响应格式**: 使用`message`格式便于解析

### 2. 情感分析实现

```python
def sentiment_analysis(review: str) -> str:
    """
    情感分析函数
    
    Args:
        review: 用户评论文本
        
    Returns:
        情感分类结果（正向/负向）
    """
    messages = [
        {
            "role": "system", 
            "content": "你是一名舆情分析师，帮我判断产品口碑的正负向，回复请用一个词语：正向 或者 负向"
        },
        {"role": "user", "content": review}
    ]
    
    response = get_response(messages)
    return response.output.choices[0].message.content

# 使用示例
review = '这款音效特别好 给你意想不到的音质。'
result = sentiment_analysis(review)
print(f"评论: {review}")
print(f"情感: {result}")  # 输出: 正向
```

**设计模式：**
- **角色设定**: 通过system消息设定AI角色
- **输出控制**: 明确指定输出格式（单个词语）
- **简洁高效**: 专注于单一任务

## Prompt工程实践

### 1. 结构化Prompt设计

#### 基础结构
```python
# 2-提示词工程使用.py
def get_completion(prompt, model="deepseek-v3"):
    """
    获取模型完成结果
    
    Args:
        prompt: 提示词
        model: 模型名称
        
    Returns:
        模型输出文本
    """
    messages = [{"role": "user", "content": prompt}]
    response = dashscope.Generation.call(
        model=model,
        messages=messages,
        result_format='message',
        temperature=0,  # 降低随机性
    )
    return response.output.choices[0].message.content
```

#### 结构化Prompt模板
```python
def build_structured_prompt(instruction, input_text, output_format=None, context=None):
    """
    构建结构化Prompt
    
    Args:
        instruction: 任务描述
        input_text: 用户输入
        output_format: 输出格式要求
        context: 上下文信息
        
    Returns:
        完整的Prompt字符串
    """
    prompt = f"""
# 目标
{instruction}

"""
    
    if output_format:
        prompt += f"""
# 输出格式
{output_format}

"""
    
    if context:
        prompt += f"""
# 上下文
{context}

"""
    
    prompt += f"""
# 用户输入
{input_text}
"""
    return prompt

# 使用示例
instruction = """
你的任务是识别用户对手机流量套餐产品的选择条件。
每种流量套餐产品包含三个属性：名称，月费价格，月流量。
根据用户输入，识别用户在上述三种属性上的需求是什么。
"""

input_text = "办个100G的套餐。"
output_format = "以 JSON 格式输出"

prompt = build_structured_prompt(instruction, input_text, output_format)
response = get_completion(prompt)
```

### 2. 输出格式控制

#### JSON输出
```python
# 强制JSON输出
output_format = """
以 JSON 格式输出
"""

prompt = f"""
# 目标
{instruction}

# 输出格式
{output_format}

# 用户输入
{input_text}
"""

response = get_completion(prompt)
# 输出: {"data": "100G"}
```

#### 条件判断输出
```python
# 规范检查示例
instruction = """
给定一段用户与手机流量套餐客服的对话，判断客服的回答是否符合规范：
- 必须有礼貌
- 必须用官方口吻
- 介绍套餐时必须准确提及产品名称、月费价格和月流量
"""

output_format = """
如果符合规范，输出：Y
如果不符合规范，输出：N
"""

context = """
用户：你们有什么流量大的套餐
客服：亲，我们现在正在推广无限套餐，每月300元就可以享受1000G流量
"""
```

### 3. Chain of Thought实现

```python
# 思维链Prompt
cot_prompt = """
请一步一步分析对话：
1. 首先，检查客服是否使用礼貌用语
2. 然后，检查是否使用官方口吻
3. 接着，检查套餐信息是否完整准确
4. 最后，给出判断结果
"""
```

## 本地模型推理

### 1. 模型加载

```python
# 3-deepseek-r1-7b使用.py
from modelscope import AutoModelForCausalLM, AutoTokenizer

def load_model(model_path: str):
    """
    加载本地模型
    
    Args:
        model_path: 模型路径
        
    Returns:
        model, tokenizer
    """
    print(f"模型路径: {model_path}")
    print(f"路径是否存在: {os.path.exists(model_path)}")
    print("开始加载模型...")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    return model, tokenizer
```

### 2. 推理流程

```python
def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 2000):
    """
    生成模型回复
    
    Args:
        model: 加载的模型
        tokenizer: 分词器
        prompt: 输入提示
        max_new_tokens: 最大生成token数
        
    Returns:
        生成的回复文本
    """
    # 构建对话
    messages = [
        {"role": "system", "content": "you are a helpful assistant"},
        {"role": "user", "content": prompt}
    ]
    
    # 应用对话模板
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # 文本转换为模型输入
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # 生成回复
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=max_new_tokens
    )
    
    # 提取新生成的部分
    generated_ids = [
        output_ids[len(input_ids):] 
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    # 解码输出
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    return response

# 使用示例
model, tokenizer = load_model("./models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")
response = generate_response(model, tokenizer, "帮我用JavaScript写一个二分查找法")
print(response)
```

**关键步骤解析：**
1. **对话模板**: 使用`apply_chat_template`构建标准对话格式
2. **Token化**: 将文本转换为模型可理解的token
3. **生成**: 模型根据输入生成回复
4. **切片处理**: 只保留新生成的token，去除输入部分
5. **解码**: 将token转换回文本

### 3. 模型下载

```python
# 4-model-download.py
from modelscope import snapshot_download

def download_model(model_name: str, cache_dir: str = "./models"):
    """
    下载模型到本地
    
    Args:
        model_name: 模型名称
        cache_dir: 缓存目录
    """
    print(f"开始下载模型: {model_name}")
    print(f"目标目录: {cache_dir}")
    
    snapshot_download(
        model_name,
        cache_dir=cache_dir
    )
    
    print("模型下载完成!")

# 使用示例
download_model('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')
```

## Ollama集成

### 1. 基础调用

```python
# 5-ollama使用.py
import requests

def query_ollama(prompt, model="deepseek-r1:8b", base_url="http://localhost:11434"):
    """
    调用Ollama API
    
    Args:
        prompt: 输入提示
        model: 模型名称
        base_url: Ollama服务地址
        
    Returns:
        模型回复
    """
    url = f"{base_url}/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=data)
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        raise Exception(f"API 请求失败: {response.text}")

# 使用示例
response = query_ollama("你好，请介绍一下你自己")
print(response)
```

### 2. 流式响应处理

```python
# 6-ollama-stream.py
import json

def query_ollama_stream(prompt, model="deepseek-r1:8b"):
    """
    流式调用Ollama
    
    Args:
        prompt: 输入提示
        model: 模型名称
    """
    url = f"{ollama_base_url}/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    with requests.post(url, json=data, stream=True) as response:
        if response.status_code == 200:
            for line in response.iter_lines(decode_unicode=True):
                if line:
                    try:
                        obj = json.loads(line)
                        # 实时打印每段响应
                        print(obj.get("response", ""), end="", flush=True)
                    except Exception as e:
                        print(f"解析流式响应出错: {e}")
        else:
            raise Exception(f"API 请求失败: {response.text}")

# 使用示例
print("流式响应：")
query_ollama_stream("帮我写一个快速排序")
```

**流式处理特点：**
- **实时输出**: 边生成边输出，用户体验更好
- **逐行解析**: Ollama返回每行一个JSON对象
- **错误处理**: 捕获解析异常，保证稳定性

## FastAPI服务封装

### 1. 服务端实现

```python
# 7-ollama-fastapi.py
import os
import requests
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# 初始化应用
app = FastAPI(title="DeepSeek API Service")

# CORS配置
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# 请求模型
class ChatRequest(BaseModel):
    prompt: str
    model: str = "deepseek-r1:8b"

# API端点
@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    对话API接口
    
    Args:
        request: 对话请求
        
    Returns:
        对话响应
    """
    url = f"{ollama_base_url}/api/generate"
    data = {
        "model": request.model,
        "prompt": request.prompt,
        "stream": False
    }
    
    response = requests.post(url, json=data)
    
    if response.status_code == 200:
        return {"response": response.json()["response"]}
    else:
        return {"error": "Failed to get response from Ollama"}, 500

# 启动服务
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**服务特点：**
- **RESTful设计**: 标准的HTTP接口
- **异步处理**: 使用async提高并发性能
- **数据验证**: Pydantic自动验证请求参数
- **跨域支持**: CORS中间件支持前端调用

### 2. 客户端实现

```python
# 7-ollama-fastapi-python客户端.py
import requests

def chat(prompt: str, model: str = "deepseek-r1:8b"):
    """
    调用FastAPI服务
    
    Args:
        prompt: 输入提示
        model: 模型名称
        
    Returns:
        服务响应
    """
    response = requests.post(
        "http://localhost:8000/api/chat",
        json={"prompt": prompt, "model": model}
    )
    return response.json()

# 使用示例
result = chat("你好，请介绍一下你自己")
print(result)
```

## 错误处理和最佳实践

### 1. 异常处理

```python
import logging
from typing import Optional

logger = logging.getLogger(__name__)

def safe_chat(prompt: str) -> Optional[str]:
    """
    安全的对话函数，包含完整的错误处理
    
    Args:
        prompt: 输入提示
        
    Returns:
        回复内容，失败返回None
    """
    try:
        # 参数验证
        if not prompt or len(prompt) > 10000:
            logger.warning(f"无效的prompt长度: {len(prompt)}")
            return None
        
        # API调用
        response = get_completion(prompt)
        
        # 结果验证
        if not response:
            logger.error("API返回空响应")
            return None
            
        return response
        
    except Exception as e:
        logger.error(f"对话失败: {e}", exc_info=True)
        return None
```

### 2. 重试机制

```python
import time
from functools import wraps

def retry(max_retries=3, delay=1):
    """重试装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    time.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator

@retry(max_retries=3, delay=2)
def chat_with_retry(prompt: str):
    """带重试的对话函数"""
    return get_completion(prompt)
```

---

*最后更新: 2026年2月21日*
*代码实现版本: v1.0*
*开发团队: AI系统开发组*
