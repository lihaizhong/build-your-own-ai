# çŸ¥è¯†åº“å¤„ç†ç³»ç»Ÿ - æµ‹è¯•å’Œéƒ¨ç½²

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

#### KnowledgeBaseOptimizerç±»æµ‹è¯•

```python
import unittest
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from question_generator import KnowledgeBaseOptimizer


class TestKnowledgeBaseOptimizer(unittest.TestCase):
    """KnowledgeBaseOptimizerç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.optimizer = KnowledgeBaseOptimizer()
        self.test_chunk = "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº"
        self.test_knowledge_base = [
            {
                "id": "kb_001",
                "content": "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
                "category": "åŸºæœ¬ä¿¡æ¯"
            },
            {
                "id": "kb_002",
                "content": "é—¨ç¥¨ä»·æ ¼å¹³æ—¥æˆäººç¥¨ä»·ä¸º399å…ƒ",
                "category": "ä»·æ ¼ä¿¡æ¯"
            }
        ]
    
    @patch('question_generator.OpenAI')
    def test_generate_questions_for_chunk(self, mock_openai):
        """æµ‹è¯•é—®é¢˜ç”ŸæˆåŠŸèƒ½"""
        # æ¨¡æ‹ŸLLMå“åº”
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = '''
        {
            "questions": [
                {"question": "ä¸Šæµ·è¿ªå£«å°¼åœ¨å“ªé‡Œï¼Ÿ", "question_type": "ç›´æ¥é—®", "difficulty": "ç®€å•"},
                {"question": "è¿ªå£«å°¼ä¹å›­çš„ä½ç½®æ˜¯ï¼Ÿ", "question_type": "é—´æ¥é—®", "difficulty": "ç®€å•"}
            ]
        }
        '''
        mock_openai.return_value.chat.completions.create.return_value = mock_response
        
        # æ‰§è¡Œæµ‹è¯•
        questions = self.optimizer.generate_questions_for_chunk(self.test_chunk, num_questions=2)
        
        # éªŒè¯ç»“æœ
        self.assertEqual(len(questions), 2)
        self.assertIn("question", questions[0])
        self.assertIn("question_type", questions[0])
        self.assertIn("difficulty", questions[0])
    
    def test_build_knowledge_index(self):
        """æµ‹è¯•ç´¢å¼•æ„å»º"""
        # æ·»åŠ ç”Ÿæˆçš„é—®é¢˜
        for chunk in self.test_knowledge_base:
            chunk['generated_questions'] = [
                {"question": "æµ‹è¯•é—®é¢˜", "question_type": "ç›´æ¥é—®", "difficulty": "ç®€å•"}
            ]
        
        # æ„å»ºç´¢å¼•
        self.optimizer.build_knowledge_index(self.test_knowledge_base)
        
        # éªŒè¯ç´¢å¼•
        self.assertIsNotNone(self.optimizer.content_bm25)
        self.assertIsNotNone(self.optimizer.question_bm25)
        self.assertEqual(len(self.optimizer.content_metadata), 2)
    
    def test_search_similar_chunks(self):
        """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
        # æ„å»ºç´¢å¼•
        self.optimizer.build_knowledge_index(self.test_knowledge_base)
        
        # æ‰§è¡Œæ£€ç´¢
        results = self.optimizer.search_similar_chunks("ä¸Šæµ·è¿ªå£«å°¼", k=1, search_type="content")
        
        # éªŒè¯ç»“æœ
        self.assertGreater(len(results), 0)
        self.assertIn("metadata", results[0])
        self.assertIn("score", results[0])
        self.assertIn("similarity", results[0])


class TestConversationKnowledgeExtractor(unittest.TestCase):
    """ConversationKnowledgeExtractorç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.extractor = ConversationKnowledgeExtractor()
        self.test_conversation = """
ç”¨æˆ·: "æˆ‘æƒ³å»ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ç©ï¼Œé—¨ç¥¨å¤šå°‘é’±ï¼Ÿ"
AI: "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­çš„é—¨ç¥¨ä»·æ ¼æ ¹æ®æ—¥æœŸæœ‰æ‰€ä¸åŒã€‚å¹³æ—¥æˆäººç¥¨ä»·ä¸º399å…ƒï¼Œå‘¨æœ«å’ŒèŠ‚å‡æ—¥ä¸º499å…ƒã€‚"
"""
    
    @patch('conversation_extractor.OpenAI')
    def test_extract_knowledge_from_conversation(self, mock_openai):
        """æµ‹è¯•å¯¹è¯çŸ¥è¯†æå–"""
        # æ¨¡æ‹ŸLLMå“åº”
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = '''
        {
            "extracted_knowledge": [
                {
                    "knowledge_type": "äº‹å®",
                    "content": "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­é—¨ç¥¨å¹³æ—¥æˆäººç¥¨ä»·399å…ƒ",
                    "confidence": 0.9,
                    "source": "AI",
                    "keywords": ["é—¨ç¥¨", "ä»·æ ¼"],
                    "category": "ä»·æ ¼ä¿¡æ¯"
                }
            ],
            "conversation_summary": "ç”¨æˆ·è¯¢é—®è¿ªå£«å°¼é—¨ç¥¨ä»·æ ¼",
            "user_intent": "æŸ¥è¯¢ä»·æ ¼ä¿¡æ¯"
        }
        '''
        mock_openai.return_value.chat.completions.create.return_value = mock_response
        
        # æ‰§è¡Œæµ‹è¯•
        result = self.extractor.extract_knowledge_from_conversation(self.test_conversation)
        
        # éªŒè¯ç»“æœ
        self.assertIn("extracted_knowledge", result)
        self.assertIn("conversation_summary", result)
        self.assertIn("user_intent", result)


class TestKnowledgeBaseHealthChecker(unittest.TestCase):
    """KnowledgeBaseHealthCheckerç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.checker = KnowledgeBaseHealthChecker()
        self.test_knowledge_base = [
            {
                "id": "kb_001",
                "content": "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
                "category": "åŸºæœ¬ä¿¡æ¯",
                "last_updated": "2024-01-15"
            }
        ]
        self.test_queries = [
            {"query": "ä¸Šæµ·è¿ªå£«å°¼åœ¨å“ªé‡Œï¼Ÿ", "expected_answer": "æµ¦ä¸œ"}
        ]
    
    def test_calculate_overall_health_score(self):
        """æµ‹è¯•å¥åº·åº¦è¯„åˆ†è®¡ç®—"""
        missing_result = {"coverage_score": 0.8}
        outdated_result = {"freshness_score": 0.9}
        conflicting_result = {"consistency_score": 0.95}
        
        score = self.checker._calculate_overall_health_score(
            missing_result, outdated_result, conflicting_result
        )
        
        # éªŒè¯è¯„åˆ†
        expected = 0.8 * 0.4 + 0.9 * 0.3 + 0.95 * 0.3
        self.assertAlmostEqual(score, expected, places=2)
    
    def test_get_health_level(self):
        """æµ‹è¯•å¥åº·ç­‰çº§åˆ¤å®š"""
        self.assertEqual(self.checker._get_health_level(0.9), "ä¼˜ç§€")
        self.assertEqual(self.checker._get_health_level(0.7), "è‰¯å¥½")
        self.assertEqual(self.checker._get_health_level(0.5), "ä¸€èˆ¬")
        self.assertEqual(self.checker._get_health_level(0.3), "éœ€è¦æ”¹è¿›")


class TestKnowledgeBaseVersionManager(unittest.TestCase):
    """KnowledgeBaseVersionManagerç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.manager = KnowledgeBaseVersionManager()
        self.test_knowledge_base = [
            {"id": "kb_001", "content": "æµ‹è¯•å†…å®¹1"},
            {"id": "kb_002", "content": "æµ‹è¯•å†…å®¹2"}
        ]
    
    def test_detect_changes(self):
        """æµ‹è¯•ç‰ˆæœ¬å·®å¼‚æ£€æµ‹"""
        kb1 = [{"id": "1", "content": "å†…å®¹1"}, {"id": "2", "content": "å†…å®¹2"}]
        kb2 = [{"id": "1", "content": "å†…å®¹1ä¿®æ”¹"}, {"id": "3", "content": "å†…å®¹3"}]
        
        changes = self.manager._detect_changes(kb1, kb2)
        
        # éªŒè¯å˜æ›´
        self.assertEqual(len(changes["added_chunks"]), 1)
        self.assertEqual(len(changes["removed_chunks"]), 1)
        self.assertEqual(len(changes["modified_chunks"]), 1)
    
    def test_calculate_version_statistics(self):
        """æµ‹è¯•ç‰ˆæœ¬ç»Ÿè®¡è®¡ç®—"""
        stats = self.manager._calculate_version_statistics(self.test_knowledge_base)
        
        self.assertEqual(stats["total_chunks"], 2)
        self.assertGreater(stats["total_content_length"], 0)
        self.assertGreater(stats["average_chunk_length"], 0)


if __name__ == '__main__':
    unittest.main(verbosity=2)
```

### 2. é›†æˆæµ‹è¯•

```python
import pytest
import tempfile
import os
from pathlib import Path


class TestIntegration:
    """é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def temp_dir(self):
        """åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir
    
    def test_complete_pipeline(self, temp_dir):
        """æµ‹è¯•å®Œæ•´å¤„ç†ç®¡é“"""
        # 1. åˆ›å»ºæµ‹è¯•çŸ¥è¯†åº“
        knowledge_base = [
            {"id": "kb_001", "content": "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº", "category": "åŸºæœ¬ä¿¡æ¯"},
            {"id": "kb_002", "content": "é—¨ç¥¨ä»·æ ¼å¹³æ—¥æˆäººç¥¨ä»·ä¸º399å…ƒ", "category": "ä»·æ ¼ä¿¡æ¯"}
        ]
        
        # 2. é—®é¢˜ç”Ÿæˆ
        optimizer = KnowledgeBaseOptimizer()
        for chunk in knowledge_base:
            chunk['generated_questions'] = optimizer.generate_questions_for_chunk(
                chunk['content'], num_questions=3
            )
        
        # 3. æ„å»ºç´¢å¼•
        optimizer.build_knowledge_index(knowledge_base)
        
        # 4. æ£€ç´¢æµ‹è¯•
        results = optimizer.search_similar_chunks("ä¸Šæµ·è¿ªå£«å°¼", k=1)
        assert len(results) > 0
        
        # 5. ç‰ˆæœ¬åˆ›å»º
        manager = KnowledgeBaseVersionManager()
        v1_info = manager.create_version(knowledge_base, "v1.0", "æµ‹è¯•ç‰ˆæœ¬")
        
        assert v1_info['version_name'] == "v1.0"
        assert v1_info['statistics']['total_chunks'] == 2
    
    def test_health_check_pipeline(self, temp_dir):
        """æµ‹è¯•å¥åº·åº¦æ£€æŸ¥ç®¡é“"""
        knowledge_base = [...]
        test_queries = [...]
        
        checker = KnowledgeBaseHealthChecker()
        report = checker.generate_health_report(knowledge_base, test_queries)
        
        assert 'overall_health_score' in report
        assert 'health_level' in report
        assert 'recommendations' in report


class TestEndToEnd:
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    def test_question_generation_and_retrieval(self):
        """æµ‹è¯•é—®é¢˜ç”Ÿæˆå’Œæ£€ç´¢çš„ç«¯åˆ°ç«¯æµç¨‹"""
        # 1. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = KnowledgeBaseOptimizer()
        
        # 2. å‡†å¤‡çŸ¥è¯†åº“
        knowledge_base = [
            {"id": "kb_001", "content": "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº", "category": "åŸºæœ¬ä¿¡æ¯"}
        ]
        
        # 3. ç”Ÿæˆé—®é¢˜
        for chunk in knowledge_base:
            questions = optimizer.generate_questions_for_chunk(chunk['content'], num_questions=5)
            chunk['generated_questions'] = questions
        
        # 4. æ„å»ºç´¢å¼•
        optimizer.build_knowledge_index(knowledge_base)
        
        # 5. æ‰§è¡Œæ£€ç´¢
        content_results = optimizer.search_similar_chunks(
            "ä¸Šæµ·è¿ªå£«å°¼åœ¨å“ªé‡Œï¼Ÿ", k=1, search_type="content"
        )
        question_results = optimizer.search_similar_chunks(
            "ä¸Šæµ·è¿ªå£«å°¼åœ¨å“ªé‡Œï¼Ÿ", k=1, search_type="question"
        )
        
        # 6. éªŒè¯ç»“æœ
        assert len(content_results) > 0 or len(question_results) > 0
```

### 3. æ€§èƒ½æµ‹è¯•

```python
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    def test_question_generation_performance(self):
        """æµ‹è¯•é—®é¢˜ç”Ÿæˆæ€§èƒ½"""
        optimizer = KnowledgeBaseOptimizer()
        test_chunk = "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ä½äºä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº"
        
        # æµ‹è¯•å•ä¸ªé—®é¢˜ç”Ÿæˆ
        start_time = time.time()
        questions = optimizer.generate_questions_for_chunk(test_chunk, num_questions=5)
        duration = time.time() - start_time
        
        print(f"ç”Ÿæˆ5ä¸ªé—®é¢˜è€—æ—¶: {duration:.2f}ç§’")
        assert duration < 10.0  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
    
    def test_index_building_performance(self):
        """æµ‹è¯•ç´¢å¼•æ„å»ºæ€§èƒ½"""
        optimizer = KnowledgeBaseOptimizer()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        knowledge_base = [
            {"id": f"kb_{i:03d}", "content": f"æµ‹è¯•å†…å®¹ {i}" * 10, "category": "æµ‹è¯•"}
            for i in range(100)
        ]
        
        # æ·»åŠ ç”Ÿæˆçš„é—®é¢˜
        for chunk in knowledge_base:
            chunk['generated_questions'] = [
                {"question": f"æµ‹è¯•é—®é¢˜ {i}", "question_type": "ç›´æ¥é—®", "difficulty": "ç®€å•"}
                for i in range(3)
            ]
        
        # æµ‹è¯•ç´¢å¼•æ„å»º
        start_time = time.time()
        optimizer.build_knowledge_index(knowledge_base)
        duration = time.time() - start_time
        
        print(f"æ„å»º100ä¸ªåˆ‡ç‰‡çš„ç´¢å¼•è€—æ—¶: {duration:.2f}ç§’")
        assert duration < 5.0  # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
    
    def test_search_performance(self):
        """æµ‹è¯•æ£€ç´¢æ€§èƒ½"""
        optimizer = KnowledgeBaseOptimizer()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶æ„å»ºç´¢å¼•
        knowledge_base = [
            {"id": f"kb_{i:03d}", "content": f"æµ‹è¯•å†…å®¹ {i}" * 10}
            for i in range(100)
        ]
        optimizer.build_knowledge_index(knowledge_base)
        
        # æµ‹è¯•æ£€ç´¢æ€§èƒ½
        test_queries = [f"æµ‹è¯•æŸ¥è¯¢ {i}" for i in range(100)]
        
        start_time = time.time()
        for query in test_queries:
            optimizer.search_similar_chunks(query, k=5)
        duration = time.time() - start_time
        
        avg_time = duration / len(test_queries) * 1000
        print(f"å¹³å‡æ£€ç´¢æ—¶é—´: {avg_time:.2f}ms")
        assert avg_time < 50.0  # å¹³å‡åº”è¯¥åœ¨50mså†…
    
    def test_concurrent_search(self):
        """æµ‹è¯•å¹¶å‘æ£€ç´¢æ€§èƒ½"""
        optimizer = KnowledgeBaseOptimizer()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        knowledge_base = [
            {"id": f"kb_{i:03d}", "content": f"æµ‹è¯•å†…å®¹ {i}" * 10}
            for i in range(100)
        ]
        optimizer.build_knowledge_index(knowledge_base)
        
        # å¹¶å‘æ£€ç´¢æµ‹è¯•
        def search_worker(query):
            return optimizer.search_similar_chunks(query, k=5)
        
        test_queries = [f"æµ‹è¯•æŸ¥è¯¢ {i}" for i in range(20)]
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(search_worker, test_queries))
        duration = time.time() - start_time
        
        print(f"å¹¶å‘20ä¸ªæŸ¥è¯¢è€—æ—¶: {duration:.2f}ç§’")
        assert len(results) == 20
```

## éƒ¨ç½²ç­–ç•¥

### 1. å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY pyproject.toml .
COPY uv.lock .

# å®‰è£…uvå’Œä¾èµ–
RUN pip install uv && uv sync --no-dev

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/data /app/user_data/indexes /app/user_data/versions /app/output

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from code.config import config; print('OK')" || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uv", "run", "python", "code/main.py", "--help"]
```

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  knowledge-processor:
    build: .
    container_name: knowledge-processor
    volumes:
      - ./data:/app/data
      - ./user_data:/app/user_data
      - ./output:/app/output
    environment:
      - PYTHONUNBUFFERED=1
      - DASHSCOPE_API_KEY=${DASHSCOPE_API_KEY}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "from code.config import config; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

### 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: knowledge-processor
  labels:
    app: knowledge-processor
spec:
  replicas: 2
  selector:
    matchLabels:
      app: knowledge-processor
  template:
    metadata:
      labels:
        app: knowledge-processor
    spec:
      containers:
      - name: knowledge-processor
        image: knowledge-processor:latest
        ports:
        - containerPort: 8000
        env:
        - name: DASHSCOPE_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: dashscope-api-key
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: user-data-volume
          mountPath: /app/user_data
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: data-pvc
      - name: user-data-volume
        persistentVolumeClaim:
          claimName: user-data-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: knowledge-processor-service
spec:
  selector:
    app: knowledge-processor
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP
```

#### é…ç½®æ–‡ä»¶ç®¡ç†

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: knowledge-processor-config
data:
  llm_model: "qwen-turbo-latest"
  text_embedding_model: "text-embedding-v4"
  text_embedding_dim: "1024"
  top_k: "5"
  score_threshold: "0.7"
  coverage_weight: "0.4"
  freshness_weight: "0.3"
  consistency_weight: "0.3"
```

### 3. CI/CDæµæ°´çº¿

#### GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        uv venv
        uv sync
    
    - name: Run tests
      run: |
        uv run pytest tests/ -v --cov=code --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t knowledge-processor:${{ github.sha }} .
        docker tag knowledge-processor:${{ github.sha }} knowledge-processor:latest
    
    - name: Push to registry
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push knowledge-processor:${{ github.sha }}
        docker push knowledge-processor:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to production
      run: |
        kubectl set image deployment/knowledge-processor \
          knowledge-processor=knowledge-processor:${{ github.sha }} \
          -n production
```

## ç›‘æ§å’Œæ—¥å¿—

### 1. æ—¥å¿—é…ç½®

```python
# logging_config.py
from loguru import logger
import sys

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO"
    )
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        "logs/app.log",
        rotation="1 day",
        retention="7 days",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="DEBUG"
    )
    
    # æ·»åŠ é”™è¯¯æ—¥å¿—å¤„ç†å™¨
    logger.add(
        "logs/error.log",
        rotation="1 day",
        retention="30 days",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="ERROR"
    )
```

### 2. æ€§èƒ½ç›‘æ§

```python
# monitoring.py
import time
from functools import wraps
from loguru import logger

def monitor_performance(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            logger.info(f"{func.__name__} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {duration:.3f}ç§’")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"{func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {duration:.3f}ç§’ï¼Œé”™è¯¯: {e}")
            raise
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@monitor_performance
def process_knowledge_base(knowledge_base):
    """å¤„ç†çŸ¥è¯†åº“"""
    # å¤„ç†é€»è¾‘
    pass
```

### 3. Prometheusç›‘æ§

```python
# prometheus_metrics.py
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'knowledge_processor_requests_total',
    'Total number of requests',
    ['method', 'endpoint']
)

REQUEST_LATENCY = Histogram(
    'knowledge_processor_request_latency_seconds',
    'Request latency in seconds',
    ['method', 'endpoint']
)

ACTIVE_INDEXES = Gauge(
    'knowledge_processor_active_indexes',
    'Number of active indexes'
)

# ä½¿ç”¨ç¤ºä¾‹
@REQUEST_LATENCY.time()
def search_knowledge(query):
    REQUEST_COUNT.labels(method='search', endpoint='/search').inc()
    # æœç´¢é€»è¾‘
    pass
```

## éƒ¨ç½²éªŒè¯

### 1. å¥åº·æ£€æŸ¥è„šæœ¬

```python
# health_check.py
import requests
import time
from loguru import logger

def check_service_health(base_url: str, timeout: int = 30) -> bool:
    """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        response = requests.get(f"{base_url}/health", timeout=timeout)
        if response.status_code == 200:
            logger.info("âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
            return True
        else:
            logger.error(f"âŒ æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def validate_deployment(base_url: str) -> bool:
    """éªŒè¯éƒ¨ç½²æ˜¯å¦æˆåŠŸ"""
    # 1. å¥åº·æ£€æŸ¥
    if not check_service_health(base_url):
        return False
    
    # 2. åŠŸèƒ½æµ‹è¯•
    test_queries = ["æµ‹è¯•æŸ¥è¯¢"]
    for query in test_queries:
        try:
            response = requests.post(
                f"{base_url}/search",
                json={"query": query},
                timeout=10
            )
            if response.status_code != 200:
                logger.error(f"âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    logger.info("ğŸ‰ éƒ¨ç½²éªŒè¯å…¨éƒ¨é€šè¿‡")
    return True
```

### 2. å›æ»šç­–ç•¥

```python
# rollback.py
import subprocess
from loguru import logger

class RollbackManager:
    """å›æ»šç®¡ç†å™¨"""
    
    def __init__(self, deployment_name: str, namespace: str = "default"):
        self.deployment_name = deployment_name
        self.namespace = namespace
    
    def rollback(self, version: str) -> bool:
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        try:
            cmd = [
                "kubectl", "set", "image", "deployment", self.deployment_name,
                f"{self.deployment_name}={self.deployment_name}:{version}",
                "-n", self.namespace
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"âœ… å›æ»šåˆ°ç‰ˆæœ¬ {version} æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ å›æ»šå¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"âŒ å›æ»šå¼‚å¸¸: {e}")
            return False
```

---

*æœ€åæ›´æ–°: 2026å¹´2æœˆ14æ—¥*
*æµ‹è¯•å’Œéƒ¨ç½²ç‰ˆæœ¬: v1.0*
*è¿ç»´å›¢é˜Ÿ: DevOpsè¿ç»´ç»„*
