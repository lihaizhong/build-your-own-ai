# RAG问答系统 - 代码实现

## 核心代码结构

### 项目代码架构

```
code/
├── main.py                      # 主程序入口
│   ├── create_complete_rag_system()    # 创建RAG系统
│   ├── query_rag_system()              # 查询处理
│   ├── interactive_rag_system()        # 交互模式
│   ├── test_rag_system()               # 测试模式
│   └── main()                          # 主函数
│
├── pdf_processor.py             # PDF处理模块
│   └── PDFProcessor            # PDF处理类
│       ├── load_and_process()          # 加载处理
│       ├── get_document_stats()        # 统计信息
│       └── save_processed_documents()  # 保存结果
│
└── deepseek_integration.py      # API集成模块
    └── DashScopeIntegration    # 集成管理类
        ├── get_llm()                   # 获取LLM
        ├── get_embeddings()            # 获取嵌入模型
        └── validate_api_keys()         # 验证密钥
```

### 模块依赖关系

```
main.py
    ├── pdf_processor.py (PDFProcessor)
    ├── deepseek_integration.py (DashScopeIntegration)
    ├── langchain_community.vectorstores.faiss (FAISS)
    ├── langchain_text_splitters (RecursiveCharacterTextSplitter)
    └── langchain_community.document_loaders (PyPDFLoader)
```

## PDFProcessor类详解

### 类定义

```python
class PDFProcessor:
    """PDF文档处理器，支持文本提取和页码记录"""
    
    def __init__(self, pdf_path: str):
        """
        初始化PDF处理器
        
        Args:
            pdf_path: PDF文件路径
        """
        self.pdf_path = pdf_path
        self.loader = PyPDFLoader(pdf_path)
```

### 核心方法实现

#### 1. load_and_process() - 加载和处理文档

```python
def load_and_process(self) -> List[Document]:
    """
    加载PDF并处理文档，为每个文档块添加页码信息
    
    Returns:
        处理后的文档列表，每个文档包含页码元数据
    """
    try:
        # 加载PDF文档
        raw_documents = self.loader.load()
        print(f"成功加载 {len(raw_documents)} 个文档块")
        
        # 为每个文档添加页码信息
        processed_documents = []
        for doc in raw_documents:
            # 创建新的文档对象，保留原始内容但添加页码
            page_number = doc.metadata.get('page', 0)
            new_doc = Document(
                page_content=doc.page_content,
                metadata={
                    **doc.metadata,
                    'page_number': page_number,
                    'source': os.path.basename(self.pdf_path)
                }
            )
            processed_documents.append(new_doc)
        
        return processed_documents
        
    except Exception as e:
        print(f"处理PDF时出错: {str(e)}")
        raise
```

**处理流程**:
```
PDF文件
    │
    ↓ PyPDFLoader.load()
原始Document列表
    │
    ↓ 遍历每个文档
    │
    ↓ 提取page元数据
    │
    ↓ 创建新Document对象
    │   • 保留原始内容
    │   • 添加page_number
    │   • 添加source文件名
    │
    ↓
处理后的Document列表
```

#### 2. get_document_stats() - 获取文档统计

```python
def get_document_stats(self) -> Dict[str, Any]:
    """
    获取文档统计信息
    
    Returns:
        包含文档统计信息的字典
    """
    try:
        raw_documents = self.loader.load()
        total_pages = len(set(doc.metadata.get('page', 0) for doc in raw_documents))
        total_chars = sum(len(doc.page_content) for doc in raw_documents)
        
        return {
            'total_documents': len(raw_documents),
            'total_pages': total_pages,
            'total_characters': total_chars,
            'pdf_file': os.path.basename(self.pdf_path)
        }
    except Exception as e:
        print(f"获取文档统计信息时出错: {str(e)}")
        return {}
```

**输出示例**:
```python
{
    'total_documents': 68,
    'total_pages': 17,
    'total_characters': 25432,
    'pdf_file': 'AI产品经理面试题65道.pdf'
}
```

#### 3. save_processed_documents() - 保存处理结果

```python
def save_processed_documents(self, output_path: str):
    """
    保存处理后的文档到文件
    
    Args:
        output_path: 输出文件路径
    """
    try:
        documents = self.load_and_process()
        
        # 创建输出目录（如果不存在）
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # 保存文档信息
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"PDF文档处理结果 - {os.path.basename(self.pdf_path)}\n")
            f.write("=" * 50 + "\n\n")
            
            for i, doc in enumerate(documents, 1):
                page_num = doc.metadata.get('page_number', '未知')
                f.write(f"文档块 {i} (页码: {page_num}):\n")
                f.write("-" * 30 + "\n")
                f.write(doc.page_content[:500] + "...\n\n")
        
        print(f"处理结果已保存到: {output_path}")
        
    except Exception as e:
        print(f"保存处理结果时出错: {str(e)}")
        raise
```

### Document对象结构

```python
Document(
    page_content="文档文本内容...",
    metadata={
        'page': 0,                    # 原始页码（0-indexed）
        'page_number': 0,             # 处理后的页码
        'source': 'document.pdf',     # 文件名
        # 其他PyPDFLoader提供的元数据...
    }
)
```

## DashScopeIntegration类详解

### 类定义

```python
class DashScopeIntegration:
    """DashScope API集成管理器"""
    
    def __init__(self):
        """初始化DashScope集成"""
        # 获取API密钥
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        
        # 验证必要密钥
        if not self.api_key:
            raise ValueError("请设置环境变量 DASHSCOPE_API_KEY")
```

### 核心方法实现

#### 1. get_llm() - 获取LLM实例

```python
def get_llm(self, model_name: str = "qwen-turbo", temperature: float = 0.0):
    """
    获取DashScope LLM实例
    
    Args:
        model_name: 模型名称,默认为"qwen-turbo"
        temperature: 温度参数,控制输出随机性
        
    Returns:
        ChatTongyi实例
    """
    print(f"使用DashScope模型: {model_name}")
    return ChatTongyi(
        model=model_name,
        api_key=self.api_key,
        temperature=temperature
    )
```

**支持的模型**:
| 模型名称 | 特点 | 适用场景 |
|---------|------|---------|
| qwen-turbo | 快速响应 | 一般问答 |
| qwen-plus | 增强能力 | 复杂任务 |
| qwen-max | 最强性能 | 高质量要求 |

#### 2. get_embeddings() - 获取嵌入模型

```python
def get_embeddings(self, model_name: str = "text-embedding-v2"):
    """
    获取嵌入模型实例
    
    Args:
        model_name: 嵌入模型名称
        
    Returns:
        嵌入模型实例
    """
    print(f"使用DashScope嵌入模型: {model_name}")
    return DashScopeEmbeddings(
        model=model_name,
        dashscope_api_key=self.api_key
    )
```

**嵌入模型特性**:
- 向量维度: 1536
- 支持中英文混合
- 批量处理优化
- 高质量语义表示

#### 3. validate_api_keys() - 验证API密钥

```python
def validate_api_keys(self) -> bool:
    """
    验证API密钥是否有效
    
    Returns:
        验证结果
    """
    try:
        # 测试LLM连接
        llm = self.get_llm()
        test_response = llm.invoke("Hello, this is a test message.")
        print("DashScope LLM API连接成功")
        
        # 测试嵌入模型
        embeddings = self.get_embeddings()
        test_embedding = embeddings.embed_query("Hello, this is a test message.")
        print("DashScope嵌入模型连接成功")
        
        return True
        
    except Exception as e:
        print(f"API密钥验证失败: {str(e)}")
        return False
```

### 便捷函数

```python
# 全局集成实例（延迟初始化）
_dashscope_integration = None

def get_dashscope_llm(model_name: str = "qwen-turbo", temperature: float = 0.0):
    """获取DashScope LLM的便捷函数"""
    global _dashscope_integration
    if _dashscope_integration is None:
        _dashscope_integration = DashScopeIntegration()
    return _dashscope_integration.get_llm(model_name, temperature)

def get_embeddings(model_name: str = "text-embedding-v2"):
    """获取嵌入模型的便捷函数"""
    global _dashscope_integration
    if _dashscope_integration is None:
        _dashscope_integration = DashScopeIntegration()
    return _dashscope_integration.get_embeddings(model_name)

def get_integration_info() -> Dict[str, Any]:
    """获取集成信息的便捷函数"""
    global _dashscope_integration
    if _dashscope_integration is None:
        _dashscope_integration = DashScopeIntegration()
    return _dashscope_integration.get_config_info()
```

## RAG系统核心实现

### 1. create_complete_rag_system() - 创建RAG系统

```python
def create_complete_rag_system(pdf_path: str = PDF_FILE_PATH):
    """
    创建完整的RAG系统
    
    Args:
        pdf_path: PDF文件路径
        
    Returns:
        完整的RAG系统实例
    """
    print("=" * 60)
    print("创建完整的RAG问答系统")
    print("=" * 60)
    
    # 1. 加载PDF文档并处理页码
    print("\n1. 加载PDF文档并处理页码...")
    processor = PDFProcessor(pdf_path)
    documents = processor.load_and_process()
    
    # 2. 文本分割
    print("\n2. 分割文档...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    split_documents = text_splitter.split_documents(documents)
    
    # 3. 创建向量数据库
    print("\n3. 创建向量数据库...")
    embeddings = get_embeddings()
    vector_store = FAISS.from_documents(split_documents, embeddings)
    
    # 4. 初始化LLM
    print("\n4. 初始化DashScope LLM...")
    llm = get_dashscope_llm()
    
    # 5. 创建检索器
    print("\n5. 创建检索器...")
    retriever = vector_store.as_retriever(search_kwargs={"k": 4})
    
    # 返回RAG系统字典
    return {
        'llm': llm,
        'retriever': retriever,
        'vector_store': vector_store
    }
```

**系统创建流程**:
```
┌─────────────────────────────────────────────────────────┐
│                  Step 1: PDF处理                         │
│  PDFProcessor.load_and_process()                        │
│  • 加载PDF文件                                          │
│  • 提取文本内容                                         │
│  • 记录页码信息                                         │
└─────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────┐
│                  Step 2: 文本分割                        │
│  RecursiveCharacterTextSplitter                         │
│  • chunk_size: 1000                                     │
│  • chunk_overlap: 200                                   │
│  • 按分隔符优先级分割                                   │
└─────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────┐
│                  Step 3: 向量化                          │
│  FAISS.from_documents()                                 │
│  • DashScope嵌入模型                                    │
│  • 生成1536维向量                                       │
│  • 创建FAISS索引                                        │
└─────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────┐
│                  Step 4: LLM初始化                       │
│  get_dashscope_llm()                                    │
│  • 模型: qwen-turbo                                     │
│  • temperature: 0.0                                     │
└─────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────┐
│                  Step 5: 检索器创建                      │
│  vector_store.as_retriever()                            │
│  • search_type: similarity                              │
│  • k: 4 (返回前4个结果)                                 │
└─────────────────────────────────────────────────────────┘
```

### 2. query_rag_system() - 查询处理

```python
def query_rag_system(rag_system, question):
    """
    查询RAG系统
    
    Args:
        rag_system: RAG系统字典
        question: 用户问题
        
    Returns:
        包含回答和来源文档的字典
    """
    # 检索相关文档
    print(f"正在检索与'{question}'相关的文档...")
    source_documents = rag_system['retriever'].invoke(question)
    
    if not source_documents:
        return {
            'result': '未找到相关文档，请尝试其他问题。',
            'source_documents': []
        }
    
    # 组合检索到的文档内容
    context = "\n\n".join([doc.page_content for doc in source_documents])
    
    # 构建提示词
    prompt = f"""基于以下文档内容回答用户问题。如果文档中没有相关信息，请如实说明。

文档内容:
{context}

用户问题: {question}

请用中文回答，并在回答后列出相关的文档页码信息。"""
    
    # 使用LLM生成回答
    print("正在生成回答...")
    response = rag_system['llm'].invoke(prompt)
    answer = response.content if hasattr(response, 'content') else str(response)
    
    return {
        'result': answer,
        'source_documents': source_documents
    }
```

**查询处理流程**:
```
用户问题
    │
    ↓ retriever.invoke(question)
检索文档列表
    │
    ↓ 组合上下文
上下文字符串
    │
    ↓ 构建提示词
完整Prompt
    │
    ↓ llm.invoke(prompt)
LLM响应
    │
    ↓ 提取回答内容
最终回答
```

### 3. interactive_rag_system() - 交互模式

```python
def interactive_rag_system(pdf_path: str | None = None):
    """
    交互式RAG问答系统
    
    Args:
        pdf_path: PDF文件路径，如果为None则使用默认路径
    """
    try:
        # 创建RAG系统
        rag_system = create_complete_rag_system(pdf_path or PDF_FILE_PATH)
        
        print("\n" + "=" * 60)
        print("RAG问答系统已就绪！")
        print("=" * 60)
        print("提示: 输入 'quit' 或 'exit' 退出系统")
        print("-" * 60)
        
        while True:
            try:
                # 获取用户输入
                question = input("\n请输入您的问题: ").strip()
                
                # 检查退出命令
                if question.lower() in ['quit', 'exit', 'q']:
                    print("感谢使用RAG问答系统，再见！")
                    break
                
                # 空输入处理
                if not question:
                    print("请输入有效的问题")
                    continue
                
                # 执行查询
                result = query_rag_system(rag_system, question)
                
                # 显示结果
                display_answer_with_sources(result, question)
                
            except KeyboardInterrupt:
                print("\n\n程序被用户中断，正在退出...")
                break
            except Exception as e:
                print(f"查询时出错: {str(e)}")
                continue
                
    except Exception as e:
        print(f"启动RAG系统失败: {str(e)}")
```

### 4. 路径处理函数

```python
def normalize_pdf_path(pdf_path: str) -> str:
    """
    规范化PDF文件路径，支持绝对路径、相对路径和~路径
    
    Args:
        pdf_path: PDF文件路径
        
    Returns:
        规范化后的绝对路径字符串
    """
    path = Path(pdf_path).expanduser().resolve()
    
    # 如果路径不存在，尝试基于当前脚本文件解析相对路径
    if not path.exists():
        relative_path = Path(__file__).parent / pdf_path
        if relative_path.exists():
            path = relative_path.resolve()
    
    return str(path)
```

## 文档分割模块

### RecursiveCharacterTextSplitter详解

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # 每个文本块的最大字符数
    chunk_overlap=200,      # 块之间的重叠字符数
    length_function=len,    # 计算长度的函数
    separators=[            # 分隔符优先级
        "\n\n",             # 首先尝试按段落分割
        "\n",               # 然后按行分割
        " ",                # 然后按空格分割
        ""                  # 最后按字符分割
    ]
)
```

### 分割算法流程

```
原始文档
    │
    ↓ 尝试按"\n\n"分割
段落列表
    │
    ↓ 检查每个段落是否超过chunk_size
    │
    ├─ 不超过 → 保留段落
    │
    └─ 超过 → 尝试按"\n"分割
              │
              ↓ 检查每行是否超过chunk_size
              │
              ├─ 不超过 → 保留行
              │
              └─ 超过 → 尝试按" "分割
                        │
                        ↓ 继续递归...
```

### 分割参数优化

```python
# 技术文档配置
tech_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=300,
    separators=["\n\n", "\n", "。", " ", ""]
)

# FAQ文档配置
faq_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\n\n", "\n", "？", " ", ""]
)
```

## 向量存储和检索

### FAISS索引创建

```python
# 创建向量存储
embeddings = get_embeddings()  # DashScope嵌入模型
vector_store = FAISS.from_documents(
    documents=split_documents,
    embedding=embeddings
)

# 创建检索器
retriever = vector_store.as_retriever(
    search_type="similarity",  # 相似度搜索
    search_kwargs={"k": 4}     # 返回前4个结果
)
```

### 检索过程详解

```python
# 检索过程
question = "产品经理需要具备哪些核心能力？"

# Step 1: 将问题转换为向量
question_embedding = embeddings.embed_query(question)

# Step 2: 在FAISS索引中搜索
# FAISS内部执行:
# - 计算问题向量与所有文档向量的相似度
# - 排序并返回top-k结果
distances, indices = vector_store.index.search(
    question_embedding.reshape(1, -1), 
    k=4
)

# Step 3: 根据索引获取原始文档
source_documents = [vector_store.docstore._dict[i] for i in indices[0]]
```

### 向量相似度计算

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """计算余弦相似度"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

def l2_distance(vec1, vec2):
    """计算L2距离"""
    return np.linalg.norm(vec1 - vec2)
```

## 错误处理机制

### 异常处理策略

```python
def safe_rag_operation(func):
    """RAG操作的安全装饰器"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except FileNotFoundError as e:
            print(f"文件未找到: {str(e)}")
            return None
        except ValueError as e:
            print(f"参数错误: {str(e)}")
            return None
        except Exception as e:
            print(f"操作失败: {str(e)}")
            return None
    return wrapper

# 使用示例
@safe_rag_operation
def load_document(pdf_path):
    processor = PDFProcessor(pdf_path)
    return processor.load_and_process()
```

### 常见异常处理

```python
# API密钥错误
try:
    llm = get_dashscope_llm()
except ValueError as e:
    print("请检查DASHSCOPE_API_KEY环境变量是否设置")

# 文件路径错误
try:
    processor = PDFProcessor(pdf_path)
except FileNotFoundError:
    print(f"PDF文件不存在: {pdf_path}")

# 网络超时
try:
    response = llm.invoke(prompt)
except TimeoutError:
    print("API请求超时，请检查网络连接")

# 空结果处理
if not source_documents:
    return {
        'result': '未找到相关文档，请尝试其他问题。',
        'source_documents': []
    }
```

## 性能优化实现

### 批量处理优化

```python
def batch_process_documents(documents, batch_size=32):
    """批量处理文档"""
    results = []
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        # 批量生成嵌入
        batch_embeddings = embeddings.embed_documents(
            [doc.page_content for doc in batch]
        )
        results.extend(batch_embeddings)
    return results
```

### 缓存实现

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embedding(text: str):
    """缓存嵌入结果"""
    return embeddings.embed_query(text)

# 向量索引持久化
def save_vector_store(vector_store, path="faiss_index"):
    """保存向量索引"""
    vector_store.save_local(path)

def load_vector_store(path="faiss_index"):
    """加载向量索引"""
    return FAISS.load_local(path, embeddings)
```

### 异步处理

```python
import asyncio

async def async_query(rag_system, question):
    """异步查询"""
    # 异步检索
    source_documents = await rag_system['retriever'].ainvoke(question)
    # 异步生成
    response = await rag_system['llm'].ainvoke(prompt)
    return response

async def batch_queries(rag_system, questions):
    """批量异步查询"""
    tasks = [async_query(rag_system, q) for q in questions]
    return await asyncio.gather(*tasks)
```

---

*最后更新: 2026年2月15日*
*文档版本: v1.0*
*开发团队: build-your-own-ai项目*
