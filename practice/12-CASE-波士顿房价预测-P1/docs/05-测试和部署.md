# æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ç³»ç»Ÿ - æµ‹è¯•å’Œéƒ¨ç½²

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

#### ModelTrainerç±»æµ‹è¯•

```python
import unittest
import numpy as np
import pandas as pd
from unittest.mock import Mock, patch
from code.model_training import ModelTrainer


class TestModelTrainer(unittest.TestCase):
    """ModelTrainerç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.trainer = ModelTrainer()
        np.random.seed(42)
        self.X_train = np.random.rand(100, 13)
        self.y_train = np.random.rand(100) * 50 + 10
        self.X_test = np.random.rand(20, 13)
        self.y_test = np.random.rand(20) * 50 + 10
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertIsNotNone(self.trainer.models)
        self.assertIsNone(self.trainer.best_model)
        self.assertEqual(self.trainer.model_scores, {})
    
    def test_initialize_models(self):
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
        self.trainer.initialize_models()
        
        self.assertEqual(len(self.trainer.models), 7)
        self.assertIn('linear_regression', self.trainer.models)
        self.assertIn('ridge', self.trainer.models)
        self.assertIn('lasso', self.trainer.models)
        self.assertIn('random_forest', self.trainer.models)
        self.assertIn('gradient_boosting', self.trainer.models)
        self.assertIn('svr', self.trainer.models)
        self.assertIn('neural_network', self.trainer.models)
    
    def test_train_single_model(self):
        """æµ‹è¯•å•æ¨¡å‹è®­ç»ƒ"""
        from sklearn.linear_model import LinearRegression
        
        model = LinearRegression()
        self.trainer.train_single_model(model, self.X_train, self.y_train, 'test_model')
        
        # éªŒè¯æ¨¡å‹å·²è®­ç»ƒ
        self.assertTrue(hasattr(model, 'coef_'))
    
    def test_train_all_models(self):
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹è®­ç»ƒ"""
        self.trainer.initialize_models()
        self.trainer.train_all_models(self.X_train, self.y_train)
        
        # éªŒè¯æ‰€æœ‰æ¨¡å‹å·²è®­ç»ƒ
        for name, model in self.trainer.models.items():
            if name == 'neural_network':
                self.assertTrue(hasattr(model, 'coefs_'))
            else:
                self.assertTrue(hasattr(model, 'coef_') or hasattr(model, 'estimators_'))
    
    def test_evaluate_models(self):
        """æµ‹è¯•æ¨¡å‹è¯„ä¼°"""
        self.trainer.initialize_models()
        self.trainer.train_all_models(self.X_train, self.y_train)
        
        results = self.trainer.evaluate_models(self.X_test, self.y_test)
        
        self.assertEqual(len(results), 7)
        for name, metrics in results.items():
            self.assertIn('MAE', metrics)
            self.assertIn('RMSE', metrics)
            self.assertIn('R2', metrics)
            self.assertGreater(metrics['R2'], -1)  # RÂ²åº”è¯¥å¤§äº-1
    
    def test_select_best_model(self):
        """æµ‹è¯•æœ€ä½³æ¨¡å‹é€‰æ‹©"""
        self.trainer.initialize_models()
        self.trainer.train_all_models(self.X_train, self.y_train)
        self.trainer.evaluate_models(self.X_test, self.y_test)
        
        best_model = self.trainer.select_best_model(metric='RMSE')
        
        self.assertIsNotNone(best_model)
        self.assertIsNotNone(self.trainer.best_model_name)
    
    def test_cross_validation(self):
        """æµ‹è¯•äº¤å‰éªŒè¯"""
        from sklearn.linear_model import LinearRegression
        
        model = LinearRegression()
        results = self.trainer.cross_validation(model, self.X_train, self.y_train, cv=3)
        
        self.assertIn('mean_rmse', results)
        self.assertIn('std_rmse', results)
        self.assertGreater(results['mean_rmse'], 0)


class TestDataPreprocessing(unittest.TestCase):
    """æ•°æ®é¢„å¤„ç†æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        np.random.seed(42)
        self.test_df = pd.DataFrame({
            'feature1': np.random.rand(100),
            'feature2': np.random.rand(100) * 100,
            'target': np.random.rand(100) * 50 + 10
        })
    
    def test_check_data_quality(self):
        """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥"""
        from code.data_preprocessing import check_data_quality
        
        report = check_data_quality(self.test_df)
        
        self.assertIn('shape', report)
        self.assertIn('missing_values', report)
        self.assertIn('duplicates', report)
        self.assertEqual(report['shape'], (100, 3))
    
    def test_handle_missing_values(self):
        """æµ‹è¯•ç¼ºå¤±å€¼å¤„ç†"""
        from code.data_preprocessing import handle_missing_values
        
        # æ·»åŠ ç¼ºå¤±å€¼
        df_with_missing = self.test_df.copy()
        df_with_missing.loc[0:5, 'feature1'] = np.nan
        
        df_processed = handle_missing_values(df_with_missing, strategy='mean')
        
        self.assertEqual(df_processed.isnull().sum().sum(), 0)
    
    def test_detect_outliers(self):
        """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹"""
        from code.data_preprocessing import detect_outliers
        
        # æ·»åŠ å¼‚å¸¸å€¼
        df_with_outliers = self.test_df.copy()
        df_with_outliers.loc[0, 'feature2'] = 10000
        
        outliers_info = detect_outliers(df_with_outliers, method='iqr')
        
        self.assertIn('feature2', outliers_info)
        self.assertGreater(outliers_info['feature2']['count'], 0)


class TestFeatureEngineering(unittest.TestCase):
    """ç‰¹å¾å·¥ç¨‹æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        np.random.seed(42)
        self.X = pd.DataFrame({
            'feature1': np.random.rand(100),
            'feature2': np.random.rand(100),
            'feature3': np.random.rand(100)
        })
        self.y = np.random.rand(100) * 50 + 10
    
    def test_scale_features(self):
        """æµ‹è¯•ç‰¹å¾æ ‡å‡†åŒ–"""
        from code.feature_engineering import scale_features
        
        X_scaled, scaler = scale_features(self.X, method='standard')
        
        self.assertEqual(X_scaled.shape, self.X.shape)
        # éªŒè¯æ ‡å‡†åŒ–åçš„å‡å€¼æ¥è¿‘0
        self.assertAlmostEqual(X_scaled.mean().mean(), 0, places=10)
    
    def test_create_polynomial_features(self):
        """æµ‹è¯•å¤šé¡¹å¼ç‰¹å¾åˆ›å»º"""
        from code.feature_engineering import create_polynomial_features
        
        X_poly, poly = create_polynomial_features(self.X, degree=2)
        
        # å¤šé¡¹å¼ç‰¹å¾æ•°åº”è¯¥å¤§äºåŸå§‹ç‰¹å¾æ•°
        self.assertGreater(X_poly.shape[1], self.X.shape[1])
    
    def test_feature_correlation_analysis(self):
        """æµ‹è¯•ç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
        from code.feature_engineering import feature_correlation_analysis
        
        df = self.X.copy()
        df['target'] = self.y
        
        corr_matrix, target_corr = feature_correlation_analysis(df, target_col='target')
        
        self.assertEqual(corr_matrix.shape, (4, 4))
        self.assertEqual(len(target_corr), 3)


class TestModelEvaluator(unittest.TestCase):
    """æ¨¡å‹è¯„ä¼°å™¨æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        np.random.seed(42)
        self.y_true = np.random.rand(50) * 50 + 10
        self.y_pred = self.y_true + np.random.randn(50) * 2  # æ·»åŠ å™ªå£°
        self.evaluator = ModelEvaluator()
    
    def test_calculate_metrics(self):
        """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
        metrics = self.evaluator.calculate_metrics(
            self.y_true, self.y_pred, 'test_model'
        )
        
        self.assertIn('MAE', metrics)
        self.assertIn('MSE', metrics)
        self.assertIn('RMSE', metrics)
        self.assertIn('R2', metrics)
        self.assertGreater(metrics['MAE'], 0)
        self.assertGreater(metrics['RMSE'], 0)
    
    def test_compare_models(self):
        """æµ‹è¯•æ¨¡å‹å¯¹æ¯”"""
        # æ·»åŠ å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ
        self.evaluator.calculate_metrics(self.y_true, self.y_pred, 'model1')
        self.evaluator.calculate_metrics(
            self.y_true, self.y_pred + 1, 'model2'
        )
        
        df_comparison = self.evaluator.compare_models()
        
        self.assertEqual(len(df_comparison), 2)
        self.assertIn('MAE', df_comparison.columns)
        self.assertIn('RMSE', df_comparison.columns)


class TestHousePricePredictor(unittest.TestCase):
    """æˆ¿ä»·é¢„æµ‹å™¨æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        np.random.seed(42)
        self.sample_features = np.random.rand(13)
    
    @patch('joblib.load')
    def test_load_models(self, mock_load):
        """æµ‹è¯•æ¨¡å‹åŠ è½½"""
        from code.prediction import HousePricePredictor
        
        predictor = HousePricePredictor(model_dir='model/')
        predictor.load_models()
        
        # éªŒè¯åŠ è½½è°ƒç”¨
        self.assertTrue(mock_load.called)
    
    def test_preprocess_input(self):
        """æµ‹è¯•è¾“å…¥é¢„å¤„ç†"""
        from code.prediction import HousePricePredictor
        
        predictor = HousePricePredictor(model_dir='model/')
        
        # æµ‹è¯•DataFrameè¾“å…¥
        df_input = pd.DataFrame([self.sample_features])
        processed = predictor.preprocess_input(df_input, use_scaler=False)
        
        self.assertEqual(processed.shape, (1, 13))


if __name__ == '__main__':
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    unittest.main(verbosity=2)
```

### 2. é›†æˆæµ‹è¯•

#### ç«¯åˆ°ç«¯æµ‹è¯•

```python
import pytest
import tempfile
import os
from pathlib import Path
import numpy as np
import pandas as pd


class TestEndToEnd:
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @pytest.fixture
    def temp_dir(self):
        """åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir
    
    @pytest.fixture
    def sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        df = pd.DataFrame(
            np.random.rand(200, 14),
            columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                    'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
        )
        return df
    
    def test_complete_pipeline(self, temp_dir, sample_data):
        """æµ‹è¯•å®Œæ•´æ•°æ®å¤„ç†ç®¡é“"""
        # 1. ä¿å­˜æµ‹è¯•æ•°æ®
        data_path = os.path.join(temp_dir, 'housing.csv')
        sample_data.to_csv(data_path, index=False)
        
        # 2. æ•°æ®é¢„å¤„ç†
        from code.data_preprocessing import load_boston_data, check_data_quality
        
        df = pd.read_csv(data_path)
        report = check_data_quality(df)
        
        assert report['shape'] == (200, 14)
        
        # 3. ç‰¹å¾å·¥ç¨‹
        from code.feature_engineering import scale_features
        
        X = df.drop('MEDV', axis=1)
        y = df['MEDV']
        X_scaled, scaler = scale_features(X, method='standard')
        
        assert X_scaled.shape == (200, 13)
        
        # 4. æ¨¡å‹è®­ç»ƒ
        from code.model_training import ModelTrainer
        from sklearn.model_selection import train_test_split
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        trainer = ModelTrainer()
        trainer.initialize_models()
        trainer.train_all_models(X_train, y_train)
        
        # 5. æ¨¡å‹è¯„ä¼°
        results = trainer.evaluate_models(X_test, y_test)
        assert len(results) == 7
        
        # 6. æœ€ä½³æ¨¡å‹é€‰æ‹©
        best_model = trainer.select_best_model(metric='RMSE')
        assert best_model is not None
    
    def test_prediction_workflow(self, temp_dir, sample_data):
        """æµ‹è¯•é¢„æµ‹å·¥ä½œæµ"""
        # å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
        from code.model_training import ModelTrainer
        from code.feature_engineering import scale_features
        
        X = sample_data.drop('MEDV', axis=1)
        y = sample_data['MEDV']
        
        X_scaled, scaler = scale_features(X, method='standard')
        
        trainer = ModelTrainer()
        trainer.initialize_models()
        trainer.train_all_models(X_scaled, y)
        trainer.select_best_model(metric='RMSE')
        trainer.scaler = scaler
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_models(temp_dir)
        
        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
        from code.prediction import HousePricePredictor
        
        predictor = HousePricePredictor(model_dir=temp_dir)
        predictor.load_models()
        predictor.load_best_model()
        
        # æ‰§è¡Œé¢„æµ‹
        sample_input = X.iloc[0].values
        prediction = predictor.predict_single(sample_input, use_best=True)
        
        assert isinstance(prediction, (int, float))
        assert prediction > 0
```

### 3. æ€§èƒ½æµ‹è¯•

#### å‹åŠ›æµ‹è¯•

```python
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    def test_training_performance(self):
        """æµ‹è¯•è®­ç»ƒæ€§èƒ½"""
        from code.model_training import ModelTrainer
        
        # å‡†å¤‡æ•°æ®
        np.random.seed(42)
        X = np.random.rand(1000, 13)
        y = np.random.rand(1000) * 50 + 10
        
        trainer = ModelTrainer()
        trainer.initialize_models()
        
        # æµ‹è¯•è®­ç»ƒæ—¶é—´
        start_time = time.time()
        trainer.train_all_models(X, y)
        end_time = time.time()
        
        duration = end_time - start_time
        print(f"\nè®­ç»ƒ1000ä¸ªæ ·æœ¬è€—æ—¶: {duration:.2f}ç§’")
        
        # éªŒè¯è®­ç»ƒæ—¶é—´åˆç†
        assert duration < 60  # åº”è¯¥åœ¨60ç§’å†…å®Œæˆ
    
    def test_prediction_performance(self):
        """æµ‹è¯•é¢„æµ‹æ€§èƒ½"""
        from code.model_training import ModelTrainer
        
        # å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
        np.random.seed(42)
        X_train = np.random.rand(500, 13)
        y_train = np.random.rand(500) * 50 + 10
        X_test = np.random.rand(100, 13)
        
        trainer = ModelTrainer()
        trainer.initialize_models()
        trainer.train_all_models(X_train, y_train)
        trainer.select_best_model()
        
        # æµ‹è¯•é¢„æµ‹æ—¶é—´
        start_time = time.time()
        for _ in range(100):
            trainer.best_model.predict(X_test)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100 * 1000
        print(f"\nå¹³å‡é¢„æµ‹æ—¶é—´: {avg_time:.2f}ms")
        
        # éªŒè¯é¢„æµ‹æ—¶é—´åˆç†
        assert avg_time < 100  # æ¯æ¬¡é¢„æµ‹åº”è¯¥å°äº100ms
    
    def test_concurrent_predictions(self):
        """æµ‹è¯•å¹¶å‘é¢„æµ‹"""
        from code.model_training import ModelTrainer
        
        # å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
        np.random.seed(42)
        X_train = np.random.rand(500, 13)
        y_train = np.random.rand(500) * 50 + 10
        
        trainer = ModelTrainer()
        trainer.initialize_models()
        trainer.train_all_models(X_train, y_train)
        trainer.select_best_model()
        
        # å¹¶å‘é¢„æµ‹
        def predict_worker(features):
            return trainer.best_model.predict(features.reshape(1, -1))[0]
        
        test_features = [np.random.rand(13) for _ in range(20)]
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(predict_worker, f) for f in test_features]
            results = [future.result() for future in as_completed(futures)]
        end_time = time.time()
        
        print(f"\nå¹¶å‘é¢„æµ‹20ä¸ªæ ·æœ¬ï¼Œ4çº¿ç¨‹: {end_time - start_time:.2f}ç§’")
        
        assert len(results) == 20
```

## éƒ¨ç½²ç­–ç•¥

### 1. å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    make \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/data /app/model /app/feature /app/prediction_result

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV DATA_DIR=/app/data
ENV MODEL_DIR=/app/model

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from code.model_training import ModelTrainer; print('OK')" || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "code/model_training.py"]
```

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  boston-housing-prediction:
    build: .
    container_name: boston-housing-prediction
    volumes:
      - ./data:/app/data
      - ./model:/app/model
      - ./prediction_result:/app/prediction_result
    environment:
      - PYTHONUNBUFFERED=1
      - DATA_DIR=/app/data
      - MODEL_DIR=/app/model
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "print('healthy')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

### 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: boston-housing-prediction
  labels:
    app: boston-housing-prediction
spec:
  replicas: 2
  selector:
    matchLabels:
      app: boston-housing-prediction
  template:
    metadata:
      labels:
        app: boston-housing-prediction
    spec:
      containers:
      - name: prediction-service
        image: boston-housing-prediction:latest
        ports:
        - containerPort: 8000
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: DATA_DIR
          value: "/app/data"
        - name: MODEL_DIR
          value: "/app/model"
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: model-volume
          mountPath: /app/model
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - python
            - -c
            - "print('alive')"
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - python
            - -c
            - "print('ready')"
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: data-pvc
      - name: model-volume
        persistentVolumeClaim:
          claimName: model-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: boston-housing-service
spec:
  selector:
    app: boston-housing-prediction
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. APIæœåŠ¡éƒ¨ç½²

#### FastAPIæœåŠ¡å®ç°

```python
# api_service.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import joblib
from typing import List, Optional

app = FastAPI(
    title="æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹API",
    description="åŸºäºæœºå™¨å­¦ä¹ çš„æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æœåŠ¡",
    version="1.0.0"
)

# å…¨å±€æ¨¡å‹å˜é‡
model = None
scaler = None


class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    features: List[float]
    model_name: Optional[str] = None


class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    prediction: float
    model_used: str
    confidence: Optional[float] = None


class BatchPredictionRequest(BaseModel):
    """æ‰¹é‡é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    features_list: List[List[float]]
    model_name: Optional[str] = None


@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, scaler
    
    try:
        model = joblib.load('model/best_model.pkl')
        scaler = joblib.load('model/standard_scaler.pkl')
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    return {"status": "healthy"}


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """å•æ ·æœ¬é¢„æµ‹"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        features = np.array(request.features).reshape(1, -1)
        features_scaled = scaler.transform(features)
        prediction = model.predict(features_scaled)[0]
        
        return PredictionResponse(
            prediction=float(prediction),
            model_used="best_model",
            confidence=None
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/predict/batch")
async def batch_predict(request: BatchPredictionRequest):
    """æ‰¹é‡é¢„æµ‹"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        features = np.array(request.features_list)
        features_scaled = scaler.transform(features)
        predictions = model.predict(features_scaled)
        
        return {
            "predictions": predictions.tolist(),
            "count": len(predictions),
            "model_used": "best_model"
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/model/info")
async def model_info():
    """æ¨¡å‹ä¿¡æ¯"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    return {
        "model_type": type(model).__name__,
        "model_params": str(model.get_params())
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4. ç›‘æ§å’Œæ—¥å¿—

#### æ—¥å¿—é…ç½®

```python
# logging_config.py
import logging
import logging.config
import os

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        },
        'detailed': {
            'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d: %(message)s'
        },
    },
    'handlers': {
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'standard',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'formatter': 'detailed',
            'filename': 'logs/prediction.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        },
        'error_file': {
            'level': 'ERROR',
            'class': 'logging.handlers.RotatingFileHandler',
            'formatter': 'detailed',
            'filename': 'logs/error.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        }
    },
    'loggers': {
        '': {
            'handlers': ['console', 'file', 'error_file'],
            'level': 'INFO',
            'propagate': False
        }
    }
}

def setup_logging():
    """é…ç½®æ—¥å¿—"""
    os.makedirs('logs', exist_ok=True)
    logging.config.dictConfig(LOGGING_CONFIG)
    return logging.getLogger(__name__)
```

### 5. CI/CDæµæ°´çº¿

#### GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install numpy pandas scikit-learn matplotlib pytest flake8
    
    - name: Lint with flake8
      run: |
        flake8 code/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 code/ --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics
    
    - name: Run tests
      run: |
        pytest tests/ -v --tb=short
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t boston-housing-prediction:${{ github.sha }} .
        docker tag boston-housing-prediction:${{ github.sha }} boston-housing-prediction:latest
    
    - name: Push to registry
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push boston-housing-prediction:${{ github.sha }}
        docker push boston-housing-prediction:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to production
      run: |
        kubectl set image deployment/boston-housing-prediction \
          prediction-service=boston-housing-prediction:${{ github.sha }} \
          -n production
```

## éƒ¨ç½²éªŒè¯

### 1. éƒ¨ç½²åéªŒè¯

```python
# deployment_validator.py
import requests
import time
import logging

logger = logging.getLogger(__name__)

def validate_deployment(base_url: str, timeout: int = 60):
    """éªŒè¯éƒ¨ç½²æ˜¯å¦æˆåŠŸ"""
    
    # 1. å¥åº·æ£€æŸ¥
    health_url = f"{base_url}/health"
    try:
        response = requests.get(health_url, timeout=5)
        if response.status_code == 200:
            logger.info("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥è¶…æ—¶æˆ–å¤±è´¥: {e}")
        return False
    
    # 2. åŠŸèƒ½æµ‹è¯•
    test_features = [
        0.00632, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2,
        4.0900, 1.0, 296.0, 15.3, 396.90, 4.98
    ]
    
    predict_url = f"{base_url}/predict"
    try:
        response = requests.post(
            predict_url, 
            json={"features": test_features}, 
            timeout=10
        )
        if response.status_code == 200:
            result = response.json()
            logger.info(f"âœ… é¢„æµ‹æµ‹è¯•æˆåŠŸ: {result['prediction']}")
        else:
            logger.error(f"âŒ é¢„æµ‹æµ‹è¯•å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹æµ‹è¯•å¼‚å¸¸: {e}")
        return False
    
    logger.info("ğŸ‰ éƒ¨ç½²éªŒè¯å…¨éƒ¨é€šè¿‡")
    return True

def wait_for_deployment(base_url: str, max_wait_time: int = 300):
    """ç­‰å¾…éƒ¨ç½²å®Œæˆ"""
    start_time = time.time()
    
    while time.time() - start_time < max_wait_time:
        if validate_deployment(base_url):
            return True
        logger.info("ç­‰å¾…éƒ¨ç½²å®Œæˆ...")
        time.sleep(10)
    
    logger.error("éƒ¨ç½²éªŒè¯è¶…æ—¶")
    return False
```

### 2. å›æ»šç­–ç•¥

```python
# rollback_manager.py
import logging
import subprocess
import time

logger = logging.getLogger(__name__)

class RollbackManager:
    """å›æ»šç®¡ç†å™¨"""
    
    def __init__(self, deployment_name: str, namespace: str = "default"):
        self.deployment_name = deployment_name
        self.namespace = namespace
    
    def get_current_version(self):
        """è·å–å½“å‰éƒ¨ç½²ç‰ˆæœ¬"""
        try:
            cmd = [
                "kubectl", "get", "deployment", self.deployment_name,
                "-o", "jsonpath={.spec.template.metadata.labels.version}",
                "-n", self.namespace
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            return result.stdout.strip()
        except Exception as e:
            logger.error(f"è·å–ç‰ˆæœ¬å¤±è´¥: {e}")
            return None
    
    def rollback_to_version(self, version: str, timeout: int = 300):
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        try:
            cmd = [
                "kubectl", "set", "image", "deployment", self.deployment_name,
                f"prediction-service=boston-housing-prediction:{version}",
                "-n", self.namespace
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"âœ… å¼€å§‹å›æ»šåˆ°ç‰ˆæœ¬: {version}")
                return self._wait_for_rollback_completion(timeout)
            else:
                logger.error(f"âŒ å›æ»šå¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å›æ»šå¼‚å¸¸: {e}")
            return False
    
    def _wait_for_rollback_completion(self, timeout: int) -> bool:
        """ç­‰å¾…å›æ»šå®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                cmd = [
                    "kubectl", "get", "deployment", self.deployment_name,
                    "-o", "jsonpath={.status.readyReplicas}",
                    "-n", self.namespace
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.stdout.strip() != "0":
                    logger.info("âœ… å›æ»šå®Œæˆ")
                    return True
                
                time.sleep(10)
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥å›æ»šçŠ¶æ€å¤±è´¥: {e}")
                return False
        
        return False
```

---

*æœ€åæ›´æ–°: 2026å¹´2æœˆ15æ—¥*
*æµ‹è¯•å’Œéƒ¨ç½²ç‰ˆæœ¬: v1.0*
*è¿ç»´å›¢é˜Ÿ: DevOpsè¿ç»´ç»„*
