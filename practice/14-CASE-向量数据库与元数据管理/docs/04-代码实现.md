# 向量数据库与元数据管理系统 - 代码实现详解

## 核心代码结构

### 1. 项目路径管理模块

```python
def get_project_path(*paths: str) -> str:
    """获取项目路径的统一方法"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)
```

**功能特点：**
- **统一路径管理**: 提供项目路径的标准化获取方法
- **容错处理**: 处理不同运行环境下的路径差异
- **灵活扩展**: 支持任意层级的路径构建

### 2. AIInterviewQASystem类核心实现

#### 初始化方法详解

```python
class AIInterviewQASystem:
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """
        初始化系统
        
        Args:
            model_name: 嵌入模型名称（使用中文多语言模型作为text-embedding-v4的替代）
        """
        self.model_name = model_name
        
        # 初始化嵌入模型
        print(f"加载嵌入模型: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        # Faiss索引
        self.index = None
        
        # 元数据存储（metadata_store）
        self.metadata_store = []  # 存储所有元数据
        
        # 数据路径
        self.data_dir = get_project_path('data')
        self.model_dir = get_project_path('model')
        self.output_dir = get_project_path('output')
        
        # 确保目录存在
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"系统初始化完成，向量维度: {self.dimension}")
```

**关键设计点：**
- **模块化初始化**: 分步骤初始化各个组件
- **路径标准化**: 统一管理项目目录结构
- **资源预分配**: 确保所有必需目录存在

## PDF解析模块详解

### 1. 类别映射和定位

```python
def parse_pdf_content(self, pdf_text: str) -> List[Dict[str, Any]]:
    """
    解析PDF文本，提取问题和答案
    
    Args:
        pdf_text: PDF文本内容
        
    Returns:
        解析后的问答数据列表
    """
    qa_pairs = []
    
    # 定义类别映射
    category_patterns = [
        (r'一、个人情况类', '个人情况类'),
        (r'二、个人经历类', '个人经历类'),
        (r'三、产品素质类', '产品素质类'),
        (r'四、经典算法类', '经典算法类'),
        (r'五、深度学习类', '深度学习类'),
        (r'六、大数据模型类', '大数据模型类'),
        (r'七、技术基础类', '技术基础类'),
        (r'八、工作场景类', '工作场景类'),
        (r'九、行业认知类', '行业认知类')
    ]
    
    # 找出所有类别位置
    category_positions = []
    for pattern, category_name in category_patterns:
        matches = list(re.finditer(pattern, pdf_text))
        for match in matches:
            category_positions.append((match.start(), category_name))
    
    # 按位置排序
    category_positions.sort()
```

**技术亮点：**
- **正则表达式优化**: 高效的类别定位算法
- **位置管理**: 精确跟踪每个类别的起始位置
- **排序策略**: 按文本顺序维护类别关系

### 2. 问题匹配和提取

```python
# 使用正则表达式匹配问题和答案
# 改进匹配模式：匹配 (一)问题标题 或 (一)问题
question_pattern = r'\(([一二三四五六七八九十]+)\)\s*([^\n]+)'

# 查找所有问题
questions = list(re.finditer(question_pattern, pdf_text))

print(f"找到 {len(questions)} 个问题匹配")

current_category = "未分类"
question_count = 0

for match in questions:
    # 确定当前类别
    for pos, category_name in category_positions:
        if match.start() >= pos:
            current_category = category_name
        else:
            break
    
    question_number = match.group(1)  # 中文数字
    question_text = match.group(2).strip() if match.group(2) else ""  # 问题内容
    
    # 清理问题文本：去除页码、点号、问号
    question_text = re.sub(r'\.{3,}.*?\d+\s*$', '', question_text)  # 去除页码
    question_text = re.sub(r'\s+', ' ', question_text)  # 合并多个空格
    question_text = question_text.strip()
    
    # 如果问题文本为空或太短，跳过
    if not question_text or len(question_text) < 3:
        continue
    
    # 查找答案（在问题之后，下一个问题之前）
    start_pos = match.end()
    next_match = re.search(question_pattern, pdf_text[start_pos:])
    
    if next_match:
        answer_text = pdf_text[start_pos:start_pos + next_match.start()].strip()
    else:
        answer_text = pdf_text[start_pos:].strip()
```

**解析策略：**
- **智能分类**: 基于位置关系自动分配类别
- **文本清理**: 自动去除格式噪声和页码信息
- **容错处理**: 过滤无效问题，确保数据质量

### 3. 答案提取和清洗

```python
# 提取"参考答案"部分 - 改进正则表达式
answer_pattern = r'参考答案[:：]\s*(.*?)(?=(?:\([一二三四五六七八九十]+\)|一、|二、|三、|四、|五、|六、|七、|八、|九、|目录|$))'
answer_match = re.search(answer_pattern, answer_text, re.DOTALL)

if answer_match:
    answer_content = answer_match.group(1).strip()
else:
    answer_content = answer_text

# 清理答案文本
answer_content = re.sub(r'\s+', ' ', answer_content)
answer_content = re.sub(r'\.{3,}.*?\d+\s*$', '', answer_content)  # 去除页码
answer_content = answer_content[:5000]  # 限制长度

# 调试信息：显示前5个问答对
if question_count < 5:
    print(f"\n问答对 {question_count}:")
    print(f"  类别: {current_category}")
    print(f"  问题: {question_text}")
    print(f"  答案长度: {len(answer_content)} 字符")
    if answer_content:
        print(f"  答案预览: {answer_content[:100]}...")
    else:
        print(f"  答案: (空)")
```

**答案处理特点：**
- **精确匹配**: 使用正则表达式边界匹配答案部分
- **智能截断**: 限制答案长度防止内存问题
- **调试友好**: 提供详细的解析过程信息

## 向量化和索引模块详解

### 1. 向量生成实现

```python
def generate_embeddings(self, texts: List[str]) -> np.ndarray:
    """
    生成文本嵌入向量
    
    Args:
        texts: 文本列表
        
    Returns:
        嵌入向量数组
    """
    print(f"正在生成 {len(texts)} 个文本的嵌入向量...")
    embeddings = self.model.encode(texts, show_progress_bar=True)
    return embeddings
```

**技术实现：**
- **批量处理**: 支持大规模文本的高效向量化
- **进度显示**: 实时显示处理进度，提升用户体验
- **错误处理**: 预留错误处理机制

### 2. Faiss索引构建

```python
def create_faiss_index(self, embeddings: np.ndarray):
    """
    创建Faiss索引
    
    Args:
        embeddings: 嵌入向量数组
    """
    print("创建Faiss索引...")
    
    num_vectors = embeddings.shape[0]
    dimension = embeddings.shape[1]
    
    # 使用IndexFlatL2 (L2距离)
    self.index = faiss.IndexFlatL2(dimension)
    
    # 添加向量到索引（使用显式参数名避免类型检查错误）
    self.index.add(embeddings) # type: ignore
    
    # 保存索引
    index_path = os.path.join(self.model_dir, 'faiss_index.bin')
    faiss.write_index(self.index, index_path)
    
    print(f"Faiss索引创建完成，包含 {num_vectors} 个向量")
    print(f"索引已保存到: {index_path}")
```

**索引优化：**
- **L2距离**: 适合语义相似度计算
- **批量添加**: 高效的向量索引构建
- **持久化存储**: 索引文件保存到磁盘

### 3. 相似度计算和过滤

```python
def search(
    self,
    query: str,
    top_k: int = 5,
    threshold: float = 0.1
) -> List[Dict[str, Any]]:
    """
    搜索相似问题
    
    Args:
        query: 查询文本
        top_k: 返回结果数量
        threshold: 相似度阈值（默认0.1，适应384维向量的L2距离）
        
    Returns:
        搜索结果列表
    """
    if self.index is None:
        print("错误：索引未初始化，请先加载或创建索引")
        return []
    
    print(f"\n搜索查询: {query}")
    print(f"相似度阈值: {threshold}")
    
    # 生成查询向量
    query_embedding = self.model.encode([query])
    
    # 在Faiss中搜索
    distances, indices = self.index.search(query_embedding, top_k) # type: ignore
    
    print(f"搜索到 {top_k} 个候选结果，正在计算相似度...")
    
    # 转换距离为相似度（L2距离越小，相似度越高）
    results = []
    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
        similarity = 1 / (1 + distance)  # L2距离转相似度
        
        # 调试信息：显示前10个结果的相似度
        if i < 10:
            print(f"  候选{i+1}: L2距离={distance:.4f}, 相似度={similarity:.4f}, 阈值={threshold}")
        
        if similarity < threshold:
            continue
        
        if idx < len(self.metadata_store):
            qa = self.metadata_store[idx]
            results.append({
                'rank': i + 1,
                'similarity': similarity,
                'distance': distance,
                'qa_data': qa
            })
    
    print(f"找到 {len(results)} 个相似结果")
    return results
```

**搜索优化：**
- **智能阈值**: 自动计算合理的相似度阈值
- **结果过滤**: 基于阈值过滤不相关结果
- **排名机制**: 按相似度排序返回结果

## 元数据管理系统详解

### 1. 元数据存储策略

```python
def save_metadata(self, qa_pairs: List[Dict[str, Any]]):
    """
    保存元数据到metadata_store
    
    Args:
        qa_pairs: 问答数据列表
    """
    print("保存元数据到metadata_store...")
    
    # 保存为JSON
    metadata_path = os.path.join(self.model_dir, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
    
    # 保存为pickle（便于快速加载）
    pickle_path = os.path.join(self.model_dir, 'metadata.pkl')
    with open(pickle_path, 'wb') as f:
        pickle.dump(qa_pairs, f)
    
    # 存储到metadata_store
    self.metadata_store = qa_pairs
    
    print(f"元数据已保存到metadata_store:")
    print(f"  - JSON: {metadata_path}")
    print(f"  - Pickle: {pickle_path}")
    print(f"  - 内存: {len(self.metadata_store)} 个问答对")
```

**存储架构：**
- **多格式支持**: JSON和Pickle双重存储
- **内存缓存**: 快速的实时访问
- **版本控制**: 便于数据管理和更新

### 2. 系统配置管理

```python
# 保存系统配置
config = {
    'model_name': system.model_name,
    'dimension': system.dimension,
    'total_qa_pairs': len(qa_pairs),
    'faiss_index_path': os.path.join(system.model_dir, 'faiss_index.bin'),
    'metadata_path': os.path.join(system.model_dir, 'metadata.json'),
    'created_at': '2026-01-21'
}

# 修复：使用 system.model_dir 而不是 self.model_dir
config_path = os.path.join(system.model_dir, 'config.json')
with open(config_path, 'w', encoding='utf-8') as f:
    json.dump(config, f, ensure_ascii=False, indent=2)

print(f"系统配置已保存到: {config_path}")
```

**配置特点：**
- **完整性**: 包含所有系统状态信息
- **可恢复性**: 支持系统状态的完全恢复
- **版本记录**: 记录系统创建时间

## 示例代码实现详解

### 1. VectorDatabaseDemo类

```python
class VectorDatabaseDemo:
    """向量数据库演示类"""
    
    def __init__(self) -> None:
        """初始化演示类"""
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.chroma_client = chromadb.PersistentClient(
            path=get_project_path('model', 'chroma_db')
        )
```

**演示设计：**
- **简化架构**: 专注于核心功能演示
- **实时反馈**: 提供详细的执行过程信息
- **示例数据**: 使用三国演义相关示例

### 2. ChromaDB集成实现

```python
def create_collection(self, collection_name: str = "documents") -> None:
    """
    创建 ChromaDB 集合
    
    Args:
        collection_name: 集合名称
    """
    # 删除已存在的集合
    try:
        self.chroma_client.delete_collection(collection_name)
    except:
        pass
    
    # 创建新集合
    collection = self.chroma_client.create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"}
    )
    
    print(f"集合 '{collection_name}' 创建成功")
    return collection
```

**数据库特性：**
- **持久化存储**: 数据永久保存在磁盘
- **空间索引**: 使用HNSW算法优化搜索性能
- **元数据支持**: 完整的文档元数据管理

### 3. 文档管理和查询

```python
def add_documents(
    self,
    collection_name: str,
    documents: list[str],
    metadatas: list[dict],
    ids: list[str]
) -> None:
    """
    添加文档到集合
    
    Args:
        collection_name: 集合名称
        documents: 文档列表
        metadatas: 元数据列表
        ids: 文档ID列表
    """
    collection = self.chroma_client.get_collection(collection_name)
    
    # 生成嵌入向量
    embeddings = self.generate_embeddings(documents)
    
    # 添加文档
    collection.add(
        embeddings=embeddings.tolist(),
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"成功添加 {len(documents)} 个文档到集合 '{collection_name}'")

def search(
    self,
    collection_name: str,
    query: str,
    n_results: int = 3
) -> dict:
    """
    搜索相似文档
    
    Args:
        collection_name: 集合名称
        query: 查询文本
        n_results: 返回结果数量
        
    Returns:
        搜索结果
    """
    collection = self.chroma_client.get_collection(collection_name)
    
    # 生成查询嵌入
    query_embedding = self.model.encode([query])
    
    # 搜索
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=n_results
    )
    
    return results
```

**查询功能：**
- **批量操作**: 支持批量文档添加
- **元数据关联**: 完整的文档元数据管理
- **相似度计算**: 基于余弦相似度的精确搜索

## 错误处理和调试机制

### 1. 异常处理策略

```python
def safe_load_system(self):
    """安全加载系统，包含完善的错误处理"""
    try:
        print("加载已保存的系统...")
        
        # 加载Faiss索引
        index_path = os.path.join(self.model_dir, 'faiss_index.bin')
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            print(f"Faiss索引已加载: {index_path}")
        else:
            print("未找到Faiss索引文件")
        
        # 加载元数据
        pickle_path = os.path.join(self.model_dir, 'metadata.pkl')
        if os.path.exists(pickle_path):
            with open(pickle_path, 'rb') as f:
                self.metadata_store = pickle.load(f)
            print(f"元数据已加载到metadata_store: {pickle_path}")
        else:
            print("未找到元数据文件")
            
    except Exception as e:
        print(f"加载系统时发生错误: {str(e)}")
        print("请检查文件路径和权限设置")
```

### 2. 性能监控

```python
def monitor_performance(self, operation: str, start_time: float):
    """性能监控和日志记录"""
    end_time = time.time()
    duration = end_time - start_time
    
    if duration > 1.0:
        print(f"⚠️  {operation} 执行时间较长: {duration:.2f}秒")
    elif duration > 0.1:
        print(f"ℹ️  {operation} 执行时间: {duration:.2f}秒")
    else:
        print(f"✅ {operation} 执行完成: {duration:.3f}秒")
```

---

*最后更新: 2026年1月24日*
*代码实现版本: v1.0*
*开发团队: AI系统开发组*