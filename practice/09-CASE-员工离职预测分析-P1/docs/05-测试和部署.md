# å‘˜å·¥ç¦»èŒé¢„æµ‹åˆ†æé¡¹ç›® - æµ‹è¯•å’Œéƒ¨ç½²

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

#### æ•°æ®å¤„ç†æ¨¡å—æµ‹è¯•

```python
import unittest
import pandas as pd
import numpy as np
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent / "code"))


class TestDataProcessing(unittest.TestCase):
    """æ•°æ®å¤„ç†æ¨¡å—æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.data_dir = Path(__file__).parent.parent / "data"
        self.train_df = pd.read_csv(self.data_dir / "train.csv")
        self.test_df = pd.read_csv(self.data_dir / "test.csv")
    
    def test_data_loading(self):
        """æµ‹è¯•æ•°æ®åŠ è½½"""
        self.assertIsNotNone(self.train_df)
        self.assertIsNotNone(self.test_df)
        self.assertGreater(len(self.train_df), 0)
        self.assertGreater(len(self.test_df), 0)
    
    def test_data_shape(self):
        """æµ‹è¯•æ•°æ®å½¢çŠ¶"""
        self.assertEqual(self.train_df.shape[1], 35)  # 35åˆ—ç‰¹å¾
        self.assertEqual(self.test_df.shape[1], 34)   # æµ‹è¯•é›†æ— ç›®æ ‡å˜é‡
    
    def test_missing_values(self):
        """æµ‹è¯•ç¼ºå¤±å€¼"""
        self.assertEqual(self.train_df.isnull().sum().sum(), 0)
        self.assertEqual(self.test_df.isnull().sum().sum(), 0)
    
    def test_target_variable(self):
        """æµ‹è¯•ç›®æ ‡å˜é‡"""
        self.assertIn('Attrition', self.train_df.columns)
        self.assertNotIn('Attrition', self.test_df.columns)
        
        unique_values = self.train_df['Attrition'].unique()
        self.assertIn('Yes', unique_values)
        self.assertIn('No', unique_values)
    
    def test_feature_types(self):
        """æµ‹è¯•ç‰¹å¾ç±»å‹"""
        numeric_cols = self.train_df.select_dtypes(include=[np.number]).columns
        categorical_cols = self.train_df.select_dtypes(include=['object']).columns
        
        self.assertGreater(len(numeric_cols), 0)
        self.assertGreater(len(categorical_cols), 0)


class TestFeatureEngineering(unittest.TestCase):
    """ç‰¹å¾å·¥ç¨‹æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.sample_data = pd.DataFrame({
            'Age': [30, 40, 25],
            'TotalWorkingYears': [5, 15, 2],
            'YearsAtCompany': [3, 10, 1],
            'JobSatisfaction': [3, 4, 2],
            'EnvironmentSatisfaction': [4, 3, 3],
            'RelationshipSatisfaction': [3, 4, 4],
            'WorkLifeBalance': [3, 2, 3],
            'MonthlyIncome': [5000, 10000, 3000],
            'JobLevel': [1, 3, 1],
            'YearsSinceLastPromotion': [1, 3, 0],
            'OverTime': ['Yes', 'No', 'Yes'],
            'BusinessTravel': ['Travel_Rarely', 'Non-Travel', 'Travel_Frequently']
        })
    
    def test_job_stability_calculation(self):
        """æµ‹è¯•å·¥ä½œç¨³å®šæ€§è®¡ç®—"""
        df = self.sample_data.copy()
        df['JobStability'] = df['YearsAtCompany'] / (df['TotalWorkingYears'] + 1)
        
        expected_values = [3/6, 10/16, 1/3]
        actual_values = df['JobStability'].tolist()
        
        for expected, actual in zip(expected_values, actual_values):
            self.assertAlmostEqual(expected, actual, places=2)
    
    def test_satisfaction_score_calculation(self):
        """æµ‹è¯•æ»¡æ„åº¦ç»¼åˆè¯„åˆ†è®¡ç®—"""
        df = self.sample_data.copy()
        df['SatisfactionScore'] = (
            df['JobSatisfaction'] + 
            df['EnvironmentSatisfaction'] + 
            df['RelationshipSatisfaction'] + 
            df['WorkLifeBalance']
        ) / 4
        
        expected_values = [3.25, 3.25, 3.0]
        actual_values = df['SatisfactionScore'].tolist()
        
        for expected, actual in zip(expected_values, actual_values):
            self.assertAlmostEqual(expected, actual, places=2)
    
    def test_career_stagnation_calculation(self):
        """æµ‹è¯•èŒä¸šå‘å±•åœæ»æŒ‡æ ‡è®¡ç®—"""
        df = self.sample_data.copy()
        df['CareerStagnation'] = df['YearsSinceLastPromotion'] / (df['YearsAtCompany'] + 1)
        
        # éªŒè¯è®¡ç®—ç»“æœ
        self.assertGreaterEqual(df['CareerStagnation'].min(), 0)
        self.assertLessEqual(df['CareerStagnation'].max(), 1)


class TestModelTraining(unittest.TestCase):
    """æ¨¡å‹è®­ç»ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        from sklearn.datasets import make_classification
        from sklearn.model_selection import train_test_split
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        X, y = make_classification(
            n_samples=100,
            n_features=20,
            n_informative=10,
            n_redundant=5,
            random_state=42
        )
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    
    def test_logistic_regression(self):
        """æµ‹è¯•é€»è¾‘å›å½’æ¨¡å‹"""
        from sklearn.linear_model import LogisticRegression
        
        model = LogisticRegression(random_state=42, max_iter=1000)
        model.fit(self.X_train, self.y_train)
        
        predictions = model.predict(self.X_test)
        probabilities = model.predict_proba(self.X_test)
        
        self.assertEqual(len(predictions), len(self.y_test))
        self.assertEqual(probabilities.shape[1], 2)
    
    def test_random_forest(self):
        """æµ‹è¯•éšæœºæ£®æ—æ¨¡å‹"""
        from sklearn.ensemble import RandomForestClassifier
        
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(self.X_train, self.y_train)
        
        predictions = model.predict(self.X_test)
        
        self.assertEqual(len(predictions), len(self.y_test))
        self.assertTrue(hasattr(model, 'feature_importances_'))
    
    def test_model_performance(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score, f1_score
        
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(self.X_train, self.y_train)
        
        predictions = model.predict(self.X_test)
        
        accuracy = accuracy_score(self.y_test, predictions)
        f1 = f1_score(self.y_test, predictions)
        
        # åŸºæœ¬æ€§èƒ½æ£€æŸ¥
        self.assertGreater(accuracy, 0.5)
        self.assertGreater(f1, 0.0)


if __name__ == '__main__':
    unittest.main(verbosity=2)
```

### 2. é›†æˆæµ‹è¯•

#### ç«¯åˆ°ç«¯æµ‹è¯•

```python
import pytest
import tempfile
import os
from pathlib import Path
import pandas as pd
import numpy as np


class TestEndToEnd:
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @pytest.fixture
    def temp_dir(self):
        """åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•"""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir
    
    @pytest.fixture
    def sample_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        n_samples = 100
        
        data = {
            'user_id': range(n_samples),
            'Age': np.random.randint(20, 60, n_samples),
            'Attrition': np.random.choice(['Yes', 'No'], n_samples, p=[0.2, 0.8]),
            'BusinessTravel': np.random.choice(['Travel_Rarely', 'Travel_Frequently', 'Non-Travel'], n_samples),
            'DailyRate': np.random.randint(100, 1500, n_samples),
            'Department': np.random.choice(['Sales', 'Research & Development', 'Human Resources'], n_samples),
            'DistanceFromHome': np.random.randint(1, 30, n_samples),
            'Education': np.random.randint(1, 5, n_samples),
            'EducationField': np.random.choice(['Life Sciences', 'Medical', 'Marketing', 'Technical Degree'], n_samples),
            'EmployeeCount': [1] * n_samples,
            'EmployeeNumber': range(1000, 1000 + n_samples),
            'EnvironmentSatisfaction': np.random.randint(1, 5, n_samples),
            'Gender': np.random.choice(['Male', 'Female'], n_samples),
            'HourlyRate': np.random.randint(30, 100, n_samples),
            'JobInvolvement': np.random.randint(1, 5, n_samples),
            'JobLevel': np.random.randint(1, 5, n_samples),
            'JobRole': np.random.choice(['Sales Executive', 'Research Scientist', 'Manager'], n_samples),
            'JobSatisfaction': np.random.randint(1, 5, n_samples),
            'MaritalStatus': np.random.choice(['Single', 'Married', 'Divorced'], n_samples),
            'MonthlyIncome': np.random.randint(1000, 20000, n_samples),
            'MonthlyRate': np.random.randint(2000, 30000, n_samples),
            'NumCompaniesWorked': np.random.randint(0, 10, n_samples),
            'Over18': ['Y'] * n_samples,
            'OverTime': np.random.choice(['Yes', 'No'], n_samples),
            'PercentSalaryHike': np.random.randint(10, 25, n_samples),
            'PerformanceRating': np.random.choice([3, 4], n_samples),
            'RelationshipSatisfaction': np.random.randint(1, 5, n_samples),
            'StandardHours': [80] * n_samples,
            'StockOptionLevel': np.random.randint(0, 3, n_samples),
            'TotalWorkingYears': np.random.randint(0, 40, n_samples),
            'TrainingTimesLastYear': np.random.randint(0, 6, n_samples),
            'WorkLifeBalance': np.random.randint(1, 5, n_samples),
            'YearsAtCompany': np.random.randint(0, 40, n_samples),
            'YearsInCurrentRole': np.random.randint(0, 20, n_samples),
            'YearsSinceLastPromotion': np.random.randint(0, 20, n_samples),
            'YearsWithCurrManager': np.random.randint(0, 20, n_samples)
        }
        
        return pd.DataFrame(data)
    
    def test_complete_pipeline(self, temp_dir, sample_data):
        """æµ‹è¯•å®Œæ•´æ•°æ®å¤„ç†ç®¡é“"""
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        train_path = os.path.join(temp_dir, "train.csv")
        sample_data.to_csv(train_path, index=False)
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(train_path)
        assert len(df) == 100
        
        # æ•°æ®é¢„å¤„ç†
        X = df.drop(['Attrition', 'user_id', 'EmployeeCount', 'StandardHours', 
                     'EmployeeNumber', 'Over18'], axis=1, errors='ignore')
        y = (df['Attrition'] == 'Yes').astype(int)
        
        # ç¼–ç åˆ†ç±»å˜é‡
        X_encoded = pd.get_dummies(X, drop_first=True)
        
        # åˆ’åˆ†æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_encoded, y, test_size=0.2, random_state=42
        )
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è®­ç»ƒæ¨¡å‹
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        predictions = model.predict(X_test_scaled)
        
        # éªŒè¯
        accuracy = accuracy_score(y_test, predictions)
        assert accuracy > 0.0
        assert len(predictions) == len(y_test)
    
    def test_prediction_output(self, temp_dir, sample_data):
        """æµ‹è¯•é¢„æµ‹è¾“å‡º"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = sample_data.drop('Attrition', axis=1)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        output_path = os.path.join(temp_dir, "predictions.csv")
        results = pd.DataFrame({
            'user_id': test_data['user_id'],
            'Attrition_Predicted': np.random.choice(['Yes', 'No'], len(test_data)),
            'Attrition_Probability': np.random.uniform(0, 1, len(test_data))
        })
        results.to_csv(output_path, index=False)
        
        # éªŒè¯è¾“å‡º
        assert os.path.exists(output_path)
        loaded_results = pd.read_csv(output_path)
        assert len(loaded_results) == len(test_data)
        assert 'Attrition_Predicted' in loaded_results.columns
        assert 'Attrition_Probability' in loaded_results.columns
```

### 3. æ€§èƒ½æµ‹è¯•

```python
import time
import numpy as np
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    def test_training_performance(self):
        """æµ‹è¯•è®­ç»ƒæ€§èƒ½"""
        # åˆ›å»ºè¾ƒå¤§æ•°æ®é›†
        X, y = make_classification(
            n_samples=1000,
            n_features=30,
            n_informative=15,
            random_state=42
        )
        
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        start_time = time.time()
        model.fit(X, y)
        end_time = time.time()
        
        training_time = end_time - start_time
        print(f"\nè®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        
        # éªŒè¯è®­ç»ƒæ—¶é—´åˆç†
        assert training_time < 30  # åº”è¯¥åœ¨30ç§’å†…å®Œæˆ
    
    def test_prediction_performance(self):
        """æµ‹è¯•é¢„æµ‹æ€§èƒ½"""
        X, y = make_classification(
            n_samples=1000,
            n_features=30,
            random_state=42
        )
        
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        # æ‰¹é‡é¢„æµ‹
        start_time = time.time()
        for _ in range(100):
            model.predict(X[:10])
        end_time = time.time()
        
        avg_prediction_time = (end_time - start_time) / 100
        print(f"\nå¹³å‡é¢„æµ‹æ—¶é—´: {avg_prediction_time*1000:.2f}æ¯«ç§’")
        
        # éªŒè¯é¢„æµ‹æ—¶é—´åˆç†
        assert avg_prediction_time < 0.1  # æ¯æ¬¡é¢„æµ‹åº”è¯¥åœ¨100æ¯«ç§’å†…
    
    def test_cross_validation_performance(self):
        """æµ‹è¯•äº¤å‰éªŒè¯æ€§èƒ½"""
        X, y = make_classification(
            n_samples=500,
            n_features=20,
            random_state=42
        )
        
        model = RandomForestClassifier(n_estimators=50, random_state=42)
        
        start_time = time.time()
        scores = cross_val_score(model, X, y, cv=5, scoring='f1')
        end_time = time.time()
        
        cv_time = end_time - start_time
        print(f"\näº¤å‰éªŒè¯æ—¶é—´: {cv_time:.2f}ç§’")
        print(f"F1åˆ†æ•°: {scores.mean():.4f} (+/- {scores.std():.4f})")
        
        # éªŒè¯äº¤å‰éªŒè¯æ—¶é—´åˆç†
        assert cv_time < 60  # åº”è¯¥åœ¨60ç§’å†…å®Œæˆ
```

## éƒ¨ç½²ç­–ç•¥

### 1. å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/data /app/model /app/output

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV DATA_DIR=/app/data
ENV MODEL_DIR=/app/model
ENV OUTPUT_DIR=/app/output

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import pandas; print('OK')" || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "code/check.py"]
```

#### requirements.txt

```
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
xgboost>=2.0.0
lightgbm>=4.0.0
imbalanced-learn>=0.11.0
joblib>=1.3.0
```

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  attrition-prediction:
    build: .
    container_name: attrition-prediction
    volumes:
      - ./data:/app/data
      - ./model:/app/model
      - ./prediction_result:/app/output
    environment:
      - PYTHONUNBUFFERED=1
      - DATA_DIR=/app/data
      - MODEL_DIR=/app/model
      - OUTPUT_DIR=/app/output
    restart: unless-stopped
```

### 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### APIæœåŠ¡éƒ¨ç½²

```python
# api.py
from fastapi import FastAPI, File, UploadFile
from pydantic import BaseModel
import pandas as pd
import numpy as np
import joblib
from pathlib import Path

app = FastAPI(title="å‘˜å·¥ç¦»èŒé¢„æµ‹API")

# åŠ è½½æ¨¡å‹
MODEL_PATH = Path("model/random_forest.joblib")
model = None
scaler = None
feature_cols = None


class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    data: list[dict]


class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    predictions: list[str]
    probabilities: list[float]
    risk_levels: list[str]


def load_model():
    """åŠ è½½æ¨¡å‹"""
    global model, scaler, feature_cols
    if MODEL_PATH.exists():
        saved_data = joblib.load(MODEL_PATH)
        model = saved_data['model']
        scaler = saved_data['scaler']
        feature_cols = saved_data['feature_cols']


@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    load_model()


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model_loaded": model is not None}


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """é¢„æµ‹æ¥å£"""
    if model is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    # è½¬æ¢æ•°æ®
    df = pd.DataFrame(request.data)
    
    # é¢„å¤„ç†
    X = df[feature_cols]
    X_scaled = scaler.transform(X)
    
    # é¢„æµ‹
    predictions = model.predict(X_scaled)
    probabilities = model.predict_proba(X_scaled)[:, 1]
    
    # é£é™©ç­‰çº§
    risk_levels = []
    for prob in probabilities:
        if prob < 0.2:
            risk_levels.append("ä½é£é™©")
        elif prob < 0.5:
            risk_levels.append("ä¸­ä½é£é™©")
        elif prob < 0.8:
            risk_levels.append("ä¸­é«˜é£é™©")
        else:
            risk_levels.append("é«˜é£é™©")
    
    return PredictionResponse(
        predictions=['Yes' if p == 1 else 'No' for p in predictions],
        probabilities=probabilities.tolist(),
        risk_levels=risk_levels
    )


@app.post("/batch_predict")
async def batch_predict(file: UploadFile = File(...)):
    """æ‰¹é‡é¢„æµ‹æ¥å£"""
    if model is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    # è¯»å–æ–‡ä»¶
    df = pd.read_csv(file.file)
    
    # é¢„å¤„ç†
    X = df[feature_cols]
    X_scaled = scaler.transform(X)
    
    # é¢„æµ‹
    predictions = model.predict(X_scaled)
    probabilities = model.predict_proba(X_scaled)[:, 1]
    
    # ç”Ÿæˆç»“æœ
    results = pd.DataFrame({
        'user_id': df['user_id'] if 'user_id' in df.columns else range(len(df)),
        'Attrition_Predicted': ['Yes' if p == 1 else 'No' for p in predictions],
        'Attrition_Probability': probabilities
    })
    
    return results.to_dict(orient='records')


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. ç›‘æ§å’Œæ—¥å¿—

#### æ—¥å¿—é…ç½®

```python
# logging_config.py
import logging
import logging.config
from datetime import datetime
from pathlib import Path

def setup_logging(log_dir: str = "logs"):
    """è®¾ç½®æ—¥å¿—"""
    Path(log_dir).mkdir(exist_ok=True)
    
    LOGGING_CONFIG = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'standard': {
                'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
            },
            'detailed': {
                'format': '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d: %(message)s'
            },
        },
        'handlers': {
            'console': {
                'level': 'INFO',
                'class': 'logging.StreamHandler',
                'formatter': 'standard',
                'stream': 'ext://sys.stdout'
            },
            'file': {
                'level': 'INFO',
                'class': 'logging.handlers.RotatingFileHandler',
                'formatter': 'detailed',
                'filename': f'{log_dir}/attrition_prediction.log',
                'maxBytes': 10485760,  # 10MB
                'backupCount': 5
            },
            'error_file': {
                'level': 'ERROR',
                'class': 'logging.handlers.RotatingFileHandler',
                'formatter': 'detailed',
                'filename': f'{log_dir}/error.log',
                'maxBytes': 10485760,
                'backupCount': 5
            }
        },
        'loggers': {
            '': {
                'handlers': ['console', 'file', 'error_file'],
                'level': 'INFO',
                'propagate': False
            }
        }
    }
    
    logging.config.dictConfig(LOGGING_CONFIG)
    return logging.getLogger(__name__)
```

#### æ€§èƒ½ç›‘æ§

```python
# monitoring.py
import time
from functools import wraps
from logger import setup_logging

logger = setup_logging()


def monitor_performance(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            end_time = time.time()
            duration = end_time - start_time
            logger.info(f"{func.__name__} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return result
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            logger.error(f"{func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œé”™è¯¯: {str(e)}")
            raise
    return wrapper


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def record(self, metric_name: str, value: float):
        """è®°å½•æŒ‡æ ‡"""
        if metric_name not in self.metrics:
            self.metrics[metric_name] = []
        self.metrics[metric_name].append(value)
    
    def get_summary(self, metric_name: str):
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        if metric_name not in self.metrics:
            return None
        
        values = self.metrics[metric_name]
        return {
            'count': len(values),
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values)
        }
```

### 4. CI/CDæµæ°´çº¿

#### GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black
    
    - name: Lint with flake8
      run: |
        flake8 code/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 code/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Format check with black
      run: |
        black --check code/
    
    - name: Run tests
      run: |
        pytest tests/ --cov=code --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t attrition-prediction:${{ github.sha }} .
        docker tag attrition-prediction:${{ github.sha }} attrition-prediction:latest
    
    - name: Push to registry
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push attrition-prediction:${{ github.sha }}
        docker push attrition-prediction:latest
```

## éƒ¨ç½²éªŒè¯

### 1. éƒ¨ç½²åéªŒè¯

```python
# deployment_validator.py
import requests
import time
import logging

logger = logging.getLogger(__name__)

def validate_deployment(base_url: str, timeout: int = 60):
    """éªŒè¯éƒ¨ç½²æ˜¯å¦æˆåŠŸ"""
    
    # 1. å¥åº·æ£€æŸ¥
    health_url = f"{base_url}/health"
    try:
        response = requests.get(health_url, timeout=5)
        if response.status_code == 200:
            logger.info("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥è¶…æ—¶æˆ–å¤±è´¥: {e}")
        return False
    
    # 2. åŠŸèƒ½æµ‹è¯•
    test_data = {
        "data": [
            {
                "Age": 35,
                "DailyRate": 800,
                "DistanceFromHome": 10,
                "Education": 3,
                "EnvironmentSatisfaction": 3,
                "JobSatisfaction": 3,
                "MonthlyIncome": 5000,
                "YearsAtCompany": 5
            }
        ]
    }
    
    predict_url = f"{base_url}/predict"
    try:
        response = requests.post(predict_url, json=test_data, timeout=10)
        if response.status_code == 200:
            result = response.json()
            logger.info(f"âœ… é¢„æµ‹æµ‹è¯•é€šè¿‡ï¼Œç»“æœ: {result}")
        else:
            logger.error(f"âŒ é¢„æµ‹æµ‹è¯•å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹æµ‹è¯•å¼‚å¸¸: {e}")
        return False
    
    logger.info("ğŸ‰ éƒ¨ç½²éªŒè¯å…¨éƒ¨é€šè¿‡")
    return True
```

### 2. å›æ»šç­–ç•¥

```python
# rollback_manager.py
import subprocess
import time
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class RollbackManager:
    """å›æ»šç®¡ç†å™¨"""
    
    def __init__(self, deployment_name: str, namespace: str = "default"):
        self.deployment_name = deployment_name
        self.namespace = namespace
    
    def get_current_version(self) -> Optional[str]:
        """è·å–å½“å‰éƒ¨ç½²ç‰ˆæœ¬"""
        try:
            cmd = [
                "kubectl", "get", "deployment", self.deployment_name,
                "-o", "jsonpath={.spec.template.metadata.labels.version}",
                "-n", self.namespace
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            return result.stdout.strip()
        except Exception as e:
            logger.error(f"è·å–å½“å‰ç‰ˆæœ¬å¤±è´¥: {e}")
            return None
    
    def rollback_to_version(self, version: str, timeout: int = 300) -> bool:
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        try:
            logger.info(f"å¼€å§‹å›æ»šåˆ°ç‰ˆæœ¬: {version}")
            
            cmd = [
                "kubectl", "set", "image", "deployment", self.deployment_name,
                f"{self.deployment_name}=attrition-prediction:{version}",
                "-n", self.namespace
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"å›æ»šå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
                return self._wait_for_rollout(timeout)
            else:
                logger.error(f"å›æ»šå¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"å›æ»šå¼‚å¸¸: {e}")
            return False
    
    def _wait_for_rollout(self, timeout: int) -> bool:
        """ç­‰å¾…å›æ»šå®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                cmd = [
                    "kubectl", "rollout", "status", "deployment", self.deployment_name,
                    "-n", self.namespace
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0:
                    logger.info("âœ… å›æ»šå®Œæˆ")
                    return True
                
                time.sleep(10)
            except Exception as e:
                logger.error(f"æ£€æŸ¥å›æ»šçŠ¶æ€å¤±è´¥: {e}")
                return False
        
        logger.error("âŒ å›æ»šè¶…æ—¶")
        return False
```

---

*æœ€åæ›´æ–°: 2026å¹´2æœˆ15æ—¥*
*æµ‹è¯•å’Œéƒ¨ç½²ç‰ˆæœ¬: v1.0*
*è¿ç»´å›¢é˜Ÿ: DevOpsè¿ç»´ç»„*
