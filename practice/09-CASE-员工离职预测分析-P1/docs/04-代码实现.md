# 员工离职预测分析项目 - 代码实现详解

## 核心代码结构

### 1. 项目路径管理模块

```python
from pathlib import Path

def get_project_path(*paths: str) -> Path:
    """获取项目路径的统一方法"""
    try:
        current_dir = Path(__file__).parent
        project_dir = current_dir.parent
        return project_dir.joinpath(*paths)
    except NameError:
        return Path.cwd().joinpath(*paths)
```

**功能特点：**
- **统一路径管理**: 提供项目路径的标准化获取方法
- **容错处理**: 处理不同运行环境下的路径差异
- **灵活扩展**: 支持任意层级的路径构建

### 2. 数据加载模块

```python
import pandas as pd

def load_data():
    """加载训练和测试数据"""
    data_dir = Path(__file__).parent.parent / "data"
    train_df = pd.read_csv(data_dir / "train.csv")
    test_df = pd.read_csv(data_dir / "test.csv")
    return train_df, test_df
```

**数据加载特点：**
- **相对路径**: 使用相对于代码文件的路径
- **统一接口**: 提供标准化的数据加载函数
- **类型安全**: 返回类型明确的DataFrame

## 数据探索模块详解

### 1. 基本信息分析

```python
def basic_info(df, name="数据集"):
    """输出数据集基本信息"""
    print(f"\n{'='*60}")
    print(f"{name}基本信息")
    print(f"{'='*60}")
    print(f"形状: {df.shape[0]} 行 × {df.shape[1]} 列")
    print(f"\n列名:")
    for i, col in enumerate(df.columns, 1):
        print(f"{i:2d}. {col}")
    
    print(f"\n数据类型:")
    print(df.dtypes.value_counts())
    
    print(f"\n前5行数据:")
    print(df.head())
    
    print(f"\n缺失值统计:")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_df = pd.DataFrame({'缺失数量': missing, '缺失百分比%': missing_pct})
    print(missing_df[missing_df['缺失数量'] > 0])
```

**功能说明：**
- **数据维度**: 显示数据集的行数和列数
- **列名列表**: 按顺序列出所有特征名称
- **类型统计**: 统计不同数据类型的数量
- **数据预览**: 显示前5行数据
- **缺失检查**: 检测并报告缺失值情况

### 2. 目标变量分析

```python
def target_analysis(df, target_col='Attrition'):
    """目标变量分析"""
    print(f"\n{'='*60}")
    print("目标变量分析")
    print(f"{'='*60}")
    
    if target_col in df.columns:
        target_counts = df[target_col].value_counts()
        target_pct = df[target_col].value_counts(normalize=True) * 100
        
        print(f"目标变量 '{target_col}' 分布:")
        for value, count in target_counts.items():
            print(f"  {value}: {count} ({target_pct[value]:.2f}%)")
        
        # 可视化目标变量分布
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        target_counts.plot(kind='bar', color=['skyblue', 'salmon'])
        plt.title(f'{target_col} 分布')
        plt.xlabel(target_col)
        plt.ylabel('数量')
        plt.xticks(rotation=0)
        
        plt.subplot(1, 2, 2)
        plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', 
                colors=['lightblue', 'lightcoral'])
        plt.title(f'{target_col} 占比')
        
        plt.tight_layout()
        plt.savefig('../docs/target_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
    else:
        print(f"目标变量 '{target_col}' 不存在于数据中")
```

**分析内容：**
- **类别统计**: 统计目标变量的各类别数量
- **比例计算**: 计算各类别的百分比
- **可视化展示**: 生成柱状图和饼图
- **文件保存**: 将图表保存到文档目录

### 3. 数值特征分析

```python
def numeric_feature_analysis(df):
    """数值特征分析"""
    print(f"\n{'='*60}")
    print("数值特征分析")
    print(f"{'='*60}")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    print(f"数值特征数量: {len(numeric_cols)}")
    print(f"数值特征: {list(numeric_cols)}")
    
    if len(numeric_cols) > 0:
        print(f"\n数值特征描述性统计:")
        print(df[numeric_cols].describe().T.round(2))
        
        # 相关性分析（仅限部分重要特征）
        important_num_cols = [col for col in numeric_cols 
                             if col not in ['user_id', 'EmployeeNumber', 
                                           'EmployeeCount', 'StandardHours']]
        if len(important_num_cols) > 1:
            correlation = df[important_num_cols].corr()
            print(f"\n数值特征相关性 (前10个特征):")
            print(correlation.iloc[:10, :10])
            
            # 相关性热图
            plt.figure(figsize=(12, 10))
            sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', 
                       center=0, square=True, linewidths=0.5)
            plt.title('数值特征相关性热图')
            plt.tight_layout()
            plt.savefig('../docs/numeric_correlation.png', dpi=300, bbox_inches='tight')
            plt.show()
```

**分析方法：**
- **特征筛选**: 自动识别数值类型特征
- **统计描述**: 计算均值、标准差、分位数等
- **相关性分析**: 计算特征之间的相关系数
- **热图可视化**: 生成相关性热图

### 4. 分类特征分析

```python
def categorical_feature_analysis(df):
    """分类特征分析"""
    print(f"\n{'='*60}")
    print("分类特征分析")
    print(f"{'='*60}")
    
    categorical_cols = df.select_dtypes(include=['object']).columns
    print(f"分类特征数量: {len(categorical_cols)}")
    print(f"分类特征: {list(categorical_cols)}")
    
    for col in categorical_cols:
        print(f"\n特征 '{col}' 的分布:")
        value_counts = df[col].value_counts()
        print(f"  唯一值数量: {df[col].nunique()}")
        for value, count in value_counts.head(10).items():
            pct = count / len(df) * 100
            print(f"  {value}: {count} ({pct:.2f}%)")
        if len(value_counts) > 10:
            print(f"  ... 还有 {len(value_counts) - 10} 个其他值")
```

**分析特点：**
- **自动识别**: 自动检测分类类型特征
- **分布统计**: 统计每个类别的数量和比例
- **唯一值计数**: 报告特征的基数
- **详细输出**: 显示每个类别的详细信息

### 5. 特征与目标关系分析

```python
def feature_target_relationship(df, target_col='Attrition'):
    """特征与目标变量关系分析"""
    if target_col not in df.columns:
        return
    
    print(f"\n{'='*60}")
    print("特征与目标变量关系分析")
    print(f"{'='*60}")
    
    # 分类特征与目标变量的关系
    categorical_cols = df.select_dtypes(include=['object']).columns
    categorical_cols = [col for col in categorical_cols if col != target_col]
    
    for col in categorical_cols[:5]:  # 只分析前5个分类特征
        print(f"\n特征 '{col}' 与 '{target_col}' 的关系:")
        cross_tab = pd.crosstab(df[col], df[target_col], normalize='index') * 100
        print(cross_tab.round(2))
        
        # 可视化
        plt.figure(figsize=(10, 6))
        cross_tab.plot(kind='bar', stacked=True)
        plt.title(f'{col} 与 {target_col} 的关系')
        plt.xlabel(col)
        plt.ylabel('百分比 (%)')
        plt.legend(title=target_col)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'../docs/{col}_vs_{target_col}.png', dpi=300, bbox_inches='tight')
        plt.show()
```

**关系分析：**
- **交叉表分析**: 计算分类特征与目标的交叉统计
- **百分比计算**: 按特征类别计算目标比例
- **堆叠柱状图**: 可视化特征与目标的关系
- **模式发现**: 发现影响离职的关键因素

## 快速检查模块详解

### 1. check.py 完整实现

```python
#!/usr/bin/env python3
"""
快速数据检查
"""

import pandas as pd
import numpy as np
from pathlib import Path

print("员工离职预测数据快速检查")
print("=" * 60)

# 加载数据
data_dir = Path(__file__).parent.parent / "data"
train_df = pd.read_csv(data_dir / "train.csv")
test_df = pd.read_csv(data_dir / "test.csv")

# 训练集信息
print("\n训练集信息:")
print(f"  形状: {train_df.shape[0]} 行 × {train_df.shape[1]} 列")
print(f"  列名: {list(train_df.columns)}")
print(f"  目标变量 'Attrition' 分布:")
if 'Attrition' in train_df.columns:
    print(train_df['Attrition'].value_counts())
    print("  比例:")
    print(train_df['Attrition'].value_counts(normalize=True).round(3))
else:
    print("  目标变量不存在")

# 测试集信息
print("\n测试集信息:")
print(f"  形状: {test_df.shape[0]} 行 × {test_df.shape[1]} 列")
print(f"  是否包含目标变量: {'Attrition' in test_df.columns}")

# 缺失值检查
print("\n训练集缺失值检查:")
missing = train_df.isnull().sum()
missing = missing[missing > 0]
if len(missing) > 0:
    print(missing)
else:
    print("  无缺失值")

# 数据类型
print("\n训练集数据类型分布:")
print(train_df.dtypes.value_counts())

# 数值特征
numeric_cols = train_df.select_dtypes(include=[np.number]).columns
print(f"\n数值特征数量: {len(numeric_cols)}")
print("重要数值特征示例:")
important_num = [col for col in numeric_cols 
                 if col not in ['user_id', 'EmployeeNumber', 
                               'EmployeeCount', 'StandardHours']]
print(important_num[:10])

# 分类特征
categorical_cols = train_df.select_dtypes(include=['object']).columns
print(f"\n分类特征数量: {len(categorical_cols)}")
print("分类特征:")
print(list(categorical_cols))

# 数据质量总结
print("\n数据质量总结:")
print("1. 数据完整，无缺失值")
print("2. 目标变量存在类别不平衡（需要验证）")
print("3. 包含数值和分类特征混合")
print("4. 特征数量较多，适合机器学习建模")

print("\n建议下一步操作:")
print("1. 深入EDA（探索性数据分析）")
print("2. 特征工程：创建新特征，编码分类变量")
print("3. 数据预处理：标准化/归一化数值特征")
print("4. 建立基线模型（如逻辑回归、随机森林）")
print("5. 模型评估与优化")
```

**模块特点：**
- **快速概览**: 快速获取数据基本信息
- **质量检查**: 检测数据质量问题
- **建议输出**: 提供下一步操作建议
- **简洁输出**: 适合快速诊断

## 特征工程实现

### 1. 数据预处理类

```python
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

class DataPreprocessor:
    """数据预处理器"""
    
    def __init__(self):
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.numeric_cols = []
        self.categorical_cols = []
        
    def fit(self, X, y=None):
        """拟合预处理器"""
        # 识别特征类型
        self.numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
        
        # 拟合标准化器
        if self.numeric_cols:
            self.scaler.fit(X[self.numeric_cols])
        
        # 拟合标签编码器
        for col in self.categorical_cols:
            le = LabelEncoder()
            le.fit(X[col].astype(str))
            self.label_encoders[col] = le
        
        return self
    
    def transform(self, X):
        """转换数据"""
        X_transformed = X.copy()
        
        # 标准化数值特征
        if self.numeric_cols:
            X_transformed[self.numeric_cols] = self.scaler.transform(X[self.numeric_cols])
        
        # 编码分类特征
        for col, le in self.label_encoders.items():
            X_transformed[col] = le.transform(X[col].astype(str))
        
        return X_transformed
    
    def fit_transform(self, X, y=None):
        """拟合并转换"""
        return self.fit(X, y).transform(X)
```

### 2. 特征构建器

```python
class FeatureBuilder:
    """特征构建器"""
    
    @staticmethod
    def create_derived_features(df):
        """创建派生特征"""
        df = df.copy()
        
        # 工作稳定性指标
        df['JobStability'] = df['YearsAtCompany'] / (df['TotalWorkingYears'] + 1)
        
        # 满意度综合评分
        df['SatisfactionScore'] = (
            df['JobSatisfaction'] + 
            df['EnvironmentSatisfaction'] + 
            df['RelationshipSatisfaction'] + 
            df['WorkLifeBalance']
        ) / 4
        
        # 薪资相对水平
        if 'JobLevel' in df.columns and 'MonthlyIncome' in df.columns:
            df['IncomeRatio'] = df['MonthlyIncome'] / df.groupby('JobLevel')['MonthlyIncome'].transform('mean')
        
        # 职业发展停滞指标
        df['CareerStagnation'] = df['YearsSinceLastPromotion'] / (df['YearsAtCompany'] + 1)
        
        # 工作强度指标
        if 'OverTime' in df.columns:
            overtime_score = df['OverTime'].map({'Yes': 1, 'No': 0}).fillna(0)
        else:
            overtime_score = 0
            
        if 'BusinessTravel' in df.columns:
            travel_score = df['BusinessTravel'].map({
                'Non-Travel': 0, 
                'Travel_Rarely': 1, 
                'Travel_Frequently': 2
            }).fillna(0)
        else:
            travel_score = 0
            
        df['WorkIntensity'] = overtime_score + travel_score
        
        return df
    
    @staticmethod
    def create_interaction_features(df):
        """创建交互特征"""
        df = df.copy()
        
        # 年龄与工作年限交互
        df['Age_WorkYears_Ratio'] = df['TotalWorkingYears'] / (df['Age'] + 1)
        
        # 薪资与级别交互
        if 'MonthlyIncome' in df.columns and 'JobLevel' in df.columns:
            df['Income_Per_Level'] = df['MonthlyIncome'] / (df['JobLevel'] + 1)
        
        # 满意度与收入交互
        if 'SatisfactionScore' in df.columns and 'MonthlyIncome' in df.columns:
            df['Satisfaction_Income'] = df['SatisfactionScore'] * np.log1p(df['MonthlyIncome'])
        
        return df
```

### 3. 特征选择器

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.feature_selection import RFE

class FeatureSelector:
    """特征选择器"""
    
    def __init__(self, method='kbest', k=20):
        self.method = method
        self.k = k
        self.selected_features = []
        
    def fit(self, X, y):
        """拟合特征选择器"""
        if self.method == 'kbest':
            selector = SelectKBest(score_func=f_classif, k=self.k)
            selector.fit(X, y)
            self.selected_features = X.columns[selector.get_support()].tolist()
            
        elif self.method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_classif, k=self.k)
            selector.fit(X, y)
            self.selected_features = X.columns[selector.get_support()].tolist()
            
        elif self.method == 'correlation':
            # 基于相关性选择
            if hasattr(y, 'name'):
                y_series = y
            else:
                y_series = pd.Series(y)
            
            correlations = X.corrwith(y_series).abs()
            self.selected_features = correlations.nlargest(self.k).index.tolist()
        
        return self
    
    def transform(self, X):
        """转换数据"""
        return X[self.selected_features]
    
    def fit_transform(self, X, y):
        """拟合并转换"""
        return self.fit(X, y).transform(X)
    
    def get_selected_features(self):
        """获取选择的特征"""
        return self.selected_features
```

## 模型训练模块

### 1. 模型训练器

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

class ModelTrainer:
    """模型训练器"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.results = {}
        
    def add_model(self, name, model):
        """添加模型"""
        self.models[name] = model
        
    def train_single_model(self, name, X_train, y_train):
        """训练单个模型"""
        if name not in self.models:
            raise ValueError(f"模型 {name} 未注册")
            
        model = self.models[name]
        model.fit(X_train, y_train)
        return model
    
    def cross_validate(self, name, X, y, cv=5):
        """交叉验证"""
        if name not in self.models:
            raise ValueError(f"模型 {name} 未注册")
            
        model = self.models[name]
        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        scores = {
            'accuracy': cross_val_score(model, X, y, cv=skf, scoring='accuracy'),
            'precision': cross_val_score(model, X, y, cv=skf, scoring='precision'),
            'recall': cross_val_score(model, X, y, cv=skf, scoring='recall'),
            'f1': cross_val_score(model, X, y, cv=skf, scoring='f1'),
            'roc_auc': cross_val_score(model, X, y, cv=skf, scoring='roc_auc')
        }
        
        self.results[name] = {
            metric: {'mean': scores.mean(), 'std': scores.std()}
            for metric, scores in scores.items()
        }
        
        return self.results[name]
    
    def train_all_models(self, X_train, y_train):
        """训练所有模型"""
        for name in self.models:
            self.train_single_model(name, X_train, y_train)
    
    def get_results_df(self):
        """获取结果数据框"""
        rows = []
        for name, metrics in self.results.items():
            for metric, values in metrics.items():
                rows.append({
                    'Model': name,
                    'Metric': metric,
                    'Mean': values['mean'],
                    'Std': values['std']
                })
        return pd.DataFrame(rows)
```

### 2. 模型评估器

```python
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

class ModelEvaluator:
    """模型评估器"""
    
    @staticmethod
    def evaluate(model, X_val, y_val):
        """评估模型"""
        y_pred = model.predict(X_val)
        y_prob = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None
        
        metrics = {
            'accuracy': accuracy_score(y_val, y_pred),
            'precision': precision_score(y_val, y_pred),
            'recall': recall_score(y_val, y_pred),
            'f1': f1_score(y_val, y_pred),
        }
        
        if y_prob is not None:
            metrics['roc_auc'] = roc_auc_score(y_val, y_prob)
        
        return metrics, y_pred, y_prob
    
    @staticmethod
    def plot_confusion_matrix(y_true, y_pred, labels=['No', 'Yes'], title='混淆矩阵'):
        """绘制混淆矩阵"""
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.title(title)
        plt.xlabel('预测值')
        plt.ylabel('真实值')
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_roc_curve(y_true, y_prob, title='ROC曲线'):
        """绘制ROC曲线"""
        fpr, tpr, _ = roc_curve(y_true, y_prob)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                 label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('假正率 (False Positive Rate)')
        plt.ylabel('真正率 (True Positive Rate)')
        plt.title(title)
        plt.legend(loc="lower right")
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def feature_importance(model, feature_names, top_n=20):
        """特征重要性分析"""
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importance = np.abs(model.coef_[0])
        else:
            print("模型不支持特征重要性分析")
            return None
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # 可视化
        plt.figure(figsize=(10, 8))
        plt.barh(range(top_n), importance_df['importance'].head(top_n))
        plt.yticks(range(top_n), importance_df['feature'].head(top_n))
        plt.xlabel('重要性')
        plt.title(f'特征重要性 Top {top_n}')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()
        
        return importance_df
```

## 预测输出模块

### 1. 预测器类

```python
class Predictor:
    """预测器"""
    
    def __init__(self, model, preprocessor=None):
        self.model = model
        self.preprocessor = preprocessor
        
    def predict(self, X):
        """预测"""
        if self.preprocessor:
            X = self.preprocessor.transform(X)
        
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)[:, 1] if hasattr(self.model, 'predict_proba') else None
        
        return predictions, probabilities
    
    def predict_with_risk_level(self, X, thresholds=[0.2, 0.5, 0.8]):
        """带风险等级的预测"""
        predictions, probabilities = self.predict(X)
        
        # 风险等级划分
        risk_levels = []
        for prob in probabilities:
            if prob < thresholds[0]:
                risk_levels.append('低风险')
            elif prob < thresholds[1]:
                risk_levels.append('中低风险')
            elif prob < thresholds[2]:
                risk_levels.append('中高风险')
            else:
                risk_levels.append('高风险')
        
        return predictions, probabilities, risk_levels
    
    def save_predictions(self, X, output_path, id_col='user_id'):
        """保存预测结果"""
        predictions, probabilities, risk_levels = self.predict_with_risk_level(X)
        
        results = pd.DataFrame({
            id_col: X[id_col] if id_col in X.columns else range(len(X)),
            'Attrition_Predicted': ['Yes' if p == 1 else 'No' for p in predictions],
            'Attrition_Probability': probabilities,
            'Risk_Level': risk_levels
        })
        
        results.to_csv(output_path, index=False)
        print(f"预测结果已保存到: {output_path}")
        
        return results
```

### 2. 报告生成器

```python
class ReportGenerator:
    """报告生成器"""
    
    @staticmethod
    def generate_summary_report(results_df, output_path=None):
        """生成汇总报告"""
        report = []
        report.append("=" * 60)
        report.append("员工离职预测分析报告")
        report.append("=" * 60)
        report.append("")
        
        # 预测统计
        report.append("一、预测统计")
        report.append("-" * 40)
        total = len(results_df)
        yes_count = (results_df['Attrition_Predicted'] == 'Yes').sum()
        no_count = (results_df['Attrition_Predicted'] == 'No').sum()
        
        report.append(f"总预测样本数: {total}")
        report.append(f"预测离职人数: {yes_count} ({yes_count/total*100:.1f}%)")
        report.append(f"预测未离职人数: {no_count} ({no_count/total*100:.1f}%)")
        report.append("")
        
        # 风险等级分布
        if 'Risk_Level' in results_df.columns:
            report.append("二、风险等级分布")
            report.append("-" * 40)
            risk_dist = results_df['Risk_Level'].value_counts()
            for level, count in risk_dist.items():
                report.append(f"{level}: {count} ({count/total*100:.1f}%)")
            report.append("")
        
        # 概率分布
        if 'Attrition_Probability' in results_df.columns:
            report.append("三、离职概率分布")
            report.append("-" * 40)
            prob = results_df['Attrition_Probability']
            report.append(f"平均离职概率: {prob.mean():.2%}")
            report.append(f"最大离职概率: {prob.max():.2%}")
            report.append(f"最小离职概率: {prob.min():.2%}")
            report.append(f"标准差: {prob.std():.4f}")
            report.append("")
        
        report.append("=" * 60)
        
        report_text = "\n".join(report)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"报告已保存到: {output_path}")
        
        return report_text
```

## 错误处理和调试

### 1. 异常处理

```python
def safe_load_data(filepath):
    """安全加载数据"""
    try:
        df = pd.read_csv(filepath)
        if df.empty:
            raise ValueError("数据文件为空")
        return df
    except FileNotFoundError:
        raise FileNotFoundError(f"数据文件不存在: {filepath}")
    except pd.errors.EmptyDataError:
        raise ValueError("数据文件为空或格式错误")
    except Exception as e:
        raise Exception(f"加载数据时发生错误: {str(e)}")

def safe_train_model(model, X, y):
    """安全训练模型"""
    try:
        model.fit(X, y)
        return model
    except ValueError as e:
        print(f"训练错误: {str(e)}")
        print("可能原因: 数据维度不匹配或包含无效值")
        return None
    except Exception as e:
        print(f"未知训练错误: {str(e)}")
        return None
```

### 2. 日志记录

```python
import logging
from datetime import datetime

def setup_logging(log_file='employee_attrition.log'):
    """设置日志"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def log_training_progress(model_name, epoch, metrics):
    """记录训练进度"""
    logger.info(f"{model_name} - Epoch {epoch}: {metrics}")
```

---

*最后更新: 2026年2月15日*
*代码实现版本: v1.0*
*开发团队: AI系统开发组*
