# 三国演义词嵌入分析系统 - 技术架构

## 架构概览

三国演义词嵌入分析系统采用模块化分层架构设计，包含数据层、处理层、模型层和应用层四个核心层次，确保系统的可扩展性、可维护性和高性能。

```
┌─────────────────────────────────────────────────────────────┐
│                      应用层 (Application Layer)               │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────┐│
│  │ 词相似度分析 │ │ 词类比计算  │ │ 可视化展示  │ │ API接口 ││
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────┘│
└─────────────────────────────────────────────────────────────┘
                              ↓↑
┌─────────────────────────────────────────────────────────────┐
│                      模型层 (Model Layer)                     │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐           │
│  │ Word2Vec模型│ │  模型管理   │ │  向量存储   │           │
│  └─────────────┘ └─────────────┘ └─────────────┘           │
└─────────────────────────────────────────────────────────────┘
                              ↓↑
┌─────────────────────────────────────────────────────────────┐
│                     处理层 (Processing Layer)                 │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐           │
│  │ 文本预处理  │ │  中文分词   │ │  特征工程   │           │
│  └─────────────┘ └─────────────┘ └─────────────┘           │
└─────────────────────────────────────────────────────────────┘
                              ↓↑
┌─────────────────────────────────────────────────────────────┐
│                      数据层 (Data Layer)                      │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐           │
│  │ 原始文本    │ │  分词语料   │ │  模型文件   │           │
│  └─────────────┘ └─────────────┘ └─────────────┘           │
└─────────────────────────────────────────────────────────────┘
```

## 核心组件架构

### 1. SanguoWord2VecAnalyzer类架构

```python
class SanguoWord2VecAnalyzer:
    """
    三国演义Word2Vec分析器
    
    核心功能模块：
    - 数据加载模块
    - 文本预处理模块  
    - 模型训练模块
    - 相似度分析模块
    - 词类比计算模块
    - 模型持久化模块
    """
    
    def __init__(self, data_path: str) -> None:
        self.data_path: str = data_path        # 数据路径
        self.sentences: list[list[str]] = []   # 分词后的句子列表
        self.model: Optional[Word2Vec] = None  # Word2Vec模型
```

### 2. 数据处理流程

```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│ 原始文本  │ →  │ 文本加载  │ →  │ 中文分词  │ →  │ 句子构建  │
│  (.txt)  │    │          │    │ (jieba)  │    │          │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
                                                     │
                                                     ↓
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│ 模型保存  │ ←  │ 模型训练  │ ←  │ 参数配置  │ ←  │ 词频统计  │
│  (.model) │    │ (Word2Vec)│    │          │    │          │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
```

## Word2Vec技术原理

### 1. 词嵌入基础概念

词嵌入（Word Embedding）是一种将词汇映射到连续向量空间的技术，使得语义相似的词在向量空间中距离相近。

```
词嵌入映射示例：

词汇空间                    向量空间
───────                    ───────
"曹操"   ──────────────→   [0.1, -0.3, 0.8, ..., 0.2]
"刘备"   ──────────────→   [0.2, -0.2, 0.7, ..., 0.3]
"孙权"   ──────────────→   [0.1, -0.4, 0.6, ..., 0.1]

语义相似性通过向量距离体现：
- "曹操"与"刘备"的距离 < "曹操"与"诸葛亮"的距离
```

### 2. CBOW模型架构

Continuous Bag of Words (CBOW) 通过上下文预测中心词：

```
输入层（上下文词）          隐藏层           输出层（中心词）
     ┌─────┐
     │ w(t-2)│───┐
     └─────┘   │
     ┌─────┐   │    ┌─────────┐    ┌─────┐
     │ w(t-1)│───┼───→│ 投影层   │───→│ w(t) │
     └─────┘   │    │ (求平均) │    └─────┘
     ┌─────┐   │    └─────────┘
     │ w(t+1)│───┤
     └─────┘   │
     ┌─────┐   │
     │ w(t+2)│───┘
     └─────┘

训练目标：最大化 P(w(t) | w(t-2), w(t-1), w(t+1), w(t+2))
```

### 3. Skip-gram模型架构

Skip-gram 通过中心词预测上下文：

```
输入层（中心词）           隐藏层           输出层（上下文词）
     ┌─────┐                              ┌─────┐
     │      │    ┌─────────┐    ┌────────→│ w(t-2)│
     │ w(t) │───→│ 投影层   │───┐│        └─────┘
     │      │    │ (向量)   │  ││        ┌─────┐
     └─────┘    └─────────┘  │├────────→│ w(t-1)│
                              ││        └─────┘
                              ││        ┌─────┐
                              │├────────→│ w(t+1)│
                              ││        └─────┘
                              ││        ┌─────┐
                              └────────→│ w(t+2)│
                                       └─────┘

训练目标：最大化 ∏ P(w(context) | w(t))
```

### 4. 两种模型对比

| 特性 | CBOW | Skip-gram |
|------|------|-----------|
| 预测方向 | 上下文→中心词 | 中心词→上下文 |
| 训练速度 | 快（3-4倍） | 慢 |
| 低频词效果 | 较差 | 更好 |
| 高频词效果 | 较好 | 良好 |
| 语义精度 | 中等 | 更高 |
| 内存占用 | 较低 | 较高 |
| 适用场景 | 大数据集、快速训练 | 小数据集、精确语义 |

### 5. 损失函数

**CBOW损失函数**：
```
L = -∑ log P(w_t | w_context)
  = -∑ log (exp(v'_w_t^T · v_context) / ∑ exp(v'_w_i^T · v_context))
```

**Skip-gram损失函数**：
```
L = -∑∑ log P(w_c | w_t)
  = -∑∑ log (exp(v'_w_c^T · v_w_t) / ∑ exp(v'_w_i^T · v_w_t))
```

其中：
- `v_w` 是输入词向量
- `v'_w` 是输出词向量
- 求和是对词汇表中所有词进行

## 中文分词技术

### 1. jieba分词原理

jieba是目前最流行的中文分词工具，采用以下算法组合：

```
分词算法流程：

1. 基于词典的正向最大匹配（FMM）
   输入："三国演义是中国古典名著"
   词典：[三国演义, 是, 中国, 古典, 名著]
   结果：["三国演义", "是", "中国", "古典", "名著"]

2. 基于词典的逆向最大匹配（BMM）
   从右向左匹配，处理歧义

3. 基于HMM的未登录词识别
   使用隐马尔可夫模型识别新词
```

### 2. 分词模式

```python
import jieba

text = "三国演义是中国古典四大名著之一"

# 1. 精确模式（默认）
words = jieba.lcut(text)
# ['三国演义', '是', '中国', '古典', '四大名著', '之一']

# 2. 全模式
words = jieba.lcut(text, cut_all=True)
# ['三国', '三国演义', '演义', '是', '中国', '古典', '四大', '名著', '之一']

# 3. 搜索引擎模式
words = jieba.lcut_for_search(text)
# ['三国', '演义', '三国演义', '是', '中国', '古典', '四大', '名著', '四大名著', '之一']
```

### 3. 自定义词典

```python
# 添加自定义词汇
jieba.add_word('诸葛亮')      # 人名
jieba.add_word('草船借箭')    # 成语
jieba.add_word('赤壁之战')    # 事件

# 删除词汇
jieba.del_word('某词')

# 加载词典文件
jieba.load_userdict('user_dict.txt')
```

## 系统配置架构

### 1. 项目路径管理

```python
from practice.shared import get_project_path

def get_project_path(*paths: str) -> str:
    """
    获取项目路径的统一方法
    
    Args:
        *paths: 相对路径组件
        
    Returns:
        绝对路径字符串
    """
    try:
        current_dir = Path(__file__).parent
        project_dir = current_dir.parent.parent  # 向上两级到practice目录
        return str(project_dir.joinpath(*paths))
    except NameError:
        return str(Path.cwd().joinpath(*paths))
```

### 2. 目录结构

```
CASE-三国演义Embedding/
├── data/                    # 原始数据
│   └── three_kingdoms.txt   # 三国演义原文
├── model/                   # 模型存储
│   └── three_kingdoms.model # Word2Vec模型文件
├── code/                    # 代码文件
│   ├── three_kingdoms.py    # 主程序
│   ├── word2vec_demo.py     # 演示程序
│   └── test_environment.py  # 环境测试
├── docs/                    # 文档目录
│   ├── 00-文档索引.md
│   ├── 01-项目概述.md
│   ├── 02-技术架构.md
│   ├── 03-使用指南.md
│   ├── 04-代码实现.md
│   └── 05-测试和部署.md
├── .venv/                   # 虚拟环境
├── pyproject.toml           # 项目配置
└── README.md                # 项目说明
```

### 3. 配置管理

```python
# 默认训练配置
DEFAULT_CONFIG = {
    'vector_size': 100,      # 向量维度
    'window': 5,             # 上下文窗口
    'min_count': 5,          # 最小词频
    'workers': 4,            # 并行线程
    'sg': 0,                 # 0=CBOW, 1=Skip-gram
    'epochs': 100,           # 训练轮数
    'seed': 42,              # 随机种子
}

# 高精度配置
HIGH_QUALITY_CONFIG = {
    'vector_size': 200,
    'window': 10,
    'min_count': 3,
    'workers': 4,
    'sg': 1,                 # Skip-gram
    'epochs': 200,
    'seed': 42,
}
```

## 性能优化架构

### 1. 内存优化

```python
# 批量处理大数据集
def process_large_corpus(corpus_path: str, batch_size: int = 10000):
    """
    分批处理大型语料库
    """
    sentences = []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        batch = []
        for line in f:
            words = jieba.lcut(line.strip())
            batch.extend(words)
            
            if len(batch) >= batch_size:
                # 每10个词构建一个句子
                for i in range(0, len(batch), 10):
                    sentences.append(batch[i:i+10])
                batch = []
        
        # 处理剩余数据
        if batch:
            for i in range(0, len(batch), 10):
                sentences.append(batch[i:i+10])
    
    return sentences
```

### 2. 并行训练优化

```python
import multiprocessing

# 自动检测CPU核心数
def get_optimal_workers():
    cpu_count = multiprocessing.cpu_count()
    # 预留1-2个核心给系统
    return max(1, cpu_count - 2)

# 训练配置
params = {
    'workers': get_optimal_workers(),  # 自动设置并行数
    # ... 其他参数
}
```

### 3. 模型缓存策略

```python
class ModelCache:
    """模型缓存管理器"""
    
    def __init__(self):
        self.cache = {}
        self.max_size = 5  # 最多缓存5个模型
    
    def get_model(self, model_path: str) -> Word2Vec:
        """获取模型（优先从缓存）"""
        if model_path in self.cache:
            return self.cache[model_path]
        
        model = Word2Vec.load(model_path)
        
        # LRU缓存管理
        if len(self.cache) >= self.max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[model_path] = model
        return model
```

## 安全架构

### 1. 数据安全
- **输入验证**: 对所有输入路径进行验证
- **异常处理**: 完善的异常捕获和处理机制
- **数据备份**: 定期备份重要模型和数据

### 2. 访问控制
- **路径安全**: 使用绝对路径，防止路径遍历
- **文件权限**: 合理设置文件和目录权限
- **资源限制**: 限制内存和CPU使用

### 3. 错误处理

```python
def safe_load_model(model_path: str) -> Optional[Word2Vec]:
    """安全加载模型"""
    try:
        if not os.path.exists(model_path):
            logger.error(f"模型文件不存在: {model_path}")
            return None
        
        if not model_path.endswith('.model'):
            logger.warning(f"非标准模型文件: {model_path}")
        
        model = Word2Vec.load(model_path)
        logger.info(f"模型加载成功: {model_path}")
        return model
        
    except Exception as e:
        logger.error(f"模型加载失败: {e}")
        return None
```

## 扩展架构

### 1. 插件化设计

```python
from abc import ABC, abstractmethod

class TextAnalyzer(ABC):
    """文本分析器基类"""
    
    @abstractmethod
    def train(self, corpus: list) -> None:
        """训练模型"""
        pass
    
    @abstractmethod
    def analyze(self, query: str) -> dict:
        """分析查询"""
        pass


class Word2VecAnalyzer(TextAnalyzer):
    """Word2Vec分析器实现"""
    
    def train(self, corpus: list) -> None:
        self.model = Word2Vec(corpus, **self.config)
    
    def analyze(self, query: str) -> dict:
        similar_words = self.model.wv.most_similar(query)
        return {'similar_words': similar_words}


class FastTextAnalyzer(TextAnalyzer):
    """FastText分析器实现"""
    
    def train(self, corpus: list) -> None:
        self.model = FastText(corpus, **self.config)
    
    def analyze(self, query: str) -> dict:
        similar_words = self.model.wv.most_similar(query)
        return {'similar_words': similar_words}
```

### 2. 多模型支持

```python
class MultiModelManager:
    """多模型管理器"""
    
    def __init__(self):
        self.models = {}
    
    def register_model(self, name: str, model: TextAnalyzer) -> None:
        """注册模型"""
        self.models[name] = model
    
    def get_model(self, name: str) -> Optional[TextAnalyzer]:
        """获取模型"""
        return self.models.get(name)
    
    def compare_results(self, query: str) -> dict:
        """比较多模型结果"""
        results = {}
        for name, model in self.models.items():
            results[name] = model.analyze(query)
        return results
```

### 3. API服务扩展

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="三国演义词嵌入API")

class SimilarityRequest(BaseModel):
    word: str
    topn: int = 10

class AnalogyRequest(BaseModel):
    positive: list[str]
    negative: list[str] = []
    topn: int = 5

@app.post("/similar")
def get_similar_words(request: SimilarityRequest):
    """获取相似词"""
    similar = model.wv.most_similar(request.word, topn=request.topn)
    return {"word": request.word, "similar_words": similar}

@app.post("/analogy")
def get_analogy(request: AnalogyRequest):
    """词类比计算"""
    result = model.wv.most_similar(
        positive=request.positive,
        negative=request.negative,
        topn=request.topn
    )
    return {"analogy_result": result}
```

---

*最后更新: 2026年2月15日*
*架构版本: v1.0*
*技术负责人: AI系统架构组*
