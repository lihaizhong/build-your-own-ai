# ä¸‰å›½æ¼”ä¹‰è¯åµŒå…¥åˆ†æç³»ç»Ÿ - æµ‹è¯•å’Œéƒ¨ç½²

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

#### SanguoWord2VecAnalyzerç±»æµ‹è¯•

```python
import unittest
import tempfile
import os
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from gensim.models import Word2Vec
import numpy as np

from code.three_kingdoms import SanguoWord2VecAnalyzer


class TestSanguoWord2VecAnalyzer(unittest.TestCase):
    """SanguoWord2VecAnalyzerç±»çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
        self.temp_dir = tempfile.mkdtemp()
        self.test_data_path = os.path.join(self.temp_dir, 'test_text.txt')
        
        # å†™å…¥æµ‹è¯•æ•°æ®
        test_text = """
        æ›¹æ“æ˜¯ä¸‰å›½æ—¶æœŸçš„é‡è¦äººç‰©ã€‚
        åˆ˜å¤‡æ˜¯èœ€æ±‰çš„å¼€å›½çš‡å¸ã€‚
        è¯¸è‘›äº®æ˜¯è‘—åçš„æ”¿æ²»å®¶ã€‚
        å­™æƒæ˜¯ä¸œå´çš„å»ºç«‹è€…ã€‚
        """
        with open(self.test_data_path, 'w', encoding='utf-8') as f:
            f.write(test_text)
        
        self.analyzer = SanguoWord2VecAnalyzer(self.test_data_path)
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        import shutil
        shutil.rmtree(self.temp_dir)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertEqual(self.analyzer.data_path, self.test_data_path)
        self.assertEqual(self.analyzer.sentences, [])
        self.assertIsNone(self.analyzer.model)
    
    def test_load_and_preprocess(self):
        """æµ‹è¯•æ–‡æœ¬åŠ è½½å’Œé¢„å¤„ç†"""
        self.analyzer.load_and_preprocess()
        
        # éªŒè¯å¥å­åˆ—è¡¨ä¸ä¸ºç©º
        self.assertGreater(len(self.analyzer.sentences), 0)
        
        # éªŒè¯æ¯ä¸ªå¥å­æ˜¯è¯åˆ—è¡¨
        for sentence in self.analyzer.sentences:
            self.assertIsInstance(sentence, list)
            for word in sentence:
                self.assertIsInstance(word, str)
    
    def test_train_word2vec(self):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒ"""
        # å…ˆé¢„å¤„ç†
        self.analyzer.load_and_preprocess()
        
        # è®­ç»ƒæ¨¡å‹
        self.analyzer.train_word2vec()
        
        # éªŒè¯æ¨¡å‹å·²åˆ›å»º
        self.assertIsNotNone(self.analyzer.model)
        self.assertIsInstance(self.analyzer.model, Word2Vec)
        
        # éªŒè¯è¯æ±‡è¡¨ä¸ä¸ºç©º
        self.assertGreater(len(self.analyzer.model.wv), 0)
    
    def test_analyze_similar_words(self):
        """æµ‹è¯•ç›¸ä¼¼è¯åˆ†æ"""
        # å‡†å¤‡æ•°æ®
        self.analyzer.load_and_preprocess()
        self.analyzer.train_word2vec()
        
        # æµ‹è¯•å·²çŸ¥è¯
        # æ³¨æ„ï¼šæµ‹è¯•æ•°æ®é‡å°ï¼Œå¯èƒ½æŸäº›è¯ä¸åœ¨è¯æ±‡è¡¨ä¸­
        vocabulary = list(self.analyzer.model.wv.key_to_index.keys())
        if vocabulary:
            test_word = vocabulary[0]
            result = self.analyzer.analyze_similar_words(test_word, topn=3)
            
            # éªŒè¯è¿”å›ç±»å‹
            self.assertIsInstance(result, list)
            if result:
                # éªŒè¯è¿”å›æ ¼å¼
                word, similarity = result[0]
                self.assertIsInstance(word, str)
                self.assertIsInstance(similarity, float)
    
    def test_word_analogy(self):
        """æµ‹è¯•è¯ç±»æ¯”"""
        self.analyzer.load_and_preprocess()
        self.analyzer.train_word2vec()
        
        vocabulary = list(self.analyzer.model.wv.key_to_index.keys())
        if len(vocabulary) >= 3:
            # æµ‹è¯•è¯ç±»æ¯”
            result = self.analyzer.word_analogy(
                positive_words=[vocabulary[0]],
                negative_words=[],
                topn=3
            )
            self.assertIsInstance(result, list)
    
    def test_save_and_load_model(self):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        # è®­ç»ƒæ¨¡å‹
        self.analyzer.load_and_preprocess()
        self.analyzer.train_word2vec()
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.temp_dir, 'test_model.model')
        self.analyzer.save_model(model_path)
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(model_path))
        
        # åˆ›å»ºæ–°åˆ†æå™¨å¹¶åŠ è½½æ¨¡å‹
        new_analyzer = SanguoWord2VecAnalyzer(self.test_data_path)
        new_analyzer.load_model(model_path)
        
        # éªŒè¯æ¨¡å‹åŠ è½½æˆåŠŸ
        self.assertIsNotNone(new_analyzer.model)


class TestWordSimilarity(unittest.TestCase):
    """è¯ç›¸ä¼¼åº¦åŠŸèƒ½æµ‹è¯•"""
    
    def setUp(self):
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        # åˆ›å»ºå°å‹æµ‹è¯•è¯­æ–™
        self.sentences = [
            ['æ›¹æ“', 'é­å›½', 'æ”¿æ²»å®¶'],
            ['åˆ˜å¤‡', 'èœ€æ±‰', 'çš‡å¸'],
            ['å­™æƒ', 'ä¸œå´', 'å›ä¸»'],
            ['è¯¸è‘›äº®', 'èœ€æ±‰', 'è°‹å£«'],
            ['å…³ç¾½', 'èœ€æ±‰', 'å°†å†›'],
            ['å¼ é£', 'èœ€æ±‰', 'å°†å†›'],
        ]
        
        self.model = Word2Vec(
            self.sentences,
            vector_size=50,
            window=2,
            min_count=1,
            epochs=50
        )
    
    def test_similarity_calculation(self):
        """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
        # æµ‹è¯•åŒä¸€è¯çš„ç›¸ä¼¼åº¦
        similarity = self.model.wv.similarity('æ›¹æ“', 'æ›¹æ“')
        self.assertAlmostEqual(similarity, 1.0, places=5)
        
        # æµ‹è¯•ä¸åŒè¯çš„ç›¸ä¼¼åº¦
        similarity = self.model.wv.similarity('å…³ç¾½', 'å¼ é£')
        self.assertGreater(similarity, -1.0)
        self.assertLess(similarity, 1.0)
    
    def test_most_similar(self):
        """æµ‹è¯•æœ€ç›¸ä¼¼è¯æŸ¥è¯¢"""
        similar = self.model.wv.most_similar('æ›¹æ“', topn=3)
        
        # éªŒè¯è¿”å›æ•°é‡
        self.assertEqual(len(similar), 3)
        
        # éªŒè¯æ ¼å¼
        for word, sim in similar:
            self.assertIn(word, self.model.wv)
            self.assertGreater(sim, -1.0)
    
    def test_doesnt_match(self):
        """æµ‹è¯•æ‰¾å‡ºä¸åˆç¾¤çš„è¯"""
        words = ['å…³ç¾½', 'å¼ é£', 'æ›¹æ“']  # æ›¹æ“ä¸å±äºèœ€æ±‰
        outlier = self.model.wv.doesnt_match(words)
        # ç»“æœå¯èƒ½æ˜¯ä»»æ„ä¸€ä¸ªï¼Œå–å†³äºè®­ç»ƒ
        self.assertIn(outlier, words)


class TestWordAnalogy(unittest.TestCase):
    """è¯ç±»æ¯”åŠŸèƒ½æµ‹è¯•"""
    
    def setUp(self):
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        self.sentences = [
            ['æ›¹æ“', 'é­å›½'],
            ['åˆ˜å¤‡', 'èœ€æ±‰'],
            ['å­™æƒ', 'ä¸œå´'],
            ['è¯¸è‘›äº®', 'èœ€æ±‰'],
        ]
        
        self.model = Word2Vec(
            self.sentences,
            vector_size=50,
            window=2,
            min_count=1,
            epochs=100
        )
    
    def test_analogy_calculation(self):
        """æµ‹è¯•è¯ç±»æ¯”è®¡ç®—"""
        # æ›¹æ“å¯¹åº”é­å›½ï¼Œåˆ˜å¤‡å¯¹åº”èœ€æ±‰
        result = self.model.wv.most_similar(
            positive=['åˆ˜å¤‡', 'é­å›½'],
            negative=['æ›¹æ“'],
            topn=3
        )
        
        # éªŒè¯è¿”å›ç»“æœ
        self.assertEqual(len(result), 3)
        
        for word, sim in result:
            self.assertIn(word, self.model.wv)


class TestChineseSegmentation(unittest.TestCase):
    """ä¸­æ–‡åˆ†è¯æµ‹è¯•"""
    
    def test_jieba_segmentation(self):
        """æµ‹è¯•jiebaåˆ†è¯"""
        import jieba
        
        text = "ä¸‰å›½æ¼”ä¹‰æ˜¯ä¸­å›½å¤å…¸å››å¤§åè‘—ä¹‹ä¸€"
        words = jieba.lcut(text)
        
        # éªŒè¯åˆ†è¯ç»“æœ
        self.assertIsInstance(words, list)
        self.assertGreater(len(words), 0)
        
        # éªŒè¯åŒ…å«å…³é”®åˆ†è¯
        self.assertIn('ä¸‰å›½æ¼”ä¹‰', words)
    
    def test_custom_dictionary(self):
        """æµ‹è¯•è‡ªå®šä¹‰è¯å…¸"""
        import jieba
        
        # æ·»åŠ è‡ªå®šä¹‰è¯
        jieba.add_word('è‰èˆ¹å€Ÿç®­')
        
        text = "è¯¸è‘›äº®è‰èˆ¹å€Ÿç®­"
        words = jieba.lcut(text)
        
        # éªŒè¯è‡ªå®šä¹‰è¯è¢«æ­£ç¡®åˆ†è¯
        self.assertIn('è‰èˆ¹å€Ÿç®­', words)


if __name__ == '__main__':
    unittest.main(verbosity=2)
```

### 2. é›†æˆæµ‹è¯•

```python
import pytest
import tempfile
import os


class TestIntegration:
    """é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def temp_project(self):
        """åˆ›å»ºä¸´æ—¶é¡¹ç›®ç¯å¢ƒ"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºæ•°æ®ç›®å½•
            data_dir = os.path.join(temp_dir, 'data')
            model_dir = os.path.join(temp_dir, 'model')
            os.makedirs(data_dir)
            os.makedirs(model_dir)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
            data_path = os.path.join(data_dir, 'test_text.txt')
            with open(data_path, 'w', encoding='utf-8') as f:
                f.write("æ›¹æ“æ˜¯é­å›½çš„å¥ åŸºäººã€‚åˆ˜å¤‡æ˜¯èœ€æ±‰çš„çš‡å¸ã€‚å­™æƒæ˜¯ä¸œå´çš„å›ä¸»ã€‚")
            
            yield {
                'temp_dir': temp_dir,
                'data_path': data_path,
                'model_dir': model_dir
            }
    
    def test_full_pipeline(self, temp_project):
        """æµ‹è¯•å®Œæ•´å¤„ç†æµç¨‹"""
        from code.three_kingdoms import SanguoWord2VecAnalyzer
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = SanguoWord2VecAnalyzer(temp_project['data_path'])
        
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        analyzer.load_and_preprocess()
        analyzer.train_word2vec()
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(temp_project['model_dir'], 'test.model')
        analyzer.save_model(model_path)
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶å­˜åœ¨
        assert os.path.exists(model_path)
        
        # é‡æ–°åŠ è½½æ¨¡å‹
        new_analyzer = SanguoWord2VecAnalyzer(temp_project['data_path'])
        new_analyzer.load_model(model_path)
        
        # éªŒè¯æ¨¡å‹å¯ç”¨
        assert new_analyzer.model is not None
    
    def test_model_persistence(self, temp_project):
        """æµ‹è¯•æ¨¡å‹æŒä¹…åŒ–"""
        from gensim.models import Word2Vec
        
        # è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
        sentences = [['æ›¹æ“', 'åˆ˜å¤‡'], ['å­™æƒ', 'å‘¨ç‘œ']]
        model = Word2Vec(sentences, vector_size=50, min_count=1, epochs=10)
        
        model_path = os.path.join(temp_project['model_dir'], 'persist.model')
        model.save(model_path)
        
        # é‡æ–°åŠ è½½
        loaded_model = Word2Vec.load(model_path)
        
        # éªŒè¯æ¨¡å‹ä¸€è‡´æ€§
        assert len(loaded_model.wv) == len(model.wv)
        assert loaded_model.wv.vector_size == model.wv.vector_size
```

### 3. æ€§èƒ½æµ‹è¯•

```python
import time
import numpy as np


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    def test_training_performance(self):
        """æµ‹è¯•è®­ç»ƒæ€§èƒ½"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        sentences = [['è¯'] * 10 for _ in range(10000)]
        
        start_time = time.time()
        model = Word2Vec(
            sentences,
            vector_size=100,
            window=5,
            min_count=1,
            epochs=10
        )
        end_time = time.time()
        
        duration = end_time - start_time
        print(f"\nè®­ç»ƒ10000ä¸ªå¥å­è€—æ—¶: {duration:.2f}ç§’")
        
        # éªŒè¯è®­ç»ƒæ—¶é—´åˆç†ï¼ˆé€šå¸¸åº”å°äº30ç§’ï¼‰
        assert duration < 30, "è®­ç»ƒæ—¶é—´è¿‡é•¿"
    
    def test_query_performance(self):
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        sentences = [[f'è¯{i}' for i in range(10)] for _ in range(1000)]
        model = Word2Vec(sentences, vector_size=100, min_count=1, epochs=10)
        
        # æµ‹è¯•å•æ¬¡æŸ¥è¯¢
        start_time = time.time()
        for _ in range(100):
            model.wv.most_similar('è¯0', topn=10)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100
        print(f"\nå¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_time*1000:.2f}æ¯«ç§’")
        
        # éªŒè¯æŸ¥è¯¢æ—¶é—´åˆç†ï¼ˆé€šå¸¸åº”å°äº10msï¼‰
        assert avg_time < 0.01, "æŸ¥è¯¢æ—¶é—´è¿‡é•¿"
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import sys
        
        # åˆ›å»ºæ¨¡å‹å‰çš„å†…å­˜
        before_size = sys.getsizeof([])
        
        # åˆ›å»ºæ¨¡å‹
        sentences = [[f'è¯{i}'] for i in range(10000)]
        model = Word2Vec(sentences, vector_size=100, min_count=1, epochs=1)
        
        # ä¼°ç®—æ¨¡å‹å¤§å°
        model_size = sys.getsizeof(model) + \
                     sys.getsizeof(model.wv.vectors) + \
                     sum(sys.getsizeof(k) for k in model.wv.key_to_index.keys())
        
        print(f"\næ¨¡å‹ä¼°ç®—å¤§å°: {model_size / 1024 / 1024:.2f}MB")
        
        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
        assert model_size < 100 * 1024 * 1024, "å†…å­˜ä½¿ç”¨è¿‡é«˜"
```

## æ¨¡å‹è´¨é‡è¯„ä¼°

### 1. å†…éƒ¨è¯„ä¼°æ–¹æ³•

```python
def evaluate_model_quality(model, test_cases: list) -> dict:
    """
    è¯„ä¼°æ¨¡å‹è´¨é‡
    
    Args:
        model: Word2Vecæ¨¡å‹
        test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    results = {
        'total': len(test_cases),
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    for case in test_cases:
        word_a, word_b, expected_relation = case
        
        try:
            similarity = model.wv.similarity(word_a, word_b)
            
            # æ ¹æ®é¢„æœŸå…³ç³»åˆ¤æ–­
            passed = False
            if expected_relation == 'high' and similarity > 0.5:
                passed = True
            elif expected_relation == 'medium' and 0.2 < similarity < 0.7:
                passed = True
            elif expected_relation == 'low' and similarity < 0.3:
                passed = True
            
            if passed:
                results['passed'] += 1
            else:
                results['failed'] += 1
            
            results['details'].append({
                'words': (word_a, word_b),
                'similarity': similarity,
                'expected': expected_relation,
                'passed': passed
            })
            
        except KeyError as e:
            results['failed'] += 1
            results['details'].append({
                'words': (word_a, word_b),
                'error': str(e),
                'passed': False
            })
    
    results['accuracy'] = results['passed'] / results['total']
    return results


# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    ('å…³ç¾½', 'å¼ é£', 'high'),      # å…„å¼Ÿå…³ç³»
    ('æ›¹æ“', 'åˆ˜å¤‡', 'medium'),    # å¯¹æ‰‹å…³ç³»
    ('è¯¸è‘›äº®', 'å­”æ˜', 'high'),    # åŒä¸€äºº
    ('é­å›½', 'èœ€æ±‰', 'medium'),    # å›½å¯¹å›½
    ('é©¬', 'è¯¸è‘›äº®', 'low'),       # æ— å…³è¯
]

# è¿è¡Œè¯„ä¼°
# results = evaluate_model_quality(model, test_cases)
# print(f"æ¨¡å‹å‡†ç¡®ç‡: {results['accuracy']:.2%}")
```

### 2. ç±»æ¯”å‡†ç¡®ç‡è¯„ä¼°

```python
def evaluate_analogy_accuracy(model, analogy_file: str) -> dict:
    """
    è¯„ä¼°è¯ç±»æ¯”å‡†ç¡®ç‡
    
    Args:
        model: Word2Vecæ¨¡å‹
        analogy_file: ç±»æ¯”æµ‹è¯•æ–‡ä»¶è·¯å¾„
        
    Returns:
        è¯„ä¼°ç»“æœ
    """
    correct = 0
    total = 0
    skipped = 0
    
    with open(analogy_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith(':'):
                continue
            
            parts = line.strip().split()
            if len(parts) != 4:
                continue
            
            word_a, word_b, word_c, word_d = parts
            
            # æ£€æŸ¥æ‰€æœ‰è¯æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
            if not all(w in model.wv for w in [word_a, word_b, word_c, word_d]):
                skipped += 1
                continue
            
            # è®¡ç®— A:B = C:?
            result = model.wv.most_similar(
                positive=[word_b, word_c],
                negative=[word_a],
                topn=1
            )
            
            predicted = result[0][0]
            
            if predicted == word_d:
                correct += 1
            
            total += 1
    
    return {
        'total': total,
        'correct': correct,
        'skipped': skipped,
        'accuracy': correct / total if total > 0 else 0
    }
```

## éƒ¨ç½²ç­–ç•¥

### 1. å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    make \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…uvåŒ…ç®¡ç†å™¨
RUN pip install uv

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY pyproject.toml .
COPY uv.lock .
COPY code/ ./code/
COPY data/ ./data/

# å®‰è£…ä¾èµ–
RUN uv sync

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p model

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from gensim.models import Word2Vec; print('OK')" || exit 1

# é»˜è®¤å‘½ä»¤
CMD ["uv", "run", "python", "code/word2vec_demo.py"]
```

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  word2vec-analyzer:
    build: .
    container_name: sanguo-word2vec
    volumes:
      - ./data:/app/data
      - ./model:/app/model
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "from gensim.models import Word2Vec; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # å¯é€‰ï¼šAPIæœåŠ¡
  api:
    build: 
      context: .
      dockerfile: Dockerfile.api
    container_name: sanguo-api
    ports:
      - "8000:8000"
    volumes:
      - ./model:/app/model
    depends_on:
      - word2vec-analyzer
    restart: unless-stopped
```

### 2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### Kuberneteséƒ¨ç½²

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sanguo-word2vec
  labels:
    app: sanguo-word2vec
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sanguo-word2vec
  template:
    metadata:
      labels:
        app: sanguo-word2vec
    spec:
      containers:
      - name: word2vec
        image: sanguo-word2vec:latest
        ports:
        - containerPort: 8000
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: model-volume
          mountPath: /app/model
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - python
            - -c
            - "from gensim.models import Word2Vec; print('OK')"
          initialDelaySeconds: 30
          periodSeconds: 30
      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: model-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: sanguo-word2vec-service
spec:
  selector:
    app: sanguo-word2vec
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. APIæœåŠ¡éƒ¨ç½²

```python
# api.py - FastAPIæœåŠ¡
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from gensim.models import Word2Vec
from typing import List, Optional

app = FastAPI(title="ä¸‰å›½æ¼”ä¹‰è¯åµŒå…¥API")

# å…¨å±€æ¨¡å‹
model = None


class SimilarityRequest(BaseModel):
    word: str
    topn: int = 10


class AnalogyRequest(BaseModel):
    positive: List[str]
    negative: Optional[List[str]] = []
    topn: int = 5


@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model
    model = Word2Vec.load('model/three_kingdoms.model')


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ä¸‰å›½æ¼”ä¹‰è¯åµŒå…¥API",
        "endpoints": ["/similar", "/analogy", "/health"]
    }


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    return {"status": "ok", "vocabulary_size": len(model.wv)}


@app.post("/similar")
async def get_similar(request: SimilarityRequest):
    """è·å–ç›¸ä¼¼è¯"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    if request.word not in model.wv:
        raise HTTPException(status_code=404, detail=f"Word '{request.word}' not in vocabulary")
    
    similar = model.wv.most_similar(request.word, topn=request.topn)
    return {
        "word": request.word,
        "similar_words": [{"word": w, "similarity": float(s)} for w, s in similar]
    }


@app.post("/analogy")
async def get_analogy(request: AnalogyRequest):
    """è¯ç±»æ¯”è®¡ç®—"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        result = model.wv.most_similar(
            positive=request.positive,
            negative=request.negative,
            topn=request.topn
        )
        return {
            "positive": request.positive,
            "negative": request.negative,
            "results": [{"word": w, "similarity": float(s)} for w, s in result]
        }
    except KeyError as e:
        raise HTTPException(status_code=404, detail=f"Word not in vocabulary: {e}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4. CI/CDæµæ°´çº¿

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      run: pip install uv
    
    - name: Install dependencies
      run: uv sync
    
    - name: Run tests
      run: uv run pytest tests/ -v
    
    - name: Run type check
      run: uv run basedpyright code/

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t sanguo-word2vec:${{ github.sha }} .
        docker tag sanguo-word2vec:${{ github.sha }} sanguo-word2vec:latest
    
    - name: Push to registry
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push sanguo-word2vec:${{ github.sha }}
        docker push sanguo-word2vec:latest
```

## éƒ¨ç½²éªŒè¯

### 1. éƒ¨ç½²åéªŒè¯è„šæœ¬

```python
# deployment_validator.py
import requests
import time
import logging

logger = logging.getLogger(__name__)


def validate_deployment(base_url: str, timeout: int = 60) -> bool:
    """éªŒè¯éƒ¨ç½²æ˜¯å¦æˆåŠŸ"""
    
    # 1. å¥åº·æ£€æŸ¥
    health_url = f"{base_url}/health"
    try:
        response = requests.get(health_url, timeout=5)
        if response.status_code == 200:
            logger.info("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False
    
    # 2. åŠŸèƒ½æµ‹è¯•
    test_queries = [
        {"word": "æ›¹æ“", "topn": 5},
        {"word": "åˆ˜å¤‡", "topn": 5},
    ]
    
    for query in test_queries:
        similar_url = f"{base_url}/similar"
        try:
            response = requests.post(similar_url, json=query, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get("similar_words"):
                    logger.info(f"âœ… æŸ¥è¯¢ '{query['word']}' æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ æŸ¥è¯¢ '{query['word']}' æ— ç»“æœ")
            else:
                logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¼‚å¸¸: {e}")
            return False
    
    logger.info("ğŸ‰ éƒ¨ç½²éªŒè¯é€šè¿‡")
    return True


if __name__ == "__main__":
    # éªŒè¯æœ¬åœ°éƒ¨ç½²
    validate_deployment("http://localhost:8000")
```

---

*æœ€åæ›´æ–°: 2026å¹´2æœˆ15æ—¥*
*æµ‹è¯•å’Œéƒ¨ç½²ç‰ˆæœ¬: v1.0*
*è¿ç»´å›¢é˜Ÿ: DevOpsè¿ç»´ç»„*
