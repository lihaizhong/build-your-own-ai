# ä¸‰å›½æ¼”ä¹‰è¯åµŒå…¥åˆ†æç³»ç»Ÿ - ä»£ç å®ç°è¯¦è§£

## æ ¸å¿ƒä»£ç ç»“æ„

### 1. é¡¹ç›®è·¯å¾„ç®¡ç†æ¨¡å—

```python
from pathlib import Path

def get_project_path(*paths: str) -> str:
    """
    è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•
    
    Args:
        *paths: ç›¸å¯¹è·¯å¾„ç»„ä»¶
        
    Returns:
        ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²
    """
    try:
        current_dir = Path(__file__).parent
        project_dir = current_dir.parent.parent
        return str(project_dir.joinpath(*paths))
    except NameError:
        # åœ¨äº¤äº’å¼ç¯å¢ƒä¸­
        return str(Path.cwd().joinpath(*paths))
```

**åŠŸèƒ½ç‰¹ç‚¹ï¼š**
- **ç»Ÿä¸€è·¯å¾„ç®¡ç†**: æä¾›é¡¹ç›®è·¯å¾„çš„æ ‡å‡†åŒ–è·å–æ–¹æ³•
- **å®¹é”™å¤„ç†**: å¤„ç†ä¸åŒè¿è¡Œç¯å¢ƒä¸‹çš„è·¯å¾„å·®å¼‚
- **çµæ´»æ‰©å±•**: æ”¯æŒä»»æ„å±‚çº§çš„è·¯å¾„æ„å»º

### 2. SanguoWord2VecAnalyzerç±»æ ¸å¿ƒå®ç°

#### åˆå§‹åŒ–æ–¹æ³•è¯¦è§£

```python
class SanguoWord2VecAnalyzer:
    """ä¸‰å›½æ¼”ä¹‰Word2Vecåˆ†æå™¨"""
    
    def __init__(self, data_path: str) -> None:
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_path: ä¸‰å›½æ¼”ä¹‰æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        """
        self.data_path: str = data_path
        self.sentences: list[list[str]] = []
        self.model: Optional[Word2Vec] = None
```

**å…³é”®è®¾è®¡ç‚¹ï¼š**
- **ç±»å‹æç¤º**: å®Œæ•´çš„ç±»å‹æ³¨è§£æ”¯æŒ
- **å»¶è¿ŸåŠ è½½**: æ¨¡å‹åœ¨éœ€è¦æ—¶æ‰åˆå§‹åŒ–
- **è·¯å¾„å¤–éƒ¨åŒ–**: æ•°æ®è·¯å¾„é€šè¿‡å‚æ•°ä¼ å…¥

## æ–‡æœ¬é¢„å¤„ç†æ¨¡å—è¯¦è§£

### 1. æ–‡æœ¬åŠ è½½å’Œåˆ†è¯

```python
def load_and_preprocess(self) -> None:
    """åŠ è½½å’Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®"""
    print("=" * 60)
    print("1. åŠ è½½ä¸‰å›½æ¼”ä¹‰æ–‡æœ¬")
    print("=" * 60)
    
    with open(self.data_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"æ–‡æœ¬æ€»é•¿åº¦: {len(text)} å­—ç¬¦")
    
    # åˆ†è¯å¤„ç†
    print("\næ­£åœ¨åˆ†è¯...")
    words = jieba.lcut(text)
    print(f"åˆ†è¯ç»“æœ: {len(words)} ä¸ªè¯")
    
    # æ„å»ºå¥å­ï¼ˆæ¯10ä¸ªè¯ä½œä¸ºä¸€ä¸ªå¥å­ï¼‰
    print("\næ„å»ºè®­ç»ƒå¥å­...")
    for i in range(0, len(words) - 10, 10):
        sentence = words[i:i+10]
        self.sentences.append(sentence)
    
    print(f"æ€»å¥å­æ•°: {len(self.sentences)}")
    
    # ç»Ÿè®¡è¯é¢‘
    print("\nè¯é¢‘ç»Ÿè®¡ï¼ˆå‰20ï¼‰:")
    word_counts = Counter(words)
    for word, count in word_counts.most_common(20):
        print(f"  {word}: {count}")
```

**æŠ€æœ¯äº®ç‚¹ï¼š**
- **è¿›åº¦åé¦ˆ**: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
- **ç»Ÿè®¡ä¿¡æ¯**: è¾“å‡ºå…³é”®æ•°æ®ç»Ÿè®¡
- **å¥å­æ„å»º**: å›ºå®šçª—å£æ„å»ºè®­ç»ƒè¯­æ–™

### 2. è¯é¢‘ç»Ÿè®¡ä¼˜åŒ–

```python
from collections import Counter
from typing import Dict, List, Tuple

def get_word_statistics(self) -> Dict[str, int]:
    """
    è·å–è¯é¢‘ç»Ÿè®¡æ•°æ®
    
    Returns:
        è¯é¢‘å­—å…¸
    """
    # åˆå¹¶æ‰€æœ‰å¥å­ä¸­çš„è¯
    all_words = []
    for sentence in self.sentences:
        all_words.extend(sentence)
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(all_words)
    return dict(word_counts)

def get_top_words(self, n: int = 20) -> List[Tuple[str, int]]:
    """
    è·å–é«˜é¢‘è¯åˆ—è¡¨
    
    Args:
        n: è¿”å›è¯æ•°é‡
        
    Returns:
        é«˜é¢‘è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(è¯, é¢‘æ¬¡)å…ƒç»„
    """
    word_counts = self.get_word_statistics()
    counter = Counter(word_counts)
    return counter.most_common(n)
```

## æ¨¡å‹è®­ç»ƒæ¨¡å—è¯¦è§£

### 1. Word2Vecè®­ç»ƒå®ç°

```python
def train_word2vec(self) -> None:
    """è®­ç»ƒWord2Vecæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("2. è®­ç»ƒWord2Vecæ¨¡å‹")
    print("=" * 60)
    
    # è®­ç»ƒå‚æ•°
    params: dict[str, int] = {
        'vector_size': 100,      # å‘é‡ç»´åº¦
        'window': 5,              # ä¸Šä¸‹æ–‡çª—å£å¤§å°
        'min_count': 5,           # æœ€å°è¯é¢‘
        'workers': 4,             # å¹¶è¡Œæ•°
        'sg': 0,                  # 0=CBOW, 1=Skip-gram
        'epochs': 100,            # è®­ç»ƒè½®æ•°
        'seed': 42                # éšæœºç§å­
    }
    
    print(f"è®­ç»ƒå‚æ•°:")
    for key, value in params.items():
        print(f"  {key}: {value}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒ...")
    self.model = Word2Vec(self.sentences, **params)
    
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"è¯æ±‡è¡¨å¤§å°: {len(self.model.wv)}")
    print(f"å‘é‡ç»´åº¦: {self.model.wv.vector_size}")
```

**è®­ç»ƒä¼˜åŒ–ï¼š**
- **å‚æ•°å¯é…ç½®**: æ‰€æœ‰å‚æ•°é›†ä¸­ç®¡ç†
- **è®­ç»ƒåé¦ˆ**: å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
- **ç»“æœéªŒè¯**: è¾“å‡ºæ¨¡å‹å…³é”®æŒ‡æ ‡

### 2. å¢é‡è®­ç»ƒæ”¯æŒ

```python
def incremental_train(self, new_sentences: List[List[str]]) -> None:
    """
    å¢é‡è®­ç»ƒæ¨¡å‹
    
    Args:
        new_sentences: æ–°çš„å¥å­åˆ—è¡¨
    """
    if self.model is None:
        raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡å‹")
    
    # æ›´æ–°è¯æ±‡è¡¨
    self.model.build_vocab(new_sentences, update=True)
    
    # ç»§ç»­è®­ç»ƒ
    self.model.train(
        new_sentences,
        total_examples=self.model.corpus_count,
        epochs=self.model.epochs
    )
    
    print(f"å¢é‡è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.model.wv)}")
```

## ç›¸ä¼¼åº¦åˆ†ææ¨¡å—è¯¦è§£

### 1. è¯ç›¸ä¼¼åº¦æŸ¥è¯¢

```python
def analyze_similar_words(
    self, 
    target_word: str, 
    topn: int = 10
) -> list[tuple[str, float]]:
    """
    åˆ†æä¸ç›®æ ‡è¯æœ€ç›¸ä¼¼çš„è¯
    
    Args:
        target_word: ç›®æ ‡è¯
        topn: è¿”å›æœ€ç›¸ä¼¼çš„è¯æ•°é‡
        
    Returns:
        ç›¸ä¼¼è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(è¯, ç›¸ä¼¼åº¦)çš„å…ƒç»„
    """
    print("\n" + "=" * 60)
    print(f"3. åˆ†æä¸'{target_word}'æœ€ç›¸è¿‘çš„è¯")
    print("=" * 60)
    
    if self.model is None or target_word not in self.model.wv:
        print(f"è­¦å‘Š: '{target_word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
        return []
    
    similar_words = self.model.wv.most_similar(target_word, topn=topn)
    
    print(f"\nä¸ '{target_word}' æœ€ç›¸è¿‘çš„ {topn} ä¸ªè¯:")
    for i, (word, similarity) in enumerate(similar_words, 1):
        print(f"  {i}. {word}: {similarity:.4f}")
    
    return similar_words
```

### 2. ç›¸ä¼¼åº¦è®¡ç®—è¯¦è§£

```python
import numpy as np

def calculate_similarity(self, word1: str, word2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    Args:
        word1: ç¬¬ä¸€ä¸ªè¯
        word2: ç¬¬äºŒä¸ªè¯
        
    Returns:
        ç›¸ä¼¼åº¦å€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
    """
    if self.model is None:
        raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    if word1 not in self.model.wv or word2 not in self.model.wv:
        raise ValueError("è¯æ±‡ä¸åœ¨è¯æ±‡è¡¨ä¸­")
    
    # è·å–è¯å‘é‡
    vec1 = self.model.wv[word1]
    vec2 = self.model.wv[word2]
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    similarity = dot_product / (norm1 * norm2)
    return float(similarity)

def calculate_distance(self, word1: str, word2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªè¯çš„æ¬§æ°è·ç¦»
    
    Args:
        word1: ç¬¬ä¸€ä¸ªè¯
        word2: ç¬¬äºŒä¸ªè¯
        
    Returns:
        æ¬§æ°è·ç¦»å€¼
    """
    if self.model is None:
        raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    vec1 = self.model.wv[word1]
    vec2 = self.model.wv[word2]
    
    distance = np.linalg.norm(vec1 - vec2)
    return float(distance)
```

## è¯ç±»æ¯”è®¡ç®—æ¨¡å—è¯¦è§£

### 1. è¯ç±»æ¯”å®ç°

```python
def word_analogy(
    self, 
    positive_words: list[str], 
    negative_words: list[str], 
    topn: int = 5
) -> list[tuple[str, float]]:
    """
    è¯ç±»æ¯”è®¡ç®—
    
    Args:
        positive_words: æ­£é¢è¯åˆ—è¡¨
        negative_words: è´Ÿé¢è¯åˆ—è¡¨
        topn: è¿”å›ç»“æœæ•°é‡
        
    Returns:
        ç±»ä¼¼è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(è¯, ç›¸ä¼¼åº¦)çš„å…ƒç»„
    """
    print("\n" + "=" * 60)
    print("4. è¯ç±»æ¯”è®¡ç®—")
    print("=" * 60)
    
    pos_str = " + ".join(positive_words)
    neg_str = " - ".join(negative_words)
    print(f"\nè®¡ç®—: {pos_str} - ({neg_str}) = ?")
    
    if self.model is None:
        print("é”™è¯¯: æ¨¡å‹æœªåŠ è½½")
        return []
    
    try:
        result = self.model.wv.most_similar(
            positive=positive_words,
            negative=negative_words,
            topn=topn
        )
        
        print(f"\nç»“æœ:")
        for i, (word, similarity) in enumerate(result, 1):
            print(f"  {i}. {word}: {similarity:.4f}")
        
        return result
    except KeyError as e:
        print(f"é”™è¯¯: è¯æ±‡è¡¨ä¸­ç¼ºå°‘è¯ - {e}")
        return []
```

### 2. ç±»æ¯”éªŒè¯å‡½æ•°

```python
def validate_analogy(
    self,
    word_a: str,
    word_b: str,
    word_c: str,
    word_d: str
) -> float:
    """
    éªŒè¯ç±»æ¯”å…³ç³» A:B = C:D
    
    Args:
        word_a, word_b, word_c, word_d: ç±»æ¯”è¯
        
    Returns:
        éªŒè¯å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    """
    if self.model is None:
        raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    # è®¡ç®—ç›®æ ‡å‘é‡: vec(D) â‰ˆ vec(B) - vec(A) + vec(C)
    expected_d = (
        self.model.wv[word_b] - 
        self.model.wv[word_a] + 
        self.model.wv[word_c]
    )
    
    # ä¸å®é™…Då‘é‡çš„ç›¸ä¼¼åº¦
    actual_d = self.model.wv[word_d]
    similarity = np.dot(expected_d, actual_d) / (
        np.linalg.norm(expected_d) * np.linalg.norm(actual_d)
    )
    
    return float(similarity)
```

## æ¨¡å‹æŒä¹…åŒ–æ¨¡å—è¯¦è§£

### 1. æ¨¡å‹ä¿å­˜

```python
def save_model(self, model_path: Optional[str] = None) -> None:
    """
    ä¿å­˜æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    if model_path is None:
        model_path = self.get_model_path()
    
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    model_dir = os.path.dirname(model_path)
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"\nä¿å­˜æ¨¡å‹åˆ°: {model_path}")
    if self.model is not None:
        self.model.save(model_path)
    print("æ¨¡å‹ä¿å­˜å®Œæˆï¼")
```

### 2. æ¨¡å‹åŠ è½½

```python
def load_model(self, model_path: Optional[str] = None) -> None:
    """
    åŠ è½½æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    if model_path is None:
        model_path = self.get_model_path()
    
    print(f"\nä» {model_path} åŠ è½½æ¨¡å‹")
    self.model = Word2Vec.load(model_path)
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print(f"è¯æ±‡è¡¨å¤§å°: {len(self.model.wv)}")
```

### 3. æ¨¡å‹ä¿¡æ¯å¯¼å‡º

```python
import json

def export_model_info(self, output_path: str) -> None:
    """
    å¯¼å‡ºæ¨¡å‹ä¿¡æ¯
    
    Args:
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if self.model is None:
        raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    info = {
        'model_type': 'Word2Vec',
        'vector_size': self.model.wv.vector_size,
        'vocabulary_size': len(self.model.wv),
        'window': self.model.window,
        'min_count': self.model.min_count,
        'epochs': self.model.epochs,
        'training_algorithm': 'Skip-gram' if self.model.sg else 'CBOW',
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print(f"æ¨¡å‹ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_path}")
```

## ç¤ºä¾‹ä»£ç å®ç°è¯¦è§£

### 1. word2vec_demo.pyåˆ†æ

```python
"""
Word2Vec æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹å¹¶è¿›è¡Œè¯å‘é‡åˆ†æ
"""

from gensim.models import Word2Vec
from practice.shared import get_project_path


def load_model():
    """
    åŠ è½½å·²è®­ç»ƒçš„Word2Vecæ¨¡å‹
    """
    model_path = get_project_path('model', 'three_kingdoms.model')
    print(f"ä» {model_path} åŠ è½½æ¨¡å‹...")
    model = Word2Vec.load(model_path)
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼è¯æ±‡è¡¨å¤§å°: {len(model.wv)}")
    return model


def analyze_word_similarity(model, word, topn=10):
    """
    åˆ†æè¯çš„ç›¸ä¼¼åº¦
    
    Args:
        model: Word2Vecæ¨¡å‹
        word: ç›®æ ‡è¯
        topn: è¿”å›æœ€ç›¸ä¼¼çš„è¯æ•°é‡
    """
    if word not in model.wv:
        print(f"è­¦å‘Š: '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
        return
    
    print(f"\nä¸ '{word}' æœ€ç›¸è¿‘çš„ {topn} ä¸ªè¯:")
    similar_words = model.wv.most_similar(word, topn=topn)
    for i, (w, sim) in enumerate(similar_words, 1):
        print(f"  {i}. {w}: {sim:.4f}")


def word_analogy_demo(model):
    """
    è¯ç±»æ¯”æ¼”ç¤º
    """
    print("\n" + "=" * 60)
    print("è¯ç±»æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    # ç¤ºä¾‹1: æ›¹æ“ + åˆ˜å¤‡ - å¼ é£ = ?
    print("\nç¤ºä¾‹1: æ›¹æ“ + åˆ˜å¤‡ - å¼ é£ = ?")
    result = model.wv.most_similar(
        positive=['æ›¹æ“', 'åˆ˜å¤‡'],
        negative=['å¼ é£'],
        topn=5
    )
    for i, (word, sim) in enumerate(result, 1):
        print(f"  {i}. {word}: {sim:.4f}")


def word_vector_demo(model):
    """
    è¯å‘é‡æ¼”ç¤º
    """
    print("\n" + "=" * 60)
    print("è¯å‘é‡æ¼”ç¤º")
    print("=" * 60)
    
    # è·å–è¯å‘é‡
    word = 'æ›¹æ“'
    if word in model.wv:
        vector = model.wv[word]
        print(f"\n'{word}' çš„è¯å‘é‡:")
        print(f"  å‘é‡ç»´åº¦: {len(vector)}")
        print(f"  å‘é‡å‰10ç»´: {vector[:10]}")
        print(f"  å‘é‡èŒƒæ•°: {np.linalg.norm(vector):.4f}")


def vocabulary_info(model):
    """
    è¯æ±‡è¡¨ä¿¡æ¯
    """
    print("\n" + "=" * 60)
    print("è¯æ±‡è¡¨ä¿¡æ¯")
    print("=" * 60)
    
    print(f"\nè¯æ±‡è¡¨å¤§å°: {len(model.wv)}")
    print(f"å‘é‡ç»´åº¦: {model.wv.vector_size}")
    
    # ç»Ÿè®¡äººç‰©å
    person_names = ['æ›¹æ“', 'åˆ˜å¤‡', 'å­™æƒ', 'è¯¸è‘›äº®', 'å…³ç¾½', 'å¼ é£', 
                    'å‘¨ç‘œ', 'èµµäº‘', 'é©¬è¶…', 'é»„å¿ ', 'è‘£å“', 'å•å¸ƒ',
                    'è¢ç»', 'å­™ç­–', 'è€å½§', 'å¸é©¬æ‡¿']
    
    print(f"\nå¸¸è§äººç‰©è¯:")
    for name in person_names:
        if name in model.wv:
            print(f"  âœ“ {name}")
        else:
            print(f"  âœ— {name} (ä¸åœ¨è¯æ±‡è¡¨ä¸­)")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ä¸‰å›½æ¼”ä¹‰ Word2Vec æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model = load_model()
    
    # è¯æ±‡è¡¨ä¿¡æ¯
    vocabulary_info(model)
    
    # è¯ç›¸ä¼¼åº¦åˆ†æ
    print("\n" + "=" * 60)
    print("è¯ç›¸ä¼¼åº¦åˆ†æ")
    print("=" * 60)
    
    analyze_word_similarity(model, 'æ›¹æ“', topn=10)
    analyze_word_similarity(model, 'åˆ˜å¤‡', topn=10)
    analyze_word_similarity(model, 'è¯¸è‘›äº®', topn=10)
    
    # è¯å‘é‡æ¼”ç¤º
    word_vector_demo(model)
    
    # è¯ç±»æ¯”æ¼”ç¤º
    word_analogy_demo(model)
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

### 2. test_environment.pyåˆ†æ

```python
"""
ç¯å¢ƒæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…
"""

import sys


def test_python_version():
    """æµ‹è¯•Pythonç‰ˆæœ¬"""
    print("æµ‹è¯• Python ç‰ˆæœ¬...")
    version = sys.version_info
    print(f"  å½“å‰ç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    
    if version.major == 3 and version.minor >= 11:
        print("  âœ“ Python ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.11)")
        return True
    else:
        print("  âœ— Python ç‰ˆæœ¬ä¸ç¬¦åˆè¦æ±‚ï¼Œéœ€è¦ >= 3.11")
        return False


def test_gensim():
    """æµ‹è¯•Gensimå®‰è£…"""
    print("\næµ‹è¯• Gensim...")
    try:
        import gensim
        print(f"  âœ“ Gensim ç‰ˆæœ¬: {gensim.__version__}")
        return True
    except ImportError:
        print("  âœ— Gensim æœªå®‰è£…")
        return False


def test_jieba():
    """æµ‹è¯•jiebaå®‰è£…"""
    print("\næµ‹è¯• jieba...")
    try:
        import jieba
        print(f"  âœ“ jieba å®‰è£…æˆåŠŸ")
        # æµ‹è¯•åˆ†è¯åŠŸèƒ½
        test_text = "ä¸‰å›½æ¼”ä¹‰æ˜¯ä¸­å›½å¤å…¸å››å¤§åè‘—ä¹‹ä¸€"
        words = jieba.lcut(test_text)
        print(f"  åˆ†è¯æµ‹è¯•: {test_text} -> {words}")
        return True
    except ImportError:
        print("  âœ— jieba æœªå®‰è£…")
        return False


def test_numpy():
    """æµ‹è¯•NumPyå®‰è£…"""
    print("\næµ‹è¯• NumPy...")
    try:
        import numpy as np
        print(f"  âœ“ NumPy ç‰ˆæœ¬: {np.__version__}")
        return True
    except ImportError:
        print("  âœ— NumPy æœªå®‰è£…")
        return False


def test_word2vec():
    """æµ‹è¯•Word2VecåŸºæœ¬åŠŸèƒ½"""
    print("\næµ‹è¯• Word2Vec åŸºæœ¬åŠŸèƒ½...")
    try:
        from gensim.models import Word2Vec
        
        # åˆ›å»ºæµ‹è¯•è¯­æ–™
        sentences = [
            ['æ›¹æ“', 'åˆ˜å¤‡', 'å­™æƒ'],
            ['è¯¸è‘›äº®', 'å…³ç¾½', 'å¼ é£'],
            ['æ›¹æ“', 'è¢ç»', 'è‘£å“'],
        ]
        
        # è®­ç»ƒå°å‹æ¨¡å‹
        model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, epochs=10)
        
        # æµ‹è¯•ç›¸ä¼¼åº¦æŸ¥è¯¢
        similar = model.wv.most_similar('æ›¹æ“', topn=2)
        print(f"  âœ“ Word2Vec åŠŸèƒ½æ­£å¸¸")
        print(f"  æµ‹è¯•æŸ¥è¯¢ 'æ›¹æ“' ç›¸ä¼¼è¯: {similar[:2]}")
        return True
    except Exception as e:
        print(f"  âœ— Word2Vec æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ä¸‰å›½æ¼”ä¹‰ Word2Vec é¡¹ç›® - ç¯å¢ƒæµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("Pythonç‰ˆæœ¬", test_python_version()))
    results.append(("Gensim", test_gensim()))
    results.append(("jieba", test_jieba()))
    results.append(("NumPy", test_numpy()))
    results.append(("Word2VecåŠŸèƒ½", test_word2vec()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for name, passed in results:
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…ã€‚")
        print("\nå®‰è£…ä¾èµ–å‘½ä»¤:")
        print("  uv sync")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

## é”™è¯¯å¤„ç†å’Œè°ƒè¯•æœºåˆ¶

### 1. å¼‚å¸¸å¤„ç†ç­–ç•¥

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def safe_load_model(model_path: str) -> Optional[Word2Vec]:
    """
    å®‰å…¨åŠ è½½æ¨¡å‹ï¼ŒåŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        
    Returns:
        Word2Vecæ¨¡å‹æˆ–None
    """
    try:
        if not os.path.exists(model_path):
            logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        if not model_path.endswith('.model'):
            logger.warning(f"éæ ‡å‡†æ¨¡å‹æ–‡ä»¶æ‰©å±•å: {model_path}")
        
        model = Word2Vec.load(model_path)
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        return model
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
        return None


def safe_similarity_query(model: Word2Vec, word: str, topn: int = 10) -> Optional[list]:
    """
    å®‰å…¨çš„ç›¸ä¼¼åº¦æŸ¥è¯¢
    
    Args:
        model: Word2Vecæ¨¡å‹
        word: æŸ¥è¯¢è¯
        topn: è¿”å›ç»“æœæ•°é‡
        
    Returns:
        ç›¸ä¼¼è¯åˆ—è¡¨æˆ–None
    """
    try:
        if word not in model.wv:
            logger.warning(f"è¯æ±‡ä¸åœ¨è¯æ±‡è¡¨ä¸­: {word}")
            return None
        
        result = model.wv.most_similar(word, topn=topn)
        return result
        
    except Exception as e:
        logger.error(f"ç›¸ä¼¼åº¦æŸ¥è¯¢å¤±è´¥: {e}")
        return None
```

### 2. æ€§èƒ½ç›‘æ§

```python
import time
from functools import wraps


def timing_decorator(func):
    """è®¡æ—¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        duration = end_time - start_time
        logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {duration:.4f}ç§’")
        
        return result
    return wrapper


@timing_decorator
def train_word2vec_with_timing(sentences, **params):
    """å¸¦è®¡æ—¶çš„æ¨¡å‹è®­ç»ƒ"""
    return Word2Vec(sentences, **params)
```

---

*æœ€åæ›´æ–°: 2026å¹´2æœˆ15æ—¥*
*ä»£ç å®ç°ç‰ˆæœ¬: v1.0*
*å¼€å‘å›¢é˜Ÿ: AIç³»ç»Ÿå¼€å‘ç»„*
