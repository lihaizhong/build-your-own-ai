"""
V28æµ‹è¯•ç‰ˆæœ¬æ¨¡å‹ - å¿«é€ŸéªŒè¯ç‰ˆ

åŸºäºV28çš„èåˆåˆ›æ–°ç­–ç•¥ï¼Œä½†ç®€åŒ–ä»¥å¿«é€ŸéªŒè¯:
1. ä¿ç•™æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šåŠ¨æ€ç‰¹å¾é€‰æ‹©ã€è‡ªé€‚åº”å‚æ•°ã€å¢å¼ºæ ¡å‡†
2. ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼šå‡å°‘äº¤å‰éªŒè¯æŠ˜æ•°å’Œè¿­ä»£æ¬¡æ•°
3. å¿«é€ŸéªŒè¯æ•ˆæœï¼šç¡®ä¿æ ¸å¿ƒç­–ç•¥æœ‰æ•ˆ
"""

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.metrics import mean_absolute_error
from sklearn.feature_selection import mutual_info_regression
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import warnings
warnings.filterwarnings('ignore')

def get_project_path(*paths):
    """è·å–é¡¹ç›®è·¯å¾„çš„ç»Ÿä¸€æ–¹æ³•"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def fast_preprocessing():
    """å¿«é€Ÿæ•°æ®é¢„å¤„ç†"""
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"åŸå§‹è®­ç»ƒé›†: {train_df.shape}")
    print(f"åŸå§‹æµ‹è¯•é›†: {test_df.shape}")
    
    # åˆå¹¶æ•°æ®è¿›è¡Œé¢„å¤„ç†
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # åŸºç¡€powerå¤„ç†
    if 'power' in all_df.columns:
        all_df['power'] = np.clip(all_df['power'], 0, 600)
        all_df['power_is_zero'] = (all_df['power'] <= 0).astype(int)
        all_df['log_power'] = np.log1p(np.maximum(all_df['power'], 1))
    
    # åˆ†ç±»ç‰¹å¾å¤„ç† - ç®€åŒ–ç‰ˆ
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model', 'brand']
    for col in categorical_cols:
        if col in all_df.columns:
            all_df[f'{col}_missing'] = (all_df[col].isnull()).astype(int)
            
            # ç®€å•å¡«å……
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])
            
            # ç®€åŒ–ç›®æ ‡ç¼–ç 
            if 'price' in all_df.columns and col != 'brand':
                target_mean = all_df.groupby(col)['price'].mean()
                global_mean = all_df['price'].mean()
                smooth_factor = 30
                count = all_df[col].value_counts()
                smooth_encoding = (target_mean * count + global_mean * smooth_factor) / (count + smooth_factor)
                all_df[f'{col}_target_enc'] = all_df[col].map(smooth_encoding).fillna(global_mean)
            
            # é¢‘ç‡ç¼–ç 
            freq_map = all_df[col].value_counts().to_dict()
            all_df[f'{col}_freq'] = all_df[col].map(freq_map)
    
    # æ—¶é—´ç‰¹å¾å·¥ç¨‹ - ç®€åŒ–ç‰ˆ
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(0).astype(int)
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    all_df['reg_season'] = all_df['reg_month'].map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 7:3, 8:3, 9:4, 10:4, 11:4})
    
    all_df.drop(columns=['regDate'], inplace=True)
    
    # å“ç‰Œç»Ÿè®¡ç‰¹å¾ - ç®€åŒ–ç‰ˆ
    if 'price' in all_df.columns:
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'count']).reset_index()
        global_mean = all_df['price'].mean()
        smooth_factor = 40
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     global_mean * smooth_factor) / (brand_stats['count'] + smooth_factor))
        brand_map = brand_stats.set_index('brand')['smooth_mean'].to_dict()
        all_df['brand_avg_price'] = all_df['brand'].map(brand_map).fillna(global_mean)
    
    # æ ‡ç­¾ç¼–ç 
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
    
    # æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                median_val = all_df[col].median()
                if not pd.isna(median_val):
                    all_df[col] = all_df[col].fillna(median_val)
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # é‡æ–°åˆ†ç¦»
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_df.shape}")
    print(f"å¤„ç†åæµ‹è¯•é›†: {test_df.shape}")
    
    return train_df, test_df

def create_fast_features(df):
    """å¿«é€Ÿç‰¹å¾å·¥ç¨‹"""
    df = df.copy()
    
    # æ ¸å¿ƒä¸šåŠ¡ç‰¹å¾
    if 'power' in df.columns and 'car_age' in df.columns:
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_decay'] = df['power'] * np.exp(-df['car_age'] * 0.05)
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        df['km_per_year'] = np.clip(df['km_per_year'], 0, 40000)
    
    # å…³é”®äº¤äº’ç‰¹å¾
    if 'power' in df.columns and 'kilometer' in df.columns:
        df['power_km_ratio'] = df['power'] / (df['kilometer'] + 1)
    
    if 'car_age' in df.columns and 'kilometer' in df.columns:
        df['age_km_interaction'] = df['car_age'] * df['kilometer'] / 1000
    
    # åˆ†æ®µç‰¹å¾
    df['age_segment'] = pd.cut(df['car_age'], bins=[-1, 3, 7, 12, float('inf')], 
                              labels=['new', 'medium', 'old', 'very_old'])
    df['age_segment'] = df['age_segment'].cat.codes
    
    # å˜æ¢ç‰¹å¾
    if 'car_age' in df.columns:
        df['log_car_age'] = np.log1p(df['car_age'])
    
    if 'kilometer' in df.columns:
        df['log_kilometer'] = np.log1p(df['kilometer'])
    
    # vç‰¹å¾ç»Ÿè®¡
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)
        df['v_range'] = df[v_cols].max(axis=1) - df[v_cols].min(axis=1)
    
    # æ•°æ®æ¸…ç†
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        null_count = df[col].isnull().sum()
        if null_count > 0:
            df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)
        
        if col not in ['SaleID', 'price'] and df[col].std() > 1e-8:
            q99 = df[col].quantile(0.99)
            q01 = df[col].quantile(0.01)
            if q99 > q01 and q99 > 0:
                df[col] = np.clip(df[col], q01, q99)
    
    return df

def fast_feature_selection(X_train, y_train, X_test, max_features=60):
    """å¿«é€Ÿç‰¹å¾é€‰æ‹©"""
    print("æ‰§è¡Œå¿«é€Ÿç‰¹å¾é€‰æ‹©...")
    
    feature_names = X_train.columns.tolist()
    
    # è®¡ç®—äº’ä¿¡æ¯åˆ†æ•°
    mi_scores = mutual_info_regression(X_train, y_train, random_state=42)
    mi_df = pd.DataFrame({
        'feature': feature_names,
        'mi_score': mi_scores
    }).sort_values('mi_score', ascending=False)
    
    # é€‰æ‹©topç‰¹å¾
    top_features = mi_df.head(max_features)['feature'].tolist()
    
    print(f"ä»{len(feature_names)}ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†{len(top_features)}ä¸ªç‰¹å¾")
    
    return X_train[top_features], X_test[top_features], mi_df

def fast_adaptive_params(X_train, y_train):
    """å¿«é€Ÿè‡ªé€‚åº”å‚æ•°"""
    n_samples, n_features = X_train.shape
    
    # ç®€åŒ–å‚æ•°è°ƒæ•´
    if n_features > 50:
        learning_rate = 0.07
        num_leaves = 35
        depth = 8
    else:
        learning_rate = 0.08
        num_leaves = 31
        depth = 7
    
    return {
        'learning_rate': learning_rate,
        'num_leaves': num_leaves,
        'max_depth': depth
    }

def train_fast_models(X_train, y_train, X_test):
    """å¿«é€Ÿè®­ç»ƒæ¨¡å‹"""
    print("å¿«é€Ÿè®­ç»ƒæ¨¡å‹...")
    
    # å¯¹æ•°å˜æ¢
    y_train_log = np.log1p(y_train)
    
    # ç®€åŒ–äº¤å‰éªŒè¯ - 3æŠ˜
    y_bins = pd.qcut(y_train, q=5, labels=False)
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    # è‡ªé€‚åº”å‚æ•°
    adaptive_params = fast_adaptive_params(X_train, y_train)
    
    # å‚æ•°è®¾ç½®
    lgb_params = {
        'objective': 'mae',
        'metric': 'mae',
        'num_leaves': adaptive_params['num_leaves'],
        'max_depth': adaptive_params['max_depth'],
        'learning_rate': adaptive_params['learning_rate'],
        'feature_fraction': 0.85,
        'bagging_fraction': 0.85,
        'bagging_freq': 5,
        'lambda_l1': 0.25,
        'lambda_l2': 0.25,
        'min_child_samples': 18,
        'random_state': 42,
    }
    
    xgb_params = {
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'max_depth': adaptive_params['max_depth'],
        'learning_rate': adaptive_params['learning_rate'],
        'subsample': 0.85,
        'colsample_bytree': 0.85,
        'reg_alpha': 0.6,
        'reg_lambda': 0.6,
        'min_child_weight': 8,
        'random_state': 42
    }
    
    catboost_params = {
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'depth': adaptive_params['max_depth'],
        'learning_rate': adaptive_params['learning_rate'],
        'iterations': 1000,
        'l2_leaf_reg': 1.2,
        'random_strength': 0.35,
        'random_seed': 42,
        'verbose': False
    }
    
    # å­˜å‚¨é¢„æµ‹ç»“æœ
    lgb_predictions = np.zeros(len(X_test))
    xgb_predictions = np.zeros(len(X_test))
    cat_predictions = np.zeros(len(X_test))
    
    # å­˜å‚¨éªŒè¯åˆ†æ•°
    lgb_scores, xgb_scores, cat_scores = [], [], []
    
    # äº¤å‰éªŒè¯è®­ç»ƒ
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_bins), 1):
        print(f"è®­ç»ƒç¬¬ {fold} æŠ˜...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]
        
        # LightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1200)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=80), lgb.log_evaluation(0)])
        
        lgb_predictions += np.expm1(np.array(lgb_model.predict(X_test))) / 3
        lgb_val_pred = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_val_pred)
        lgb_scores.append(lgb_mae)
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1200, early_stopping_rounds=80)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        xgb_predictions += np.expm1(xgb_model.predict(X_test)) / 3
        xgb_val_pred = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_val_pred)
        xgb_scores.append(xgb_mae)
        
        # CatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=80, 
                     verbose=False)
        
        cat_predictions += np.expm1(cat_model.predict(X_test)) / 3
        cat_val_pred = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_val_pred)
        cat_scores.append(cat_mae)
        
        print(f"  LGB: {lgb_mae:.2f}, XGB: {xgb_mae:.2f}, CAT: {cat_mae:.2f}")
    
    print(f"\nå¹³å‡éªŒè¯åˆ†æ•°:")
    print(f"  LightGBM: {np.mean(lgb_scores):.2f}")
    print(f"  XGBoost: {np.mean(xgb_scores):.2f}")
    print(f"  CatBoost: {np.mean(cat_scores):.2f}")
    
    return lgb_predictions, xgb_predictions, cat_predictions, {
        'lgb_scores': lgb_scores,
        'xgb_scores': xgb_scores,
        'cat_scores': cat_scores
    }

def fast_ensemble(lgb_pred, xgb_pred, cat_pred, scores_info):
    """å¿«é€Ÿé›†æˆ"""
    print("æ‰§è¡Œå¿«é€Ÿé›†æˆ...")
    
    # åŸºäºæ€§èƒ½çš„æƒé‡
    lgb_score = np.mean(scores_info['lgb_scores'])
    xgb_score = np.mean(scores_info['xgb_scores'])
    cat_score = np.mean(scores_info['cat_scores'])
    
    total_inv_score = 1/lgb_score + 1/xgb_score + 1/cat_score
    weights = {
        'lgb': (1/lgb_score) / total_inv_score,
        'xgb': (1/xgb_score) / total_inv_score,
        'cat': (1/cat_score) / total_inv_score
    }
    
    print(f"é›†æˆæƒé‡:")
    for model, weight in weights.items():
        print(f"  {model.upper()}: {weight:.3f}")
    
    ensemble_pred = (weights['lgb'] * lgb_pred + 
                    weights['xgb'] * xgb_pred + 
                    weights['cat'] * cat_pred)
    
    return ensemble_pred

def fast_calibration(predictions, y_train):
    """å¿«é€Ÿæ ¡å‡†"""
    print("æ‰§è¡Œå¿«é€Ÿæ ¡å‡†...")
    
    train_mean = y_train.mean()
    pred_mean = predictions.mean()
    
    # ç®€å•æ ¡å‡†
    calibration_factor = train_mean / pred_mean if pred_mean > 0 else 1.0
    calibration_factor = np.clip(calibration_factor, 0.9, 1.1)
    
    final_predictions = predictions * calibration_factor
    final_predictions = np.maximum(final_predictions, 0)
    
    print(f"  æ ¡å‡†å› å­: {calibration_factor:.4f}")
    
    return final_predictions

def v28_fast_test():
    """V28å¿«é€Ÿæµ‹è¯•æµç¨‹"""
    print("=" * 60)
    print("å¼€å§‹V28å¿«é€Ÿæµ‹è¯•")
    print("éªŒè¯èåˆåˆ›æ–°ç­–ç•¥æ•ˆæœ")
    print("=" * 60)
    
    # æ­¥éª¤1: å¿«é€Ÿé¢„å¤„ç†
    print("\næ­¥éª¤1: å¿«é€Ÿé¢„å¤„ç†...")
    train_df, test_df = fast_preprocessing()
    
    # æ­¥éª¤2: å¿«é€Ÿç‰¹å¾å·¥ç¨‹
    print("\næ­¥éª¤2: å¿«é€Ÿç‰¹å¾å·¥ç¨‹...")
    train_df = create_fast_features(train_df)
    test_df = create_fast_features(test_df)
    
    # å‡†å¤‡ç‰¹å¾
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    # æ­¥éª¤3: å¿«é€Ÿç‰¹å¾é€‰æ‹©
    print("\næ­¥éª¤3: å¿«é€Ÿç‰¹å¾é€‰æ‹©...")
    X_train_selected, X_test_selected, feature_importance = fast_feature_selection(
        X_train, y_train, X_test, max_features=60)
    
    # æ­¥éª¤4: ç‰¹å¾ç¼©æ”¾
    print("\næ­¥éª¤4: ç‰¹å¾ç¼©æ”¾...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # æ­¥éª¤5: å¿«é€Ÿè®­ç»ƒ
    print("\næ­¥éª¤5: å¿«é€Ÿè®­ç»ƒæ¨¡å‹...")
    lgb_pred, xgb_pred, cat_pred, scores_info = train_fast_models(
        X_train_selected, y_train, X_test_selected)
    
    # æ­¥éª¤6: å¿«é€Ÿé›†æˆ
    print("\næ­¥éª¤6: å¿«é€Ÿé›†æˆ...")
    ensemble_pred = fast_ensemble(lgb_pred, xgb_pred, cat_pred, scores_info)
    
    # æ­¥éª¤7: å¿«é€Ÿæ ¡å‡†
    print("\næ­¥éª¤7: å¿«é€Ÿæ ¡å‡†...")
    final_predictions = fast_calibration(ensemble_pred, y_train)
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nV28å¿«é€Ÿæµ‹è¯•æœ€ç»ˆé¢„æµ‹ç»Ÿè®¡:")
    print(f"å‡å€¼: {final_predictions.mean():.2f}")
    print(f"æ ‡å‡†å·®: {final_predictions.std():.2f}")
    print(f"èŒƒå›´: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # ä¿å­˜ç»“æœ
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v28_fast_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\nV28å¿«é€Ÿæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("V28å¿«é€Ÿæµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print("âœ… èåˆåˆ›æ–°ç­–ç•¥éªŒè¯")
    print("âœ… åŠ¨æ€ç‰¹å¾é€‰æ‹©")
    print("âœ… è‡ªé€‚åº”å‚æ•°è°ƒä¼˜")
    print("âœ… æ™ºèƒ½é›†æˆæƒé‡")
    print("ğŸš€ æ ¸å¿ƒç­–ç•¥éªŒè¯å®Œæˆ!")
    print("=" * 60)
    
    return final_predictions, scores_info

if __name__ == "__main__":
    test_pred, scores_info = v28_fast_test()
    print("V28å¿«é€Ÿæµ‹è¯•å®Œæˆ! ğŸš€")