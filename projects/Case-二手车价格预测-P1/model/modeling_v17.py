"""
V17版本模型 - 冲击500分目标

基于V16的突破性进展，实施以下高级优化策略:
1. 高级特征工程 - 业务逻辑特征和交互特征
2. 超参数精细调优 - 基于V16最佳配置的网格搜索
3. Stacking集成策略 - 线性回归元学习器
4. 数据质量提升 - 精确异常值处理和分布一致性
5. 模型融合优化 - 多层集成和智能校准
"""

import os
from typing import Tuple, Dict, Any, List, Union
import pandas as pd  # type: ignore
import numpy as np  # type: ignore
from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV  # type: ignore
from sklearn.ensemble import RandomForestRegressor, StackingRegressor  # type: ignore
from sklearn.linear_model import Ridge, LinearRegression  # type: ignore
from sklearn.preprocessing import LabelEncoder, RobustScaler, PolynomialFeatures  # type: ignore
from sklearn.metrics import mean_absolute_error  # type: ignore
from sklearn.feature_selection import SelectFromModel  # type: ignore
import lightgbm as lgb  # type: ignore
import xgboost as xgb  # type: ignore
from catboost import CatBoostRegressor  # type: ignore
import matplotlib.pyplot as plt  # type: ignore
from scipy import stats  # type: ignore
import warnings
warnings.filterwarnings('ignore')

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def get_project_path(*paths: str) -> str:
    """获取项目路径的统一方法"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(current_dir)
        return os.path.join(project_dir, *paths)
    except NameError:
        return os.path.join(os.getcwd(), *paths)

def get_user_data_path(*paths: str) -> str:
    """获取用户数据路径"""
    return get_project_path('user_data', *paths)

def load_and_preprocess_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    高级数据加载和预处理 - 精确异常值处理
    """
    train_path = get_project_path('data', 'used_car_train_20200313.csv')
    test_path = get_project_path('data', 'used_car_testB_20200421.csv')
    
    train_df = pd.read_csv(train_path, sep=' ', na_values=['-'])
    test_df = pd.read_csv(test_path, sep=' ', na_values=['-'])
    
    print(f"原始训练集: {train_df.shape}")
    print(f"原始测试集: {test_df.shape}")
    
    # 合并数据进行统一预处理
    all_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)
    
    # 高级power异常值处理 - 基于统计分布
    if 'power' in all_df.columns:
        # 使用IQR方法检测异常值
        Q1 = all_df['power'].quantile(0.25)
        Q3 = all_df['power'].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + 3 * IQR  # 更宽松的边界
        all_df['power'] = np.clip(all_df['power'], 0, min(upper_bound, 600))
        
        # 添加power异常值标记
        all_df['power_outlier'] = ((all_df['power'] <= 0) | (all_df['power'] >= upper_bound)).astype(int)
    
    # 分类特征高级缺失值处理
    categorical_cols = ['fuelType', 'gearbox', 'bodyType', 'model']
    for col in categorical_cols:
        if col in all_df.columns:
            # 基于其他特征的智能填充
            if col == 'model' and 'brand' in all_df.columns:
                # 同品牌下最常见的型号
                for brand in all_df['brand'].unique():
                    brand_mask = all_df['brand'] == brand
                    brand_mode = all_df[brand_mask][col].mode()  # type: ignore
                    if len(brand_mode) > 0:
                        all_df.loc[brand_mask & all_df[col].isnull(), col] = brand_mode.iloc[0]  # type: ignore
            
            # 全局众数填充剩余缺失值
            mode_value = all_df[col].mode()
            if len(mode_value) > 0:
                all_df[col] = all_df[col].fillna(mode_value.iloc[0])  # type: ignore
            
            # 缺失值标记特征
            all_df[f'{col}_missing'] = (all_df[col].isnull()).astype(int)
    
    # 时间特征工程 - 更精细的处理
    all_df['regDate'] = pd.to_datetime(all_df['regDate'], format='%Y%m%d', errors='coerce')
    current_year = 2020
    all_df['car_age'] = current_year - all_df['regDate'].dt.year
    all_df['car_age'] = all_df['car_age'].fillna(all_df['car_age'].median()).astype(int)
    
    # 添加注册月份和季度特征
    all_df['reg_month'] = all_df['regDate'].dt.month.fillna(6).astype(int)
    all_df['reg_quarter'] = all_df['regDate'].dt.quarter.fillna(2).astype(int)
    all_df.drop(columns=['regDate'], inplace=True)
    
    # 高级品牌统计特征
    if 'price' in all_df.columns:
        brand_stats = all_df.groupby('brand')['price'].agg(['mean', 'std', 'count', 'median']).reset_index()
        
        # 更智能的平滑 - 基于样本数量调整平滑因子
        brand_stats['smooth_factor'] = np.where(brand_stats['count'] < 10, 100, 
                                              np.where(brand_stats['count'] < 50, 50, 30))
        brand_stats['smooth_mean'] = ((brand_stats['mean'] * brand_stats['count'] + 
                                     all_df['price'].mean() * brand_stats['smooth_factor']) / 
                                    (brand_stats['count'] + brand_stats['smooth_factor']))
        
        # 品牌价格稳定性指标
        brand_stats['price_cv'] = brand_stats['std'] / (brand_stats['mean'] + 1e-6)
        
        # 映射品牌特征
        brand_maps = {
            'brand_avg_price': brand_stats.set_index('brand')['smooth_mean'],
            'brand_price_std': brand_stats.set_index('brand')['std'].fillna(0),
            'brand_count': brand_stats.set_index('brand')['count'],
            'brand_price_cv': brand_stats.set_index('brand')['price_cv'].fillna(0)
        }
        
        for feature_name, brand_map in brand_maps.items():
            all_df[feature_name] = all_df['brand'].map(brand_map).fillna(all_df['price'].mean() if 'price' in brand_map else 0)  # type: ignore
    
    # 标签编码 - 保留编码映射用于一致性
    categorical_cols = ['model', 'brand', 'fuelType', 'gearbox', 'bodyType']
    label_encoders = {}
    
    for col in categorical_cols:
        if col in all_df.columns:
            le = LabelEncoder()
            all_df[col] = le.fit_transform(all_df[col].astype(str))
            label_encoders[col] = le
    
    # 高级数值特征处理
    numeric_cols = all_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['price', 'SaleID']:
            null_count = all_df[col].isnull().sum()
            if null_count > 0:
                # 基于相关特征的智能填充
                if col in ['kilometer', 'power']:
                    # 基于车龄的填充
                    for age_group in all_df['car_age'].quantile([0, 0.25, 0.5, 0.75, 1]).values:
                        age_mask = (all_df['car_age'] >= age_group) & (all_df['car_age'] <= age_group + 2)
                        group_median = all_df[age_mask][col].median()  # type: ignore
                        if not pd.isna(group_median):  # type: ignore
                            all_df.loc[age_mask & all_df[col].isnull(), col] = group_median  # type: ignore[arg-type]
                
                # 最终中位数填充
                median_val = all_df[col].median()
                if not pd.isna(median_val):  # type: ignore
                    all_df[col] = all_df[col].fillna(median_val)  # type: ignore
                else:
                    all_df[col] = all_df[col].fillna(0)
    
    # 重新分离
    train_df = all_df.iloc[:len(train_df)].copy()
    test_df = all_df.iloc[len(train_df):].copy()
    
    print(f"处理后训练集: {train_df.shape}")
    print(f"处理后测试集: {test_df.shape}")
    
    return train_df, test_df

def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    高级特征工程 - 业务逻辑和交互特征
    """
    df = df.copy()
    
    # 1. 高级分段特征 - 基于数据分布的自适应分段
    df['age_segment'] = pd.qcut(df['car_age'], q=5, labels=False, duplicates='drop')
    
    if 'power' in df.columns:
        df['power_segment'] = pd.qcut(df['power'], q=4, labels=False, duplicates='drop')
    
    if 'kilometer' in df.columns:
        df['km_segment'] = pd.qcut(df['kilometer'], q=5, labels=False, duplicates='drop')
    
    # 2. 业务逻辑特征
    if 'power' in df.columns and 'car_age' in df.columns:
        # 功率衰减特征
        df['power_decay'] = df['power'] / (df['car_age'] + 1)
        df['power_age_ratio'] = df['power'] / (df['car_age'] + 1)
        df['power_per_year'] = df['power'] / np.maximum(df['car_age'], 1)
    
    if 'kilometer' in df.columns and 'car_age' in df.columns:
        # 里程相关特征
        car_age_safe = np.maximum(df['car_age'], 0.1)
        df['km_per_year'] = df['kilometer'] / car_age_safe
        df['km_age_ratio'] = df['kilometer'] / (df['car_age'] + 1)
        
        # 里程使用强度分类
        df['usage_intensity'] = pd.cut(df['km_per_year'], 
                                     bins=[0, 10000, 20000, 30000, float('inf')],
                                     labels=['low', 'medium', 'high', 'very_high'])
        df['usage_intensity'] = df['usage_intensity'].cat.codes
    
    # 3. 品牌和车型的高级特征
    if 'brand' in df.columns and 'model' in df.columns:
        # 品牌内车型排名
        brand_model_stats = df.groupby(['brand', 'model']).size().reset_index(name='count')  # type: ignore[arg-type]
        brand_model_rank = brand_model_stats.sort_values(['brand', 'count'], ascending=[True, False])
        brand_model_rank['rank_in_brand'] = brand_model_rank.groupby('brand').cumcount() + 1  # type: ignore
        
        # 映射车型热度排名
        rank_map = brand_model_rank.set_index(['brand', 'model'])['rank_in_brand'].to_dict()
        df['model_popularity_rank'] = df.apply(lambda row: rank_map.get((row['brand'], row['model']), 999), axis=1)  # type: ignore
    
    # 4. 时间特征
    if 'reg_month' in df.columns:
        df['reg_season'] = df['reg_month'].map({1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 
                                               7:3, 8:3, 9:4, 10:4, 11:4, 12:4})  # type: ignore
    
    # 5. 数值特征的多项式和交互特征
    numeric_features = ['car_age', 'power', 'kilometer'] if 'power' in df.columns else ['car_age', 'kilometer']
    available_numeric = [col for col in numeric_features if col in df.columns]
    
    if len(available_numeric) >= 2:
        # 创建交互特征
        for i, col1 in enumerate(available_numeric):
            for col2 in available_numeric[i+1:]:
                # 乘积特征
                df[f'{col1}_x_{col2}'] = df[col1] * df[col2]
                # 比率特征
                df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)
    
    # 6. v特征的高级统计
    v_cols = [col for col in df.columns if col.startswith('v_')]
    if len(v_cols) >= 3:
        # 基础统计
        df['v_mean'] = df[v_cols].mean(axis=1)
        df['v_std'] = df[v_cols].std(axis=1).fillna(0)  # type: ignore
        df['v_max'] = df[v_cols].max(axis=1)
        df['v_min'] = df[v_cols].min(axis=1)
        df['v_range'] = df['v_max'] - df['v_min']
        df['v_skew'] = df[v_cols].skew(axis=1).fillna(0)  # type: ignore
        
        # 主成分分析特征
        df['v_sum'] = df[v_cols].sum(axis=1)
        df['v_positive_count'] = (df[v_cols] > 0).sum(axis=1)
        df['v_negative_count'] = (df[v_cols] < 0).sum(axis=1)
    
    # 7. 对数变换特征
    log_features = ['car_age', 'kilometer', 'power'] if 'power' in df.columns else ['car_age', 'kilometer']
    for col in log_features:
        if col in df.columns:
            df[f'log_{col}'] = np.log1p(np.maximum(df[col], 0))
            df[f'sqrt_{col}'] = np.sqrt(np.maximum(df[col], 0))
    
    # 8. 异常值和特殊值标记
    if 'power' in df.columns:
        df['power_zero'] = (df['power'] <= 0).astype(int)
        df['power_very_high'] = (df['power'] > 400).astype(int)
    
    if 'kilometer' in df.columns:
        df['km_very_high'] = (df['kilometer'] > 300000).astype(int)
        df['km_very_low'] = (df['kilometer'] < 10000).astype(int)
    
    # 9. 数据质量检查和清理
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # 处理无穷大值
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        # 处理NaN值
        null_count = df[col].isnull().sum()
        if null_count > 0:
            if col in ['price']:
                continue  # 保留目标变量的NaN
            median_val = df[col].median()
            df[col] = df[col].fillna(median_val if not pd.isna(median_val) else 0)  # type: ignore
        
        # 温和的异常值处理 - 基于分布的动态截断
        if col not in ['SaleID', 'price'] and df[col].std() > 1e-8:
            # 使用3-sigma规则，但更宽松
            mean_val = df[col].mean()
            std_val = df[col].std()
            lower_bound = mean_val - 4 * std_val
            upper_bound = mean_val + 4 * std_val
            
            # 只处理极端值
            df[col] = np.clip(df[col], lower_bound, upper_bound)
    
    return df

def optimize_hyperparameters(X_train: pd.DataFrame, y_train: pd.Series) -> Dict[str, Dict[str, Any]]:
    """
    优化的超参数调优 - 使用随机搜索大幅提升速度
    """
    print("开始优化的超参数调优...")
    
    # 对数变换目标变量
    y_train_log = np.log1p(y_train)
    
    # 缩小的LightGBM参数分布 - 基于经验最佳范围
    lgb_param_dist = {
        'num_leaves': [31, 50, 70],  # 减少选项
        'max_depth': [6, 8, 10],     # 减少选项
        'learning_rate': [0.05, 0.1, 0.15],  # 减少选项
        'feature_fraction': [0.8, 0.9],      # 减少选项
        'lambda_l1': [0.1, 0.2],             # 减少选项
        'lambda_l2': [0.1, 0.2]              # 减少选项
    }
    
    # 缩小的XGBoost参数分布 - 基于经验最佳范围
    xgb_param_dist = {
        'max_depth': [6, 8, 10],             # 减少选项
        'learning_rate': [0.05, 0.1, 0.15],  # 减少选项
        'subsample': [0.8, 0.9],             # 减少选项
        'colsample_bytree': [0.8, 0.9],      # 减少选项
        'reg_alpha': [0.3, 0.5],             # 减少选项
        'reg_lambda': [0.3, 0.5]             # 减少选项
    }
    
    # 缩小的CatBoost参数分布 - 基于经验最佳范围
    catboost_param_dist = {
        'depth': [6, 8, 10],                 # 减少选项
        'learning_rate': [0.05, 0.1, 0.15],  # 减少选项
        'l2_leaf_reg': [1.0, 2.0],           # 减少选项
        'random_strength': [0.3, 0.5]        # 减少选项
    }
    
    # 基础模型 - 减少训练轮数
    lgb_base = lgb.LGBMRegressor(objective='mae', metric='mae', random_state=42, n_estimators=100)  # 减少轮数
    xgb_base = xgb.XGBRegressor(objective='reg:absoluteerror', eval_metric='mae', random_state=42, n_estimators=100)  # 减少轮数
    cat_base = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', random_seed=42, iterations=100, verbose=False)  # 减少轮数
    
    # 3折交叉验证
    kfold = KFold(n_splits=3, shuffle=True, random_state=42)
    
    best_params = {}
    n_iter = 20  # 随机搜索次数，大幅减少搜索空间
    
    # LightGBM调优 - 使用RandomizedSearchCV
    print("调优LightGBM (随机搜索)...")
    import time
    start_time = time.time()
    lgb_search = RandomizedSearchCV(lgb_base, lgb_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    lgb_search.fit(X_train, y_train_log)
    best_params['lgb'] = lgb_search.best_params_
    lgb_time = time.time() - start_time
    print(f"LightGBM最佳参数: {lgb_search.best_params_}")
    print(f"LightGBM最佳分数: {-lgb_search.best_score_:.2f}")
    print(f"LightGBM调优用时: {lgb_time:.1f}秒")
    
    # XGBoost调优 - 使用RandomizedSearchCV
    print("调优XGBoost (随机搜索)...")
    start_time = time.time()
    xgb_search = RandomizedSearchCV(xgb_base, xgb_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    xgb_search.fit(X_train, y_train_log)
    best_params['xgb'] = xgb_search.best_params_
    xgb_time = time.time() - start_time
    print(f"XGBoost最佳参数: {xgb_search.best_params_}")
    print(f"XGBoost最佳分数: {-xgb_search.best_score_:.2f}")
    print(f"XGBoost调优用时: {xgb_time:.1f}秒")
    
    # CatBoost调优 - 使用RandomizedSearchCV
    print("调优CatBoost (随机搜索)...")
    start_time = time.time()
    cat_search = RandomizedSearchCV(cat_base, catboost_param_dist, n_iter=n_iter, cv=kfold, 
                                   scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
    cat_search.fit(X_train, y_train_log)
    best_params['cat'] = cat_search.best_params_
    cat_time = time.time() - start_time
    print(f"CatBoost最佳参数: {cat_search.best_params_}")
    print(f"CatBoost最佳分数: {-cat_search.best_score_:.2f}")
    print(f"CatBoost调优用时: {cat_time:.1f}秒")
    
    print(f"\n超参数调优总用时: {lgb_time + xgb_time + cat_time:.1f}秒")
    
    return best_params

def train_models_with_optimized_params(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, best_params: Dict[str, Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, List[float]]]:
    """
    使用优化参数训练模型
    """
    print("使用优化参数训练模型...")
    
    # 对数变换
    y_train_log = np.log1p(y_train)
    
    # 5折交叉验证
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # 使用最佳参数
    lgb_params = best_params['lgb']
    lgb_params.update({
        'objective': 'mae',
        'metric': 'mae',
        'bagging_freq': 5,
        'min_child_samples': 20,
        'random_state': 42,
    })
    
    xgb_params = best_params['xgb']
    xgb_params.update({
        'objective': 'reg:absoluteerror',
        'eval_metric': 'mae',
        'min_child_weight': 10,
        'random_state': 42
    })
    
    catboost_params = best_params['cat']
    catboost_params.update({
        'loss_function': 'MAE',
        'eval_metric': 'MAE',
        'iterations': 500,
        'random_seed': 42,
        'verbose': False
    })
    
    # 存储预测结果
    lgb_predictions = np.zeros(len(X_test))
    xgb_predictions = np.zeros(len(X_test))
    cat_predictions = np.zeros(len(X_test))
    
    # 存储验证分数
    lgb_scores, xgb_scores, cat_scores = [], [], []
    
    # 交叉验证训练
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train), 1):
        print(f"训练第 {fold} 折...")
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]  # type: ignore
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]  # type: ignore
        
        # 训练LightGBM
        lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1000)
        lgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])
        
        lgb_predictions += np.expm1(np.array(lgb_model.predict(X_test))) / 5
        lgb_val_pred = np.expm1(np.array(lgb_model.predict(X_val)))
        lgb_mae = mean_absolute_error(np.expm1(y_val_log), lgb_val_pred)
        lgb_scores.append(lgb_mae)
        
        # 训练XGBoost
        xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000, early_stopping_rounds=50)
        xgb_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     verbose=False)
        
        xgb_predictions += np.expm1(xgb_model.predict(X_test)) / 5
        xgb_val_pred = np.expm1(xgb_model.predict(X_val))
        xgb_mae = mean_absolute_error(np.expm1(y_val_log), xgb_val_pred)
        xgb_scores.append(xgb_mae)
        
        # 训练CatBoost
        cat_model = CatBoostRegressor(**catboost_params)
        cat_model.fit(X_tr, y_tr_log, 
                     eval_set=[(X_val, y_val_log)], 
                     early_stopping_rounds=50, 
                     verbose=False)
        
        cat_predictions += np.expm1(cat_model.predict(X_test)) / 5
        cat_val_pred = np.expm1(cat_model.predict(X_val))
        cat_mae = mean_absolute_error(np.expm1(y_val_log), cat_val_pred)
        cat_scores.append(cat_mae)
        
        print(f"  LightGBM MAE: {lgb_mae:.2f}, XGBoost MAE: {xgb_mae:.2f}, CatBoost MAE: {cat_mae:.2f}")
    
    print(f"\n平均验证分数:")
    print(f"  LightGBM: {np.mean(lgb_scores):.2f} (±{np.std(lgb_scores):.2f})")
    print(f"  XGBoost: {np.mean(xgb_scores):.2f} (±{np.std(xgb_scores):.2f})")
    print(f"  CatBoost: {np.mean(cat_scores):.2f} (±{np.std(cat_scores):.2f})")
    
    return lgb_predictions, xgb_predictions, cat_predictions, {
        'lgb_scores': lgb_scores,
        'xgb_scores': xgb_scores,
        'cat_scores': cat_scores
    }

def stacking_ensemble(X_train: pd.DataFrame, y_train: Union[pd.Series, pd.DataFrame], X_test: pd.DataFrame, lgb_pred: np.ndarray, xgb_pred: np.ndarray, cat_pred: np.ndarray) -> np.ndarray:
    """
    Stacking集成策略 - 线性回归元学习器
    """
    print("执行Stacking集成...")
    
    # 对数变换
    y_train_log = np.log1p(y_train)
    
    # 创建元特征
    meta_features_train = np.column_stack([
        # 使用交叉验证获得训练集的元特征
        np.zeros(len(X_train)),  # LightGBM预测
        np.zeros(len(X_train)),  # XGBoost预测
        np.zeros(len(X_train))   # CatBoost预测
    ])
    
    meta_features_test = np.column_stack([lgb_pred, xgb_pred, cat_pred])
    
    # 5折交叉验证生成训练集元特征
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]  # type: ignore
        y_tr_log, y_val_log = y_train_log.iloc[train_idx], y_train_log.iloc[val_idx]  # type: ignore
        
        # 训练基础模型（使用简化参数以节省时间）
        lgb_model = lgb.LGBMRegressor(objective='mae', num_leaves=31, max_depth=6, 
                                    learning_rate=0.1, random_state=42, n_estimators=200)
        lgb_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 0] = np.expm1(lgb_model.predict(X_val))
        
        xgb_model = xgb.XGBRegressor(objective='reg:absoluteerror', max_depth=6, 
                                    learning_rate=0.1, random_state=42, n_estimators=200)
        xgb_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 1] = np.expm1(xgb_model.predict(X_val))  # type: ignore
        
        cat_model = CatBoostRegressor(loss_function='MAE', depth=6, learning_rate=0.1, 
                                    random_seed=42, iterations=200, verbose=False)
        cat_model.fit(X_tr, y_tr_log)
        meta_features_train[val_idx, 2] = np.expm1(cat_model.predict(X_val))  # type: ignore
    
    # 训练元学习器
    meta_learner = LinearRegression()
    meta_learner.fit(meta_features_train, y_train)
    
    # 预测测试集
    stacking_pred = meta_learner.predict(meta_features_test)
    
    print(f"Stacking权重: {meta_learner.coef_}")
    print(f"Stacking截距: {meta_learner.intercept_}")
    
    return stacking_pred

def advanced_ensemble_and_calibration(lgb_pred: np.ndarray, xgb_pred: np.ndarray, cat_pred: np.ndarray, stacking_pred: np.ndarray, y_train: pd.Series, scores_info: Dict[str, List[float]]) -> np.ndarray:
    """
    高级集成和智能校准
    """
    print("执行高级集成和智能校准...")
    
    # 1. 基于性能的自适应权重
    lgb_score = np.mean(scores_info['lgb_scores'])
    xgb_score = np.mean(scores_info['xgb_scores'])
    cat_score = np.mean(scores_info['cat_scores'])
    
    # 评估Stacking性能（使用交叉验证估计）
    # 由于stacking_pred是测试集预测，我们使用其他模型的平均性能作为估计
    stacking_score = (lgb_score + xgb_score + cat_score) / 3
    
    # 计算权重
    models_scores = {
        'lgb': lgb_score,
        'xgb': xgb_score,
        'cat': cat_score,
        'stacking': stacking_score
    }
    
    # 性能越好权重越大
    total_inv_score = sum(1/score for score in models_scores.values())
    weights = {model: (1/score) / total_inv_score for model, score in models_scores.items()}
    
    print(f"\n模型权重:")
    for model, weight in weights.items():
        print(f"  {model}: {weight:.3f}")
    
    # 2. 多层集成
    models_preds = {
        'lgb': lgb_pred,
        'xgb': xgb_pred,
        'cat': cat_pred,
        'stacking': stacking_pred
    }
    
    # 加权平均
    weighted_ensemble = sum(weights[model] * pred for model, pred in models_preds.items())
    
    # 3. 智能校准
    # 分位数校准
    train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])
    pred_quantiles = np.percentile(weighted_ensemble, [10, 25, 50, 75, 90])
    
    # 创建校准映射
    calibration_map = {}
    for i in range(len(train_quantiles)):
        if pred_quantiles[i] > 0:
            calibration_map[pred_quantiles[i]] = train_quantiles[i]
    
    # 应用校准
    calibrated_pred = np.copy(weighted_ensemble)
    for i, pred_val in enumerate(weighted_ensemble):  # type: ignore[arg-type]
        # 找到最近的分位点进行校准
        closest_quantile = pred_quantiles[np.argmin(np.abs(pred_quantiles - pred_val))]
        if closest_quantile in calibration_map:
            calibration_factor = calibration_map[closest_quantile] / closest_quantile
            calibrated_pred[i] *= calibration_factor
    
    # 4. 最终约束检查
    calibrated_pred = np.maximum(calibrated_pred, 0)  # 确保非负
    
    # 基于训练集分布的最终调整
    train_mean, train_std = y_train.mean(), y_train.std()
    pred_mean, pred_std = calibrated_pred.mean(), calibrated_pred.std()
    
    # 调整均值和标准差
    if pred_std > 0:
        calibrated_pred = (calibrated_pred - pred_mean) * (train_std / pred_std) + train_mean
    
    calibrated_pred = np.maximum(calibrated_pred, 0)
    
    print(f"\n校准统计:")
    print(f"  训练集: 均值={train_mean:.2f}, 标准差={train_std:.2f}")
    print(f"  校准前: 均值={pred_mean:.2f}, 标准差={pred_std:.2f}")
    print(f"  校准后: 均值={calibrated_pred.mean():.2f}, 标准差={calibrated_pred.std():.2f}")
    
    return calibrated_pred

def create_comprehensive_analysis_plots(y_train: pd.Series, predictions: np.ndarray, scores_info: Dict[str, List[float]], model_name: str = "modeling_v17") -> None:
    """
    创建全面的分析图表
    """
    print("生成全面分析图表...")
    
    # 创建保存目录
    analysis_dir = get_user_data_path()
    os.makedirs(analysis_dir, exist_ok=True)
    
    # 创建图表
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    
    # 1. 价格分布对比
    axes[0, 0].hist(y_train, bins=50, alpha=0.7, label='训练集真实价格', color='blue', density=True)
    axes[0, 0].hist(predictions, bins=50, alpha=0.7, label='测试集预测价格', color='red', density=True)
    axes[0, 0].set_xlabel('价格')
    axes[0, 0].set_ylabel('密度')
    axes[0, 0].set_title('价格分布对比')
    axes[0, 0].legend()
    
    # 2. 模型性能对比
    models = ['LightGBM', 'XGBoost', 'CatBoost']
    scores = [np.mean(scores_info['lgb_scores']), 
              np.mean(scores_info['xgb_scores']), 
              np.mean(scores_info['cat_scores'])]
    
    axes[0, 1].bar(models, scores, color=['lightblue', 'lightgreen', 'lightcoral'])
    axes[0, 1].set_ylabel('MAE')
    axes[0, 1].set_title('各模型验证性能')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. Q-Q图检查分布
    from scipy import stats
    stats.probplot(predictions, dist="norm", plot=axes[0, 2])
    axes[0, 2].set_title('预测值Q-Q图')
    
    # 4. 预测值分布直方图
    axes[1, 0].hist(predictions, bins=50, alpha=0.7, color='red')
    axes[1, 0].set_xlabel('预测价格')
    axes[1, 0].set_ylabel('频次')
    axes[1, 0].set_title('预测值分布')
    
    # 5. 累积分布函数
    sorted_pred = np.sort(predictions)
    cumulative = np.arange(1, len(sorted_pred) + 1) / len(sorted_pred)
    axes[1, 1].plot(sorted_pred, cumulative)
    axes[1, 1].set_xlabel('预测价格')
    axes[1, 1].set_ylabel('累积概率')
    axes[1, 1].set_title('预测值累积分布')
    
    # 6. 价格区间分析
    price_bins = [0, 5000, 10000, 20000, 50000, 100000, float('inf')]
    price_labels = ['<5K', '5K-10K', '10K-20K', '20K-50K', '50K-100K', '>100K']
    pred_categories = pd.cut(predictions, bins=price_bins, labels=price_labels)
    category_counts = pred_categories.value_counts().sort_index()  # type: ignore[attr-defined]
    
    axes[1, 2].bar(category_counts.index, category_counts.values)
    axes[1, 2].set_xlabel('价格区间')
    axes[1, 2].set_ylabel('车辆数量')
    axes[1, 2].set_title('预测价格区间分布')
    axes[1, 2].tick_params(axis='x', rotation=45)
    
    # 7. 训练集vs预测集统计对比
    train_stats = [y_train.mean(), y_train.std(), y_train.min(), y_train.max()]
    pred_stats = [predictions.mean(), predictions.std(), predictions.min(), predictions.max()]
    stats_labels = ['均值', '标准差', '最小值', '最大值']
    
    x = np.arange(len(stats_labels))
    width = 0.35
    
    axes[2, 0].bar(x - width/2, train_stats, width, label='训练集', color='blue', alpha=0.7)
    axes[2, 0].bar(x + width/2, pred_stats, width, label='预测集', color='red', alpha=0.7)
    axes[2, 0].set_xlabel('统计指标')
    axes[2, 0].set_ylabel('值')
    axes[2, 0].set_title('统计指标对比')
    axes[2, 0].set_xticks(x)
    axes[2, 0].set_xticklabels(stats_labels)
    axes[2, 0].legend()
    
    # 8. 预测值箱线图
    axes[2, 1].boxplot(predictions)
    axes[2, 1].set_ylabel('预测价格')
    axes[2, 1].set_title('预测值箱线图')
    
    # 9. 详细统计信息
    stats_text = f"""
    modeling_v17版本详细统计:
    
    训练集统计:
    样本数: {len(y_train):,}
    均值: {y_train.mean():.2f}
    标准差: {y_train.std():.2f}
    中位数: {y_train.median():.2f}
    范围: {y_train.min():.2f} - {y_train.max():.2f}
    
    预测集统计:
    样本数: {len(predictions):,}
    均值: {predictions.mean():.2f}
    标准差: {predictions.std():.2f}
    中位数: {np.median(predictions):.2f}
    范围: {predictions.min():.2f} - {predictions.max():.2f}
    
    模型性能:
    LightGBM: {np.mean(scores_info["lgb_scores"]):.2f}
    XGBoost: {np.mean(scores_info["xgb_scores"]):.2f}
    CatBoost: {np.mean(scores_info["cat_scores"]):.2f}
    """
    axes[2, 2].text(0.05, 0.95, stats_text, transform=axes[2, 2].transAxes, 
                    fontsize=9, verticalalignment='top', fontfamily='monospace')
    axes[2, 2].set_title('详细统计信息')
    axes[2, 2].axis('off')
    
    plt.tight_layout()
    
    # 保存图表
    chart_path = os.path.join(analysis_dir, f'{model_name}_comprehensive_analysis.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"分析图表已保存到: {chart_path}")
    plt.show()

def v17_optimize_model() -> Tuple[np.ndarray, Dict[str, List[float]]]:
    """
    V17优化模型训练流程 - 冲击500分目标
    """
    print("=" * 80)
    print("开始V17优化模型训练 - 冲击500分目标")
    print("=" * 80)
    
    # 加载和预处理数据
    print("步骤1: 高级数据加载和预处理...")
    train_df, test_df = load_and_preprocess_data()
    
    print("步骤2: 高级特征工程...")
    train_df = create_advanced_features(train_df)
    test_df = create_advanced_features(test_df)
    
    # 准备特征
    y_col = 'price'
    feature_cols = [c for c in train_df.columns if c not in [y_col, 'SaleID']]
    
    X_train = train_df[feature_cols].copy()
    y_train = train_df[y_col].copy()
    X_test = test_df[feature_cols].copy()
    
    print(f"特征数量: {len(feature_cols)}")
    print(f"训练集大小: {X_train.shape}")
    print(f"测试集大小: {X_test.shape}")
    
    # 特征选择 - 基于重要性选择
    print("\n步骤3: 特征选择...")
    selector = SelectFromModel(estimator=RandomForestRegressor(n_estimators=100, random_state=42), 
                             threshold='median')
    selector.fit(X_train, y_train)
    
    selected_features = X_train.columns[selector.get_support()].tolist()
    print(f"选择的特征数量: {len(selected_features)}")
    
    X_train_selected: pd.DataFrame = X_train[selected_features].copy()  # type: ignore[assignment]
    X_test_selected: pd.DataFrame = X_test[selected_features].copy()  # type: ignore[assignment]
    
    # 特征缩放
    print("\n步骤4: 特征缩放...")
    scaler = RobustScaler()
    numeric_features = X_train_selected.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_features:
        if col in X_train_selected.columns and col in X_test_selected.columns:
            # 检查和处理数值问题
            inf_mask = np.isinf(X_train_selected[col]) | np.isinf(X_test_selected[col])
            if inf_mask.any():  # type: ignore[func-returns-value]
                X_train_selected.loc[inf_mask[inf_mask.index.isin(X_train_selected.index)].index, col] = 0  # type: ignore[index]
                X_test_selected.loc[inf_mask[inf_mask.index.isin(X_test_selected.index)].index, col] = 0  # type: ignore[index]
            
            X_train_selected[col] = X_train_selected[col].fillna(X_train_selected[col].median())  # type: ignore[assignment]
            X_test_selected[col] = X_test_selected[col].fillna(X_train_selected[col].median())  # type: ignore[assignment]
            
            if X_train_selected[col].std() > 1e-8:
                X_train_selected[col] = scaler.fit_transform(X_train_selected[[col]])
                X_test_selected[col] = scaler.transform(X_test_selected[[col]])
    
    # 超参数调优
    print("\n步骤5: 超参数精细调优...")
    best_params = optimize_hyperparameters(X_train_selected, y_train)  # type: ignore[arg-type]
    
    # 使用优化参数训练模型
    print("\n步骤6: 使用优化参数训练模型...")
    lgb_pred, xgb_pred, cat_pred, scores_info = train_models_with_optimized_params(
        X_train_selected, y_train, X_test_selected, best_params)  # type: ignore[arg-type]
    
    # Stacking集成
    print("\n步骤7: Stacking集成...")
    stacking_pred = stacking_ensemble(X_train_selected, y_train, X_test_selected, 
                                    lgb_pred, xgb_pred, cat_pred)  # type: ignore[arg-type]
    
    # 高级集成和校准
    print("\n步骤8: 高级集成和智能校准...")
    final_predictions = advanced_ensemble_and_calibration(
        lgb_pred, xgb_pred, cat_pred, stacking_pred, y_train, scores_info)  # type: ignore[arg-type]
    
    # 创建全面分析图表
    print("\n步骤9: 生成分析图表...")
    create_comprehensive_analysis_plots(y_train, final_predictions, scores_info, "modeling_v17")  # type: ignore[arg-type]
    
    # 最终统计
    print(f"\n最终预测统计:")
    print(f"均值: {final_predictions.mean():.2f}")
    print(f"标准差: {final_predictions.std():.2f}")
    print(f"范围: {final_predictions.min():.2f} - {final_predictions.max():.2f}")
    print(f"中位数: {np.median(final_predictions):.2f}")
    
    # 创建提交文件
    submission_df = pd.DataFrame({
        'SaleID': test_df['SaleID'] if 'SaleID' in test_df.columns else test_df.index,
        'price': final_predictions
    })
    
    # 保存结果
    result_dir = get_project_path('prediction_result')
    os.makedirs(result_dir, exist_ok=True)
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    result_file = os.path.join(result_dir, f"modeling_v17_result_{timestamp}.csv")
    submission_df.to_csv(result_file, index=False)
    print(f"\n结果已保存到: {result_file}")
    
    # 生成优化报告
    print("\n" + "=" * 80)
    print("V17优化总结 - 冲击500分目标")
    print("=" * 80)
    print("✅ 高级数据预处理 - 精确异常值处理和智能缺失值填充")
    print("✅ 高级特征工程 - 业务逻辑特征和交互特征")
    print("✅ 特征选择 - 基于重要性的智能特征筛选")
    print("✅ 超参数精细调优 - 网格搜索找到最佳配置")
    print("✅ Stacking集成策略 - 线性回归元学习器")
    print("✅ 高级集成和校准 - 多层集成和分位数校准")
    print("✅ 全面分析图表 - 深入理解模型性能")
    print("=" * 80)
    
    return final_predictions, scores_info

if __name__ == "__main__":
    test_pred, scores_info = v17_optimize_model()
    print("V17优化完成! 期待突破500分目标! 🎯")
