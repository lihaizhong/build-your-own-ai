# 神经网络基础与Tensorflow实战

深度学习框架：Tensorflow、Keras、PyTorch、Paddle

学习点：

- 神经网络结构
- 激活函数
- 损失函数
- 反向传播
- 梯度下降
- 优化方法（SGD、Adam）

实战：

- 使用 numpy 搭建神经网络
- 使用 Tensorflow 搭建神经网络

---

Tensorflow 适合生产环境，大规模部署；PyTorch 快速实验，学术研究。

---

模拟生物的神经元信号传递规则：兴奋 和 抑制。

- 神经元接收输入信号
- 所有信号的加权总和超过一个“阔值”，就被“激活”（兴奋），向下传递信息
- 否则就保持“抑制”

这种简单的开关机制构成了整个网络决策和学习的根本。

---

一个神经元就是一个决策小单元。上面说的开关就是 **激活函数** 实现。

- 接收上游信号
- 做决定：信息是否足够重要？是否需要继续向下传递？

整个神经网络由无数个神经元组成。

---

神经网络 由 **输入层**、**隐藏层**、**输出层** 组成。

---

常见激活函数

1. ReLU（修正线性单元）
    - 公式：$f(x) = max(0, x)$
    - 特点：当输入为正时输出原值，负值时输出0。
    - 优点：计算简单，能有效缓解梯度消失问题，最常用于隐藏层。
    - 示例：如果输入是 -2，输出0；如果是3，输出3。
2. Sigmoid（S型函数）
    - 公式：$f(x) = 1 / (1 + e^{-x})$
    - 特点：将输入压缩到 0~1 之间，适合二分类问题的输出层。
    - 缺点：在深层网络中容易出现梯度消失（计算梯度时接近0），导致训练困难。
3. Tanh（双曲正切函数）
    - 公式：$f(x) = (e^x - e^{-x}) / (e^x + e^{-x})$
    - 特点：将输入压缩到 -1~1 之间，比 Sigmoid 更对称。
    - 缺点：同样存在梯度消失问题。
4. Softmax（多分类专用）
    - 用途：通常用于多分类问题的输出层，将输出转化为概率分布（所有输出和为1）。
    - 示例：识别猫/狗/鸟的分类时，输出可能是 [0.1, 0.8, 0.1]，表示80%概率是狗。

激活函数就像神经元的“开关”：
    - 当输入特征达到“合适条件”时，神经元“点亮”（输入非0）；
    - 不符合特征时，神经元“熄灭”（输入等于0）；

正是这种“点亮-熄灭”的组合，让神经网络能从边缘、纹理一步一步拼出眼睛、鼻子、耳朵，最终认出猫和狗！

---

- 激活函数的特点是非线性的，而数据的分布绝大多数是非线性的 => 可以强化网络的学习能力
- 不同的激活函数特点不同，应用也不同

> Sigmoid 和 Tanh 函数输出值在 (0, 1) 和 (-1, 1) 之间 => 适合处理概率值

> 会产生梯度消失 => 不适合深层网络训练

> ReLU 的有效导数是常数 1，解决了深层网络中出现的梯度消失问题 => 更适合深层网络训练

---

反向传播
