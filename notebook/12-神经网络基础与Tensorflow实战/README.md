# 神经网络基础与Tensorflow实战

深度学习框架：Tensorflow、Keras、PyTorch、Paddle

学习点：

- 神经网络结构
- 激活函数
- 损失函数
- 反向传播
- 梯度下降
- 优化方法（SGD、Adam）

实战：

- 使用 numpy 搭建神经网络
- 使用 Tensorflow 搭建神经网络

---

Tensorflow 适合生产环境，大规模部署；PyTorch 快速实验，学术研究。

---

模拟生物的神经元信号传递规则：兴奋 和 抑制。

- 神经元接收输入信号
- 所有信号的加权总和超过一个“阔值”，就被“激活”（兴奋），向下传递信息
- 否则就保持“抑制”

这种简单的开关机制构成了整个网络决策和学习的根本。

---

一个神经元就是一个决策小单元。上面说的开关就是 **激活函数** 实现。

- 接收上游信号
- 做决定：信息是否足够重要？是否需要继续向下传递？

整个神经网络由无数个神经元组成。

---

神经网络 由 **输入层**、**隐藏层**、**输出层** 组成。

---

使用 numpy 模拟前向传播

1. 初始化网络
    - 设置 3 层神经网络的 $W$ 和 $b$
2. 定义激活函数
    - 第一层，第二层的激活函数使用 Sigmoid
    - 输出层的激活函数使用 恒等函数，即 $g(x) = x$
3. 向前传播
    - 前向传播比较简单，就是向量点乘（即加权求和），然后经过一个激活函数。最终输出预测结构 $Y'$

> 第一层、第二层：$z^i = W^i * a^{i - 1} + b^i$

> 输出层：$a^i = g^i(z^i)$

---

常见激活函数

1. ReLU（修正线性单元）
    - 公式：$f(x) = max(0, x)$
    - 特点：当输入为正时输出原值，负值时输出0。
    - 优点：计算简单，能有效缓解梯度消失问题，最常用于隐藏层。
    - 示例：如果输入是 -2，输出0；如果是3，输出3。
2. Sigmoid（S型函数）
    - 公式：$f(x) = 1 / (1 + e^{-x})$
    - 特点：将输入压缩到 0~1 之间，适合二分类问题的输出层。
    - 缺点：在深层网络中容易出现梯度消失（计算梯度时接近0），导致训练困难。
3. Tanh（双曲正切函数）
    - 公式：$f(x) = (e^x - e^{-x}) / (e^x + e^{-x})$
    - 特点：将输入压缩到 -1~1 之间，比 Sigmoid 更对称。
    - 缺点：同样存在梯度消失问题。
4. Softmax（多分类专用）
    - 用途：通常用于多分类问题的输出层，将输出转化为概率分布（所有输出和为1）。
    - 示例：识别猫/狗/鸟的分类时，输出可能是 [0.1, 0.8, 0.1]，表示80%概率是狗。

激活函数就像神经元的“开关”：
    - 当输入特征达到“合适条件”时，神经元“点亮”（输入非0）；
    - 不符合特征时，神经元“熄灭”（输入等于0）；

正是这种“点亮-熄灭”的组合，让神经网络能从边缘、纹理一步一步拼出眼睛、鼻子、耳朵，最终认出猫和狗！

---

- 激活函数的特点是非线性的，而数据的分布绝大多数是非线性的 => 可以强化网络的学习能力
- 不同的激活函数特点不同，应用也不同

> Sigmoid 和 Tanh 函数输出值在 (0, 1) 和 (-1, 1) 之间 => 适合处理概率值

> 会产生梯度消失 => 不适合深层网络训练

> ReLU 的有效导数是常数 1，解决了深层网络中出现的梯度消失问题 => 更适合深层网络训练

---

从 0 编写一个神经网络

1. 定义网络结构
    - 指定 输入层、隐藏层、输出层 的大小
2. 初始化模型参数
3. 循环操作：
    1. 执行前向传播
    2. 计算损失函数
    3. 执行后向传播
    4. 权值更新

---

损失函数（Loss Function）

- 用来衡量模型预测值与真实值不一致的程度，是一个非负数函数
- 如果我们想要衡量神经网络对于两个类别的分类能力，可以使用二分类交叉熵损失函数（binary crossentropy）

> 损失函数计算：平均绝对误差（Mean Absolute Error, MAE）和 均方误差（Mean Squared Error, MSE）

---

反向传播

就是通过 **链式法则**，从最终结果开始，一层层地向后计算每一个节点、每个参数对最终结果的“影响力”（即梯度）。

> 在真实的神经网络中，计算出这个“影响力”后，就可以根据它来微调参数 => 这就是“学习”的过程。

优化方法

- SGD
    - 算法收敛速度快，但容易收敛到局部最优。
    - 缺点是更新方向依赖于当前 batch 计算出的梯度，因而很不稳定
- Momentum
    - 借用了物理中的动量概念，更新的时候在一定程度上保留之前更新的方向
    - 同时利用当前 batch 的梯度微调最终的更新方向，即模拟了运动惯性
- AdaGrad
    - 自适应梯度算法，能够在训练中自动的对 learning rate 进行调整
    - 对于出现频率较低参数采用较大的 $α$ 更新
    - 对于出现频率较高的参数采用较小的 $α$ 更新
- RMSProp
    - 均方根传播，AdaGrad 会累积啊之前所有的梯度平方
    - 而 RMSProp 仅仅是计算对应的平均值
    - 可以缓解 AdaGrad 算法学习率下降较快的问题
- Adam
    - 结合了 AdaGrad 和 RMSProp 算法最优的性能
    - 它还是能提供解决稀疏梯度和噪声问题的优化方法
    - 在深度学习中使用较多

---

前馈神经网络的训练过程可以分为以下三步：

- 前向计算每一层的状态和激活值，直到最后一层
- 反向计算每一层的参数的偏导数
- 更新参数

---

TensorFlow 中的计算图与会话管理

计算图：是一个由 **节点（操作）** 和 **边（张量）** 组成的数据流图，描述计算的依赖关系。

优点：

- 高效执行：图优化（如算子融合）可提升性能。
- 跨平台支持：可倒出道移动端、嵌入式设备等。

> 计算图可加速计算，图模式下运行在 C++ 层执行，避免 Python 循环的慢速交互。

> 能并行优化：自动调度无依赖的节点并行计算（如GPU/多核CPU）。

什么是张量？

张量（Tensor）是 TensorFlow 中的核心数据结构，可以理解为多维数组。

- 标量（0维）：单个数值（如 3.0）。
- 向量（1维）：一维数组（如 [1, 2, 3]）。
- 矩阵（2维）：二维表格（如 [[1, 2], [3, 4]]）。
- 高维张量：如 RGB 图片（3维： [高度, 宽度, 通道]）。

> 张量存储数据，并在计算图中流动，支持 **自动求导** 和 **GPU加速**。

1. tf.function：自动构建计算图
    - 作用：将普通 Python 函数转换为 TensorFlow 计算图，提升执行效率（如减少 Python 调用开销）
    - 优势：
        - 加速训练/推理：适合循环、矩阵运算等密集计算。
        - 跨平台部署：可导出为 SavedModel 供 TensorFlow Serving 使用。
2. 即时执行（Eager Execution）
    - 一种交互式编程模式，让代码像普通 Python 程序一样逐行运行（类似 NumPy），无需先构建计算图，便于调试。
    - 与图的协作：
        - 调试时：用即时执行快速验证逻辑。
        - 部署时：用 `@tf.function` 转换为图模式提升性能。

> TF 1.x：需显示构建图和会话，适合高性能场景。
>
> TF 2.x：默认即时执行，tf.function 实现图模式加速。

---



