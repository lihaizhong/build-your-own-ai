Q：Qwen3-Coder-480B-A35B-Instruct、Qwen3-Coder-30B-A3B-Instruct、Qwen3-Coder-Plus哪个好？
Qwen3-Coder-Plus > Qwen3-Coder-480B-A35B-Instruct > Qwen3-Coder-30B-A3B-Instruct

Q：win10 wucai复制操作不太方便，怎么解决
0.0.57 已经更新了，针对win10的复制黏贴
鼠标右键 也可以黏贴


Q：wucai经常超时该怎么解决呢？
方法1：/compact 压缩下上下文，可能上下文太长了，导致推理时间过慢
方法2：多按ESC，先中止，再继续提问

Q：
1.原始文档PDF(16M)>通过minerU转成了.md文件(733K)
2.数据预处理文档&数据分块处理 >通过脚本得到了预处理和分块后的.json文件
#Q：是否需要做预处理，我看有些数据较少的CASE是没有这一步的；如果需要需要注意些什么
如果数据量不大，可以直接放进去，不用预处理

#Q: 对于企业级的数据，数据预处理的工作是交给AI还是需要人工做数据治理/数据开发？
都有，AI做初步的，人工做校验


Lingma排在哪？

Q：如果我有一个10MB的文件，想问LLM总结我的文档，请问也是一样的流程，先切分，再找TOP10，让LLM总结是吗？
对的，如果10MB的文件，文本长度 > LLM上下文窗口的长度，就需要按照这个流程


而且在输入那里没办法用鼠标直接点击前面的内容。


Q：qwen3-vl现在有小尺寸版本吗，ollama上看见只有完整版
有8B，4B，A3B


Q：已经完成的 打卡 还传到wucai么
可以

Q：老师, 现在有可以处理图片文字的模型吗
Qwen3-VL

Q：一般一款软件的代码量极大。针对现有的一个软件源码管理。是否可以利用RAG的原理来管理代码？
是的，codebase 就是通过RAG的方式


我按我case的步骤问哈


Q：jupiterhub怎么用的
jupyter lab，安装anaconda，然后 win开始的地方，输入 jupyter 可以看到这个应用

Q：linux下的terminal，上下键不支持，只有alacritty


Q：wucai写代码有版本管理了 我写的前端页面 然后更新新的功能后，直接老的功能优化又不见了

我的预处理的时候是不是把所有的数据都读到显存里，一起做embedding？
8.向量数据库检索>通过faiss的查询功能实现 》#Q：是否理解成会自动读取第4步的.index文件？



Q：刚才那个网站是什么
下一版的wucai，计划在 wucai里面 /work 进入到历史的项目，就会在本地有这个页面，查看历史项目

Q：Lingma底层就是qwen3-coder-plus吗？
是的

qwen3-coder的30B，Q4 GGUF能在本地Ollama里面跑，现存16G就行

Q：老师动态知识库怎么构建？


Q：老师，ima 笔记是不是用 DeepSeek +faiss 搭建出来的
原理是类似的，

知识 => 召回 => 重排序
1000万chunks => 快速筛选 1000 (召回） => 排序 10 （精细化的计算）

Q：打分的依据是什么？
神经网络的训练，是通过大量的 input 和 output
一开始可以人工进行标注
比如：
query = "What is the capital of France?"
docs = ["Paris is the capital of France.", "Berlin is the capital of Germany."]

人工标注 [1, 0]

打分是样本学习

如果用autodl部署的话，是不是就不用调用API了


Q：embedding模型为啥不需要gpu
需要，神经网络都需要
CPU 和 GPU 相差20倍的时间


Q：老师，那为什么不直接用rerank模型来检索。
rerank  精确的模型，时间会长
embedding 批量的语义相似的粗筛，faiss => 100ms 针对10亿的数据


Q：训练大模型是一直跟大模型进行提问交互吗
input, output =>  让大模型拟合

Q：rerank是不是像通用模型的教练模型
就是score打分模型，给 query, answer进行打分


示例1: 上下文依赖型Query
对话历史: 
用户: "我想了解一下上海迪士尼乐园的最新项目。"
AI: "上海迪士尼乐园最新推出了'疯狂动物城'主题园区，这里有朱迪警官和尼克狐的互动体验。"
用户: "这个园区有什么游乐设施？"
AI: "'疯狂动物城'园区目前有疯狂动物城警察局、朱迪警官训练营和尼克狐的冰淇淋店等设施。"

当前查询: 还有其他设施吗？
改写结果: 除了疯狂动物城警察局、朱迪警官训练营和尼克狐的冰淇淋店之外，'疯狂动物城'园区还有其他设施吗？

Q：Query改写一般用啥大模型,还是通用大模型
通用大模型


示例2: 对比型Query
对话历史:
用户: "我想了解一下上海迪士尼乐园的最新项目。"
AI: "上海迪士尼乐园最新推出了疯狂动物城主题园区，还有蜘蛛侠主题园区"

当前查询: 哪个游玩的时间比较长，比较有趣
改写结果: 改写后的查询：  
“上海迪士尼乐园的疯狂动物城主题园区和蜘蛛侠主题园区，哪个游玩时间更长、体验更有趣？”



示例6: 自动识别Query类型
测试查询 1: 还有其他游乐项目吗？
  识别类型: 上下文依赖型
  改写结果: 除了已知的游乐项目外，还有哪些其他游乐项目？
  置信度: 0.95

测试查询 2: 哪个园区更好玩？
  识别类型: 对比型
  改写结果: 请比较各个园区的娱乐性，指出哪个更好玩。
  置信度: 0.95

测试查询 3: 都适合小朋友吗？
  识别类型: 模糊指代型
  改写结果: 哪些产品或活动适合小朋友？
  置信度: 0.95

测试查询 4: 有什么餐厅？价格怎么样？
  识别类型: 多意图型
  改写结果: 有哪些餐厅？这些餐厅的价格怎么样？
  置信度: 0.95

测试查询 5: 这不会也要排队两小时吧？
  识别类型: 反问型
  改写结果: 这需要排队两小时吗？
  置信度: 0.95


Query应用场景一般是什么


Q：要是prompt太长了超过大模型理解的长度怎么办？
一般改写，是因为用户提问可能比较短，没有说清楚


Q：query改写这部分，可能还是需要一些语言相关的模型吧，需要深度理解
LLM 就是一个语言模型，query改写 是让LLM来改写

Q：我们什么时候用到改写
对于用户提问模糊的时候

Q：怎么判断 需不需要改写用户的 问题？
针对问答系统，业务场景是：智能客服

Query只是一段代码，是借助于大模型运行，跟Rerank那种本身就是一个大模型不一样对吗？Query是不是直接嵌在我们训练好的大模型里面？


Q：多轮的对话，是需要将几轮的上下文，带入后，改写会更准确，这里范围需要如何设置带入多大的上下文
通过规则设定，一般设置k=5

CoT Chain of Tought

置信度：

帮我设计XXXX方案，给我3个方案以及置信度


Q：中国的滑雪胜地在哪里？

黑龙江哈尔滨常年冬天下大雪，雪质量XX 

这个知识能回答什么问题？

设备故障的解决方案

A001故障，遇到这个问题，应该Step1, Step2, ...

LLM prompt

你是一个 doc2query专家，帮我基于以下的知识，代理生成几个用户常问的问题，方便后续做问题的检索匹配
以下是知识：
====
本文介绍了深度学习模型训练中的优化技巧，包括：
1. 使用 AdamW 优化器替代传统的 SGD。
2. 采用混合精度训练，减少显存占用。
3. 使用分布式训练技术加速大规模模型的训练……
===
帮我生成几个问题，这些问题可以用以上知识来回答，问题可以是抽象概括的，因为用户往往问的问题不是很细致


以后得知识
= 
问题 + 知识

NotebookLM > ima


  1. 查询: 如果我想体验最刺激的过山车，应该去哪个区域？

如果答案里面 没有明确说明 XXX这个可以让你体验最刺激的过山车 => query 和 answer之间的关联程度就不大
doc2query，我们给知识库，增加了一些问题
这些问题 可以和用户的query，构建更好的联系


上海迪士尼乐园门票价格为：成人票平日399元，周末及节假日499元；儿童票（身高1.0-1.4米）平日299元，周末374元，1.0米以下儿童免费。园区通常每天开放，营业时间为上午8:00至晚上8:00。必玩项目包括创极速光轮（明日世界）、七个小矮人矿山车（梦幻世界）、加勒比海盗：战争之潮（宝藏湾）和翱翔·飞越地平线（探险岛）。部分游乐项目设有身高限制。游客可携带密封包装的零食和水入园，但禁止携带玻璃瓶及酒精饮料。园内停车场收费为100元/天。

Q：grahpRAG文件夹里没有可以执行的脚本呐，如果我想试试自己跑呢~ 
github上面可以看到graphrag

Q：知识入库，是把整理好的“内容”入库？还是整个JSON “内容”加“关键字”一起入库？
整理好的内容


Q：老师，topk 和 rerank 可以一起用吗，关联关系和顺序是怎样的，还是说要分开用
TopK 一般是embedding里面筛选的个数，比如 Top30
rerank 是针对传入的30个chunk 进行精细化的打分，然后再筛 Top5

打卡的任务再说下吧，没懂怎么打卡


Q：TopK、rerank都是自定义的变量还是通用的概念
TopK 一般是在embedding 召回中定义的 TopK的个数


Q：今天介绍了很多召回方法，怎么选择合适的召回方法
small-to-big，一般用于多文档，长文档
query2doc, doc2query => 知识库的优化
关键词、向量 混合召回 => 关键词召回10个 + 向量召回10个


Q：k的数量大是不是就不需要rerank
TopK 如果在上一阶段embedding筛选出来的是30个，需要rerank

上上次课的word_similarity，我这运行报错，这个怎么解决？
ImportError: cannotimport name 'triu' from 'scipy.linalg.special matrices’


Q：Query 几种改写类型， 先判断是否需要改写？
让大模型来进行识别是哪种？
几种如何结合起来，最佳实践是什么？
时效性等规则使用，联网并行处理的排序吗？


LLM prompt：
1）判断是否需要改写


JSON示例
{
   '是否要改写': Yes,
   '改写的类型': ...
   '改写的内容': ...
}


Q：能否不用rerank模型，直接用规则定义？就是RAG的有哪些步骤优化，可以少用些token？主要考虑成本
可以，rerank是可选


Q：对于rag数据库，如果发现了冲突或者错误信息，如何去操作呢？之前增和查都学了，但是删和改怎么做呢？
1）冲突检测
2）人工来做决策 => 假设人的指令是 以第1个为准 




Q：为什么呢？量大的时候不是更容易回答准确么（答案都放入上下文了）


Q：topK,k不是可以指定吗？如果把K
topK,k不是可以指定吗？如果把K设小，还需要rerank吗？
K小的话，不需要rerank

Q：对长文本进行RAG知识库检索， 是对chunk内容进行doc2query/small2big之类的优化操作？
doc2query/small2big 是优化操作的一种；



Q：是用的python3.11版本吗？
是的


Q：用sql做召回、topk、rerank
解释的话：我认为“召回”相当于一条查询，TOPK相当于limit,rerank相当于将结果再进行order by


Q：老师，你可以告诉我们你在实际项目中，用到哪些提高召回率的方法比较多吗？
1）路由
将知识库分类
2）关键词、向量化 


Q：GraphRAG 中的 Graph 只是一种数据索引或者说关系图构建模式吧，构建的时候知识超过了LLM的上下文限制怎么办呢 一本书可能很长
Step1，index 建立entity和community的过程，
Step2，推理回答



Q：Embedding后的向量源于原文件的上下文，如果删掉某些知识点，会不会破坏知识库里面知识点之间的关联关系或信息之间的依赖关系？
chunk => embedding

Q：如果我用API调用模型，我上传的知识是不是就到公开平台了

Q：这个准确率越高是不是就是模型的幻觉越低？可以这么理解吗？


Q：加不了课的话，可以给一些推荐的论文书籍或者视频也行呢
GraphRAG github里面说明
回来找找，放到 class.wucai.com 的打卡

Q：graphrag对于多文本的召回率提升大吗
会好一些

embadding里K大的的化，好像也没有必要rerank了。所以K+rerank是不是了回答为更准确呢（比如K大了会召回无关知识，会答错，所以用rerank

优化chunk内容的准确率越高是不是就是模型的幻觉越低？还是说会有召回速度的提升？
