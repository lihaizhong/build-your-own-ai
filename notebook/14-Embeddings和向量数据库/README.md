# Embeddings和向量数据库

西雅图酒店数据集： https://github.com/susanli2016/Machine-
Learning-with-Python/blob/master/Seattle_Hotels.csv

计算当前酒店特征向量与整个酒店特征矩阵的余弦相似度，取相似度最大的 Top-k 个

## 余弦相似度

- 通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
- 判断两个向量大致方向是否相同。
    - 方向相同时，余弦相似度为 1；
    - 两个向量夹角为 90% 时，余弦相似度的值为 0；
    - 方向完全相反时，余弦相似度的值为 -1。
- 两个向量之间夹角的余弦值为 [-1, 1]

## 什么是 N-Gram（N 元语法）

- 基于一个假设：第 n 个词出现与前 n - 1 个词相关，而与其他任何词不相关
- `N = 1` 时为 unigram，`N = 2` 时为 bigram，`N = 3` 时为 trigram
- N-Gram 指的是给定一段文本，其中的 N 个 item 的序列
    - 比如，文本 `A B C D E`，对应的 Bi-Gram 为 `A B`，`B C`，`C D`，`D E`
- 当一阶特征不够用时，可以用 N-Gram 作为新的特征。
    - 比如，在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征，采用 N-Gram => 可以理解是相邻两个关键词的特征组合。

## CountVectorizer

- 将文本中的词语转换为词频矩阵
- fit + transform：计算各个词语出现的次数
- get_feature_names()：可获得所有文本的关键词
- toarray()：查看词频矩阵的结果。

## TF-IDF

TF: Term Frequency，词频

- 一个单词的重要性和它在文档中出现的次数呈正比。

IDF: Inverse Document Frequency，逆向文档频率

- 一个单词在文档中的区分度。这个单词出现的文档数越少，区分度越大，IDF越大。

## 基于内容的推荐设计

1. 对酒店描述进行特征提取
    - N-Gram，提取 N 个连续字的集合，作为特征
    - TF-IDF，按照（min_df, max_df）提取关键词，并生成 TF-IDF矩阵
2. 计算酒店之间的相似度矩阵
    - 余弦相似度
3. 对于指定的酒店，选择相似度最大的 Top-K 个酒店进行输出

## Embedding

- 一种降维方式，将不同特征转换为维度相同的向量
- 离线变量转换成 one-hot => 维度非常高，可以将它转换为固定 size 的 embedding 向量
- 任何物体都可以将它转换为向量的形式，从 Trait #1 - #N
- 向量之间可以使用相似度进行计算
- 当我们进行推荐的时候，可以选择相似度最大的


