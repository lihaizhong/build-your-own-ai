{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968723a0",
   "metadata": {},
   "source": [
    "# DeepSeek-R1-7B æ¨¡å‹æ¨ç†ä»£ç è¯¦è§£\n",
    "\n",
    "## æ–‡ä»¶æ¦‚è¿°\n",
    "è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ **DeepSeek-R1-Distill-Qwen-7B** æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„LLMæ¨ç†ç¤ºä¾‹ï¼Œæ¶µç›–ä»æ¨¡å‹åŠ è½½åˆ°ç”Ÿæˆè¾“å‡ºçš„å…¨è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c2472",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ å¯¼å…¥å’ŒåŠ è½½æ¨¡å‹\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»ModelScopeå¯¼å…¥å¿…è¦çš„ç±»å¹¶åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/autodl-tmp/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"  # auto\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33e81e",
   "metadata": {},
   "source": [
    "### ä»£ç è¯´æ˜\n",
    "\n",
    "| å‚æ•° | è¯´æ˜ |\n",
    "|------|------|\n",
    "| `AutoModelForCausalLM` | å› æœè¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ |\n",
    "| `AutoTokenizer` | åˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºtoken ID |\n",
    "| `torch_dtype=\"auto\"` | è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆfloat16æˆ–float32ï¼‰ |\n",
    "| `device_map=\"cuda\"` | å°†æ¨¡å‹éƒ¨ç½²åˆ°GPUä¸Šè¿è¡Œ |\n",
    "\n",
    "### å…³é”®æ¦‚å¿µ\n",
    "\n",
    "- **å› æœè¯­è¨€å»ºæ¨¡**ï¼šåªèƒ½çœ‹åˆ°å‰é¢çš„tokenï¼Œç”¨äºç”Ÿæˆå¼ä»»åŠ¡\n",
    "- **åˆ†è¯å™¨**ï¼šå°†æ–‡æœ¬å’Œtoken IDç›¸äº’è½¬æ¢çš„å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951eb62",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ æ„å»ºèŠå¤©æ¶ˆæ¯\n",
    "\n",
    "æŒ‰ç…§ OpenAI èŠå¤©æ ¼å¼æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œå®šä¹‰ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"å¸®æˆ‘å†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾æ³•\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "print(\"æ¶ˆæ¯æ ¼å¼ï¼š\")\n",
    "for msg in messages:\n",
    "    print(f\"  {msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baec8dc",
   "metadata": {},
   "source": [
    "### æ¶ˆæ¯æ ¼å¼çš„ä¼˜åŠ¿\n",
    "\n",
    "- âœ… æ¸…æ™°åŒºåˆ†ä¸åŒè§’è‰²çš„å†…å®¹\n",
    "- âœ… ä¾¿äºå¤šè½®å¯¹è¯ç®¡ç†\n",
    "- âœ… ç¬¦åˆä¸»æµLLMçš„è¾“å…¥æ ¼å¼\n",
    "- âœ… æ˜“äºæ‰©å±•å’Œç»´æŠ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ef231",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ åº”ç”¨èŠå¤©æ¨¡æ¿\n",
    "\n",
    "å°†ç»“æ„åŒ–æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"æ ¼å¼åŒ–åçš„æ–‡æœ¬ï¼š\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af46112",
   "metadata": {},
   "source": [
    "### å‚æ•°è¯´æ˜\n",
    "\n",
    "| å‚æ•° | è¯´æ˜ |\n",
    "|------|------|\n",
    "| `messages` | è¾“å…¥çš„æ¶ˆæ¯åˆ—è¡¨ |\n",
    "| `tokenize=False` | è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆè€Œétoken IDï¼‰ |\n",
    "| `add_generation_prompt=True` | æ·»åŠ ç”Ÿæˆæç¤ºç¬¦ï¼Œå‘Šè¯‰æ¨¡å‹å¼€å§‹ç”Ÿæˆæ–‡æœ¬ |\n",
    "\n",
    "### è½¬æ¢æ•ˆæœç¤ºä¾‹\n",
    "\n",
    "```\n",
    "åŸå§‹æ ¼å¼ï¼š\n",
    "[{\"role\": \"system\", ...}, {\"role\": \"user\", ...}]\n",
    "\n",
    "è½¬æ¢åï¼š\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "å¸®æˆ‘å†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾æ³•\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86239301",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ å‡†å¤‡æ¨¡å‹è¾“å…¥\n",
    "\n",
    "å°†æ–‡æœ¬åˆ†è¯å¹¶è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå‡†å¤‡è¾“å…¥æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de991af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"æ¨¡å‹è¾“å…¥ç»“æ„ï¼š\")\n",
    "print(f\"  Input IDs shape: {model_inputs['input_ids'].shape}\")\n",
    "print(f\"  Attention Mask shape: {model_inputs['attention_mask'].shape}\")\n",
    "print(f\"\\nå‰10ä¸ªtoken ID: {model_inputs['input_ids'][0][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8912e39",
   "metadata": {},
   "source": [
    "### ä»£ç è¯´æ˜\n",
    "\n",
    "- `tokenizer([text], ...)`ï¼šå¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
    "  - `[text]` ä»¥åˆ—è¡¨å½¢å¼è¾“å…¥ï¼Œæ”¯æŒæ‰¹å¤„ç†\n",
    "  - è¿”å›token IDåºåˆ—\n",
    "- `return_tensors=\"pt\"`ï¼šè¿”å›PyTorchå¼ é‡æ ¼å¼\n",
    "- `.to(model.device)`ï¼šå°†å¼ é‡ç§»åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰\n",
    "\n",
    "### è¾“å‡ºç»“æ„\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': tensor([[token_id_1, token_id_2, ...]]),\n",
    "    'attention_mask': tensor([[1, 1, 1, ...]])  # æŒ‡ç¤ºå“ªäº›ä½ç½®æ˜¯çœŸå®token\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb8e53",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "æ‰§è¡Œè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å¼€å§‹ç”Ÿæˆæ–‡æœ¬...ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰\\n\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2000\n",
    ")\n",
    "\n",
    "print(f\"ç”Ÿæˆå®Œæˆï¼\")\n",
    "print(f\"ç”Ÿæˆçš„tokenæ€»æ•°: {generated_ids.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751cd1f",
   "metadata": {},
   "source": [
    "### ç”ŸæˆåŸç†\n",
    "\n",
    "1. ğŸ”¹ è¾“å…¥åˆå§‹tokenåºåˆ—\n",
    "2. ğŸ”¹ æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„token\n",
    "3. ğŸ”¹ å°†é¢„æµ‹çš„tokenåŠ å…¥åºåˆ—\n",
    "4. ğŸ”¹ é‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°ç”Ÿæˆ[EOS]ï¼ˆç»“æŸï¼‰tokenæˆ–è¾¾åˆ°max_new_tokens\n",
    "\n",
    "### å‚æ•°è¯´æ˜\n",
    "\n",
    "| å‚æ•° | è¯´æ˜ |\n",
    "|------|------|\n",
    "| `**model_inputs` | è§£åŒ…è¾“å…¥å­—å…¸ä½œä¸ºå…³é”®å­—å‚æ•° |\n",
    "| `max_new_tokens=2000` | é™åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•° |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf9e54",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ æå–ç”Ÿæˆéƒ¨åˆ†\n",
    "\n",
    "ç”Ÿæˆçš„output_idsåŒ…å«è¾“å…¥éƒ¨åˆ†å’Œç”Ÿæˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬åªéœ€è¦ç”Ÿæˆçš„æ–°tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿ç•™ç”Ÿæˆéƒ¨åˆ†ï¼ˆç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "print(f\"è¾“å…¥tokenæ•°: {len(model_inputs.input_ids[0])}\")\n",
    "print(f\"ç”Ÿæˆtokenæ•°: {len(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9590dfb",
   "metadata": {},
   "source": [
    "### æå–é€»è¾‘ç¤ºä¾‹\n",
    "\n",
    "```\n",
    "output_ids:      [token_1, token_2, ..., token_100, token_101, ...]\n",
    "                  â””â”€è¾“å…¥éƒ¨åˆ†â”€â”˜      â””â”€â”€â”€â”€ç”Ÿæˆéƒ¨åˆ†â”€â”€â”€â”€â”˜\n",
    "len(input_ids) = 100\n",
    "\n",
    "ç»“æœï¼š[token_101, token_102, ...]\n",
    "```\n",
    "\n",
    "ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1ff81",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ è§£ç å’Œè¾“å‡º\n",
    "\n",
    "å°†token IDåºåˆ—è½¬æ¢å›æ–‡æœ¬å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"æ¨¡å‹ç”Ÿæˆç»“æœï¼š\")\n",
    "print(\"=\"*60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665225b2",
   "metadata": {},
   "source": [
    "### ä»£ç è¯´æ˜\n",
    "\n",
    "- `batch_decode()`ï¼šå°†token IDåºåˆ—è½¬æ¢å›æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "- `skip_special_tokens=True`ï¼šå¿½ç•¥ç‰¹æ®Štokenï¼ˆå¦‚[CLS]ã€[SEP]ç­‰ï¼‰\n",
    "- `[0]`ï¼šå–æ‰¹æ¬¡ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æœ\n",
    "\n",
    "### è¾“å‡ºç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba23ed3",
   "metadata": {},
   "source": [
    "## ğŸ“Š å®Œæ•´æµç¨‹å›¾\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  è¾“å…¥æ–‡æœ¬   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [æ¨¡æ¿åº”ç”¨]     â”‚  â†’ æ ¼å¼åŒ–ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [åˆ†è¯]         â”‚  â†’ è½¬æ¢ä¸ºtoken IDåºåˆ—\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [ç§»åˆ°GPU]      â”‚  â†’ å‡†å¤‡æ¨¡å‹è¾“å…¥\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [ç”Ÿæˆ]         â”‚  â†’ é€tokenè‡ªå›å½’ç”Ÿæˆ\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [æå–æ–°token]  â”‚  â†’ ç§»é™¤è¾“å…¥éƒ¨åˆ†\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [è§£ç ]         â”‚  â†’ token ID è½¬å›æ–‡æœ¬\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  è¾“å‡ºç»“æœ   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d2461",
   "metadata": {},
   "source": [
    "## ğŸ”‘ å…³é”®æ¦‚å¿µæ€»ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¯´æ˜ | ä½œç”¨ |\n",
    "|------|------|------|\n",
    "| **AutoModelForCausalLM** | å› æœè¯­è¨€æ¨¡å‹ | ç”¨äºæ–‡æœ¬ç”Ÿæˆ |\n",
    "| **Tokenizer** | æ–‡æœ¬-tokenè½¬æ¢å·¥å…· | ç¼–ç å’Œè§£ç  |\n",
    "| **Chat Template** | èŠå¤©æ ¼å¼æ¨¡æ¿ | ç»Ÿä¸€è¾“å…¥æ ¼å¼ |\n",
    "| **Device Mapping** | è®¾å¤‡æ˜ å°„ | æŒ‡å®šè®¡ç®—è®¾å¤‡ |\n",
    "| **Generation** | æ–‡æœ¬ç”Ÿæˆ | è‡ªå›å½’ç”Ÿæˆ |\n",
    "| **Batch Processing** | æ‰¹å¤„ç† | æé«˜æ•ˆç‡ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9db908",
   "metadata": {},
   "source": [
    "## â“ å¸¸è§é—®é¢˜\n",
    "\n",
    "### Q1: ä¸ºä»€ä¹ˆè¦ä½¿ç”¨chat_templateï¼Ÿ\n",
    "**A:** ä¸åŒçš„æ¨¡å‹å¯èƒ½æœ‰ä¸åŒçš„è¾“å…¥æ ¼å¼ã€‚ChatTemplateæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢ã€‚\n",
    "\n",
    "### Q2: max_new_tokensçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "**A:** é™åˆ¶ç”Ÿæˆçš„é•¿åº¦ï¼Œé˜²æ­¢æ— é™ç”Ÿæˆæˆ–èµ„æºæµªè´¹ã€‚tokenæ•°è¶Šå¤šï¼Œç”Ÿæˆè€—æ—¶å’Œæ˜¾å­˜æ¶ˆè€—è¶Šå¤§ã€‚\n",
    "\n",
    "### Q3: ç”Ÿæˆé€Ÿåº¦ä¸ä»€ä¹ˆå› ç´ ç›¸å…³ï¼Ÿ\n",
    "**A:**\n",
    "- æ¨¡å‹å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰\n",
    "- ç¡¬ä»¶é…ç½®ï¼ˆGPUæ˜¾å­˜å’Œæ€§èƒ½ï¼‰\n",
    "- ç”Ÿæˆé•¿åº¦ï¼ˆmax_new_tokensï¼‰\n",
    "- ä½¿ç”¨çš„æ•°æ®ç±»å‹ï¼ˆfloat32 vs float16ï¼‰\n",
    "\n",
    "### Q4: å¦‚ä½•åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Ÿ\n",
    "**A:**\n",
    "- ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼ˆå¦‚int8ã€int4ï¼‰\n",
    "- ä½¿ç”¨GPUè€ŒéCPU\n",
    "- æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚\n",
    "- ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆKV cacheï¼‰\n",
    "- ä½¿ç”¨æµå¼ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb2cdc",
   "metadata": {},
   "source": [
    "## ğŸš€ æ‰©å±•åº”ç”¨\n",
    "\n",
    "### æ‰¹é‡ç”Ÿæˆç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00813d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡ç”Ÿæˆå¤šä¸ªè¯·æ±‚\n",
    "prompts = [\n",
    "    \"å¸®æˆ‘å†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾æ³•\",\n",
    "    \"è§£é‡Šä»€ä¹ˆæ˜¯OOP\",\n",
    "    \"å¦‚ä½•ä¼˜åŒ–Pythonä»£ç \"\n",
    "]\n",
    "\n",
    "messages_list = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "print(f\"å·²å‡†å¤‡ {len(messages_list)} ä¸ªè¯·æ±‚\")\n",
    "for i, msg in enumerate(messages_list, 1):\n",
    "    print(f\"  {i}. {msg[-1]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f3130",
   "metadata": {},
   "source": [
    "### æµå¼ç”Ÿæˆç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨TextIteratorStreamerå®ç°æµå¼è¾“å‡º\n",
    "# è¿™æ ·å¯ä»¥å®æ—¶æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬ï¼Œè€Œä¸ç”¨ç­‰åˆ°å…¨éƒ¨ç”Ÿæˆå®Œæˆ\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# streamer = TextIteratorStreamer(tokenizer)\n",
    "# generation_kwargs = dict(**model_inputs, streamer=streamer, max_new_tokens=2000)\n",
    "\n",
    "# thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "# thread.start()\n",
    "\n",
    "# for text in streamer:\n",
    "#     print(text, end='', flush=True)\n",
    "\n",
    "print(\"æµå¼ç”Ÿæˆå®ç°ç¤ºä¾‹ï¼ˆå·²æ³¨é‡Šï¼Œå¯æ ¹æ®éœ€è¦å¯ç”¨ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571afb6",
   "metadata": {},
   "source": [
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "è¿™ä¸ªnotebookè¯¦ç»†å±•ç¤ºäº†ï¼š\n",
    "\n",
    "1. âœ… **æ¨¡å‹åŠ è½½** - ä»ModelScopeåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "2. âœ… **æ¶ˆæ¯æ„å»º** - æŒ‰ç…§æ ‡å‡†æ ¼å¼æ„å»ºèŠå¤©æ¶ˆæ¯\n",
    "3. âœ… **æ ¼å¼è½¬æ¢** - ä½¿ç”¨ChatTemplateç»Ÿä¸€è¾“å…¥æ ¼å¼\n",
    "4. âœ… **åˆ†è¯å¤„ç†** - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„å¼ é‡\n",
    "5. âœ… **æ–‡æœ¬ç”Ÿæˆ** - è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬\n",
    "6. âœ… **ç»“æœå¤„ç†** - æå–å’Œè§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
    "\n",
    "é€šè¿‡ç†è§£è¿™ä¸ªæµç¨‹ï¼Œä½ å¯ä»¥ï¼šn- ä½¿ç”¨ä»»ä½•Hugging Faceå…¼å®¹çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ\n",
    "- æ„å»ºè‡ªå·±çš„èŠå¤©åº”ç”¨\n",
    "- ä¼˜åŒ–æ¨ç†æ€§èƒ½\n",
    "- å®ç°æ›´å¤æ‚çš„NLPä»»åŠ¡\n",
    "\n",
    "---\n",
    "\n",
    "*æ–‡æ¡£æœ€åæ›´æ–°ï¼š2026å¹´2æœˆ4æ—¥*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
