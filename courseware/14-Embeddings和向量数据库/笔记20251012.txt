Q：老师，使用wucai进行打卡的时候一直让登录，但是跳转到网页登录后wucai好像不能识别已经登陆了，一直卡在登录界面

更新下wucai版本
npm install -g @wucai/wucai-code 
然后再输入wucai进入

Q：老师，我用了很多种方式，然后最好成绩都是在497到520之间了
特征工程，可以和AI交互一下，比较好的特征工程的方法

Q：我想了解一下。如果公司有不同的业务是需要部署多个对应的模型吗
底层的大模型可以部署一个，共用；
通过多张显卡，vllm 提高并发量
qwen-32b
qwen3-coder-30B-A3B

Q:老师，在vibe coding时感觉不能随心所欲的让LLM生成结果，有种LLM脱离掌控的感觉，这是因为prompt有问题？还是LLM本身问题？

让LLM在执行前，说下他的思路，以及置信度（0-100）

Q：http://class.wucai.com/tasks,这个页面需要注册吗？
如果同步到wucai打卡，以及个人作业，是需要注册的

Q：wucai 后台使用的是那个模型？ Qwen3-coder 吗？
Qwen3-Coder-Plus

Q：modelscope上下载一个模型，本地运行下一直提示缺包，用pycharm安装上最后又提示这个，说是版本兼容问题，降级又会提示缺少接口，这种情况怎么弄


Q：那调参是否是对一个模型进行调参，会不会一个业务调参影响另一个业务
模型参数调整了，会LLM输出结果会有影响

停用词 = 常见的单词，没有明显特征的，the, a, an, I, he

1-gram, 2-gram, 3-gram
每个酒店的描述 = [x1, x2, ..., x3347] 上面的特征值
152家酒店，和 152家酒店之间，两两的相似度

Embeddings 是 “给东西写特征数字卡”，向量数据库是 “存卡还能快速找相似卡的柜”


Q：老师，这个152，还是没有理解，是1gram,2gram,3gram之和，代表什么意思呢
152家酒店
每家酒店，都有3347维度的特征（1gram 1000, 2gram 1000, 3gram 1347的维度）


Q：为什么都是3347维？每个酒店的描述长度不是不一样吗？
每个酒店desc 不一样，但是 1gram, 2gram, 3gram 是所有酒店可能的特征


Q：3千多个维度是怎么算的？
1gram的特征维度 + 2gram的特征维度  + 3gram的特征维度 

Q：再讲一下Ngram的意思
特征工程，

如果我们还是用ngram这种方式，特征维度会非常多
对于上万个特征，计算和存储都很会很大 => 很稀疏（很多位置，特征值都为0）

3347 特征维度很稀疏（很多0）=> embedding (vec_size=50)

Q：这是相当于拟合了一个神经网络吗？
是的，


model = word2vec.Word2Vec(sentences, vector_size=100, window=3, min_count=1)


为什么要选择猪八戒，孙悟空这些单词来算邻居？这样是为了达到什么目的


fahsflks:21:13:14
老师,
工具怎么判断这100个维度都是什么?


Q：怎么选择是100还是300好
vec_size 比较小 => 计算速度快，精度一般，对于数据量不大的，
vec_size 比较大 =>计算速度慢，精度比较高

Q：邻居有什么作用呢
学习这个 西游记中的单词的 word embedding，没有人工标注
无监督学习，


Q：孙悟空的向量 是啥意思 ？ 怎么这么多值？？
人工指定 vector_size=100


Q: 用N元语法计算的时候，1元、2元、3元，，，N元，会在计算一次相似度的时候，使用多次N元语法么？比如，同时用了2元和3元分别做相似度的计算？
一个酒店 （1元特征，2元特征，3元特征，一共3347维）


Q：邻居知道什么意思了，如何通过邻居进行压缩的？？邻居出现的概率？？
邻居是一种标注（这里是无监督，不是人的标注）


老师，回到上面说的酒店的相似度对比那里，是矩阵里的每个单元格，都


我喜欢吃苹果，苹果的令居到底是我喜欢吃，还是水果？
=> 我/喜欢/吃/苹果/和/香蕉
我/喜欢/吃/苹果/和/橘子

苹果的邻居，如果windows=2
喜欢2，吃2，和1，香蕉1，橘子1
（1万个单词： x1, x2, ..., x10000）
(1, 1, 0, 0, 0, ..., 2)


把所有的邻居加起来当做这个词的一个特征，是这样吗


Q：老师，回到上面说的酒店的相似度对比那里，是矩阵里的每个单元格，都用3347维来对比计算吗？
对的，每个酒店都是3347维


可以简单，模拟一下计算过程吗？



Q：陈博士，无监督学习时候，对于已经有学习过的单词它会参考预训练的数据吗？
word2vec 本身就是可以从0 开始训练


Q：记住了什么呢，离开了西游记是不是就没有用了

https://huggingface.co/spaces/mteb/leaderboard

编写Python，读取这篇论文的内容，写入到 .md
什么是词袋模型，帮我讲解
帮我整理一篇小白友好的论文笔记，可以用svg表明其中的流程，制作 report.html

Q：如果wucai没有work指令，需要更新wucai
npm install -g @wucai/wucai-code

pip install gensim -i https://pypi.tuna.tsinghua.edu.cn/simple

Q：embedding模型和一般的大模型有啥区别？
LLM 是一个问答神经网络，Query =》 Response 

embedding模型，统计模型，计算单词的特征（描述了这个文本的特征工程）

LLM：回答问题
embedding：相似度检索，查询 query 和 知识的相似度

query （原始文本）=> embedding向量
# index = faiss数据库
distances, retrieved_ids = index.search(query_vector, k)

--- 排名 1 (相似度得分/距离: 0.3222) ---
ID: 2
原始文本: 对于在线购买的迪士尼门票，如果需要退票，必须在票面日期前48小时通过原购买渠道提交申请，并可能收取手续费。
元数据: {'source': 'online_policy.html', 'category': '退票政策', 'author': 'E-commerceTeam'}

--- 排名 2 (相似度得分/距离: 0.3312) ---
ID: 0
原始文本: 迪士尼乐园的门票一经售出，原则上不予退换。但在特殊情况下，如恶劣天气导致园区关闭，可在官方指引下进行改期或退款。
元数据: {'source': 'official_faq_v1.pdf', 'category': '退票政策', 'author': 'Admin'}

--- 排名 3 (相似度得分/距离: 1.0135) ---
ID: 1
原始文本: 购买“奇妙年卡”的用户，可以享受一年内多次入园的特权，并且在餐饮和购物时有折扣。


将faiss数据库进行保存到本地，方便后续进行读取



Q：英文一个词能几个意思的，比如like可以是喜欢或者像，向量计算这么确定是哪个意思，是一个词对应几个向量吗


Q：在我们通过网络问大模型时，大模型会不会用到向量数据库？固定的大模型有没有固定版本的向量数据库？
上下文，kimi 来检索信息；
向量数据库是给大模型做数据筛选；在RAG系统中用

Q：这里面的元数据是怎么得到的？
可以从文件属性中获取

Q：一般企业应用中，都会保存faiss数据库到本地，然后持续存储公司业务相关的向量，还是会使用在线的，比如SaaS
SaaS是追求快速上线
在本地进行管理的多一些

Q：一个字在向量数据库中只有一种表达方法吗
query => embedding 是确定

Q：向量数据库应用场景是什么
企业智能客服，私有化知识的问题


Q：为啥要存数据库？是自己不断积攒专属于自己的数据库吗？
对的


Q：没有OpenAPI的APIKey吧，不有运行
用到了OpenAI工具库，但不是OpenAPI KEY，用的是 DASHSCOPE_API_KEY


Q：向量是AI自动分配的，还是人类规定的
向量是 embedding模型计算得出来


Q：后面完全可以下载一个Bert模型用sentence transformer加载吧，或者使用qwenembedding3这种专门的向量模型
是的！


Q：Embeddings和向量数据库这部分内容的最终目的是什么？搭建自己的数据库，然后快速检索与自己问题相关的知识吗？
目前就是做知识检索 和 智能问题 


Q：如果不断累积更新数据库时，后期发现前期选择的向量维度不够怎么办？支持动态调整维度么？
都需要重新存储

Q：embedding运行的时候是逐词进行的吗？对同一个数据集可以分阶段进行embedding吗？比如一个小说我分两次去embedding
切分chunk => 进行embedding


Q：通过向量数据库查询问题与向大模型提问有什么区别？怎么把向量数据库与大模型结合起来使用？
Step1，用向量数据库 检索相似的知识
Step2，把检索到的知识，作为上下文 + query => LLM


Q：embedding做知识检索会用到大模型吗，还是说这个数据库相当于一个小的大模型
embedding模型 和 LLM不一样，embedding模型能力 < LLM



Q：这里的迪斯尼的实例不需要分词了是吗？西游记的实例我们先做的分词再向量化，这里是整个一句话向量化的吗？什么区别呢？
API


Q：存入向量数据库中的数据，可以反向找回原始数据


Q：如果embedding完后，企业又有新的知识进来，也要重新embedding吗
对新的知识进行embedding

Q：PDF、Word、Excel也是用文本Embedding模型切分吗？
可以，不过Excel用可以用 SQL数据库


Q：redis缓存的query和answer，那是不是query要完全一样，才会触发从redis里取answer。
对的


Q：PDF、Word、Excel也是用文本Embedding模型切分吗？还是只能TXT
PDF, word ,excel 要先转化成文本 => 文本embedding


我们给deepseek 这种大模型提问的时候， 大模型最终也是检索向量数据库么？


Q：把一个有文字，表格，图片的报告给embedding工具，可以实现么？
text embedding工具 接受的是text => 输出的text embedding
pdf => MinerU => embedding


Q：老师 怎么增量啊，之前的不是都训练好了，不是需要跟新的知识一起再重新训练啊
增量，是把新的知识 放进去即可


Q：排名里的embedding是什么, 和自己用word2ve 不一样吗
embedding是别人预训练好的，可以直接使用的
word2vec是一个简化版的embedding训练工具

