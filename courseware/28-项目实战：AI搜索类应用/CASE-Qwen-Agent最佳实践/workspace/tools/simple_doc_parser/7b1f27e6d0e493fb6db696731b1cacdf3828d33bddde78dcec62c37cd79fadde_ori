[
  {
    "page_num": 1,
    "content": [
      {
        "text": "QWEN TECHNICAL REPORT\n",
        "font-size": 17,
        "token": 8
      },
      {
        "text": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu\nHan, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou,\nXiaohuan Zhou, Tianhang Zhu.\n",
        "font-size": 10,
        "token": 209
      },
      {
        "text": "Qwen Team, Alibaba Group∗\n",
        "font-size": 10,
        "token": 8
      },
      {
        "text": "ABSTRACT\n",
        "font-size": 12,
        "token": 3
      },
      {
        "text": "Large language models (LLMs) have revolutionized the field of artificial intelli-\ngence, enabling natural language processing tasks that were previously thought\nto be exclusive to humans. In this work, we introduce QWEN1, the first install-\nment of our large language model series. QWEN is a comprehensive language\nmodel series that encompasses distinct models with varying parameter counts. It\nincludes QWEN, the base pretrained language models, and QWEN-CHAT, the chat\nmodels finetuned with human alignment techniques. The base language models\nconsistently demonstrate superior performance across a multitude of downstream\ntasks, and the chat models, particularly those trained using Reinforcement Learning\nfrom Human Feedback (RLHF), are highly competitive. The chat models pos-\nsess advanced tool-use and planning capabilities for creating agent applications,\nshowcasing impressive performance even when compared to bigger models on\ncomplex tasks like utilizing a code interpreter. Furthermore, we have developed\ncoding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as\nmathematics-focused models, MATH-QWEN-CHAT, which are built upon base\nlanguage models. These models demonstrate significantly improved performance\nin comparison with open-source models, and slightly fall behind the proprietary\nmodels.\n",
        "font-size": 7,
        "token": 262
      },
      {
        "text": "∗Authors are ordered alphabetically by the last name. Correspondence to: ericzhou.zc@alibaba-inc.com.\n1QWEN is a moniker of Qianwen, which means “thousands of prompts” in Chinese. The pronunciation of\n“QWEN” can vary depending on the context and the individual speaking it. Here is one possible way to pronounce\nit: /kwEn/.\n",
        "font-size": 7,
        "token": 85
      },
      {
        "text": "1\n",
        "font-size": 10,
        "token": 2
      }
    ]
  },
  {
    "page_num": 2,
    "content": [
      {
        "text": "Contents\n",
        "font-size": 14,
        "token": 2
      },
      {
        "text": "1 Introduction\n 2 Pretraining\n 2.1 Data .\n .\n .\n .\n .\n 2.2 Tokenization .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n\t\n\t\n 2.3 Model\n .\n .\n .\n",
        "font-size": 10,
        "token": 47
      },
      {
        "text": ".\n.\n.\n\t\n\t\n2.3.1 Architecture .\n.\n2.3.2 Context Length Extension\t\n",
        "font-size": 10,
        "token": 21
      },
      {
        "text": ".\n.\n",
        "font-size": 10,
        "token": 2
      },
      {
        "text": ".\n.\n .\n .\n 2.4 Training .\n .\n .\n .\n .\n .\n .\n .\n 2.5 Experimental Results .\n .\n .\n\t\n\t\n 3 Alignment\n 3\n 4\n 4\n 6\n",
        "font-size": 10,
        "token": 43
      },
      {
        "text": "6\n6\n7\n 8\n 8\n 9\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": "3.1 Supervised Finetuning .\n.\n.\n",
        "font-size": 10,
        "token": 11
      },
      {
        "text": "3.1.1 Data .\n.\n3.1.2 Training .\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": ".\n.\n",
        "font-size": 10,
        "token": 2
      },
      {
        "text": ".\n.\n",
        "font-size": 10,
        "token": 2
      },
      {
        "text": ".\n.\n .\n",
        "font-size": 10,
        "token": 3
      },
      {
        "text": "\t\n\t\n.\n\t\n. .\n",
        "font-size": 10,
        "token": 5
      },
      {
        "text": "9\n9\n10\n",
        "font-size": 10,
        "token": 7
      },
      {
        "text": "3.2 Reinforcement Learning from Human Feedback\t\n\t\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "10\n3.2.1 Reward Model\n10\n3.2.2 Reinforcement Learning\t11\n . . .\n .\n 3.3 Automatic and Human Evaluation of Aligned Models\t11\n 3.4 Tool Use, Code Interpreter, and Agent\n\t\n 4 CODE-QWEN: Specialized Model for Coding\n 4.1 Code Pretraining .\n .\n .\n .\n\t\n 4.2 Code Supervised Fine-Tuning\t\n 4.3 Evaluation .\n .\n .\n .\n .\n .\n .\n .\n\t\n 5 MATH-QWEN: Specialized Model for Mathematics Reasoning\n 5.1 Training .\n .\n 5.2 Evaluation .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n\t\n\t\n 6 Related Work\n 6.1 Large Language Models\n\t\n 6.2 Alignment .\n .\n .\n .\n .\n .\n 6.3 Tool Use and Agents\n 6.4 LLM for Coding .\n .\n .\n .\n .\n 6.5 LLM for Mathematics .\n .\n .\n .\n .\n\t\n\t21\n\t21\n\t\n 7 Conclusion\n A Appendix\n A.1 More Training Details .\n .\n .\n",
        "font-size": 10,
        "token": 237
      },
      {
        "text": "A.1.1 Data Format for QWEN-CHAT\t\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "\t\n\t\n\t\n . . .\n A.2 Evaluation .\n .\n .\n",
        "font-size": 10,
        "token": 11
      },
      {
        "text": "\t\nA.2.1 Automatic Evaluation\t\nA.2.2 Human Evaluation\t\n .\n .\n .\n .\n .\n .\n A.3 Analysis of Code Interpreter\n\t\n 2\n 13\n 16\n 16\n 17\n 17\n 17\n 17\n 20\n 20\n 20\n 20\n 22\n 22\n 35\n",
        "font-size": 10,
        "token": 90
      },
      {
        "text": "35\n35\n",
        "font-size": 10,
        "token": 6
      },
      {
        "text": "35\n35\n40\n 58\n",
        "font-size": 10,
        "token": 13
      }
    ]
  },
  {
    "page_num": 3,
    "content": [
      {
        "text": "1\n INTRODUCTION\n",
        "font-size": 12,
        "token": 5
      },
      {
        "text": "Large language models (LLMs) (Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2020; Brown\net al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Thoppilan et al., 2022; Touvron\net al., 2023a;b) have revolutionized the field of artificial intelligence (AI) by providing a powerful\nfoundation for complex reasoning and problem-solving tasks. These models have the ability to\ncompress vast knowledge into neural networks, making them incredibly versatile agents. With a\nchat interface, LLMs can perform tasks that were previously thought to be the exclusive domain of\nhumans, especially those involving creativity and expertise (OpenAI, 2022; Ouyang et al., 2022; Anil\net al., 2023; Google, 2023; Anthropic, 2023a;b). They can engage in natural language conversations\nwith humans, answering questions, providing information, and even generating creative content such\nas stories, poems, and music. This has led to the development of a wide range of applications, from\nchatbots and virtual assistants to language translation and summarization tools.\n",
        "font-size": 10,
        "token": 301
      },
      {
        "text": "LLMs are not just limited to language tasks. They can also function as a generalist agent (Reed et al.,\n2022; Bai et al., 2022a; Wang et al., 2023a; AutoGPT, 2023; Hong et al., 2023), collaborating with\nexternal systems, tools, and models to achieve the objectives set by humans. For example, LLMs\ncan understand multimodal instructions (OpenAI, 2023; Bai et al., 2023; Liu et al., 2023a; Ye et al.,\n2023; Dai et al., 2023; Peng et al., 2023b), execute code (Chen et al., 2021a; Zheng et al., 2023; Li\net al., 2023d), use tools (Schick et al., 2023; LangChain, Inc., 2023; AutoGPT, 2023), and more.\nThis opens up a whole new world of possibilities for AI applications, from autonomous vehicles and\nrobotics to healthcare and finance. As these models continue to evolve and improve, we can expect\nto see even more innovative and exciting applications in the years to come. Whether it’s helping us\nsolve complex problems, creating new forms of entertainment, or transforming the way we live and\nwork, LLMs are poised to play a central role in shaping the future of AI.\n",
        "font-size": 10,
        "token": 333
      },
      {
        "text": "Figure 1: Model Lineage of the Qwen Series. We have pretrained the language models, namely\nQWEN, on massive datasets containing trillions of tokens. We then use SFT and RLHF to align\nQWEN to human preference and thus we have QWEN-CHAT and specifically its improved version\nQWEN-CHAT-RLHF. Additionally, we also develop specialized models for coding and mathematics,\nsuch as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT based on QWEN with similar\ntechniques. Note that we previously released the multimodal LLM, QWEN-VL and QWEN-VL-\nCHAT (Bai et al., 2023), which are also based on our QWEN base models.\n",
        "font-size": 10,
        "token": 169
      },
      {
        "text": "Despite their impressive capabilities, LLMs are often criticized for their lack of reproducibility,\nsteerability, and accessibility to service providers. In this work, we are pleased to present and release\nthe initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase\nQianwen, which translates to “thousands of prompts” and conveys the notion of embracing a wide\nrange of inquiries. QWEN is a comprehensive language model series that encompasses distinct models\nwith varying parameter counts. The model series include the base pretrained language models, chat\nmodels finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement\nlearning with human feedback (RLHF), etc., as well as specialized models in coding and math. The\ndetails are outlined below:\n",
        "font-size": 8,
        "token": 175
      },
      {
        "text": "3\n",
        "font-size": 10,
        "token": 2
      }
    ]
  },
  {
    "page_num": 4,
    "content": [
      {
        "text": "1. The base language models, namely QWEN, have undergone extensive training using up to 3\ntrillion tokens of diverse texts and codes, encompassing a wide range of areas. These models\nhave consistently demonstrated superior performance across a multitude of downstream\ntasks, even when compared to their more significantly larger counterparts.\n",
        "font-size": 10,
        "token": 65
      },
      {
        "text": "2. The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task\nperforming, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that\nthe SFT models can achieve superior performance. Furthermore, we have trained reward\nmodels to mimic human preference and applied them in RLHF for chat models that can\nproduce responses preferred by humans. Through the human evaluation of a challenging test,\nwe find that QWEN-CHAT models trained with RLHF are highly competitive, still falling\nbehind GPT-4 on our benchmark.\n",
        "font-size": 10,
        "token": 121
      },
      {
        "text": "3. In addition, we present specialized models called CODE-QWEN, which includes CODE-\nQWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14B-\nCHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained\non extensive datasets of code and further fine-tuned to handle conversations related to\ncode generation, debugging, and interpretation. The results of experiments conducted on\nbenchmark datasets, such as HumanEval (Chen et al., 2021b), MBPP (Austin et al., 2021),\nand HumanEvalPack (Muennighoff et al., 2023), demonstrate the high level of proficiency\nof CODE-QWEN in code understanding and generation.\n",
        "font-size": 10,
        "token": 174
      },
      {
        "text": "4. This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle\nmathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-\nQWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins\nand are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021).\n",
        "font-size": 10,
        "token": 115
      },
      {
        "text": "5. Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile\nability to comprehend visual and language instructions. These models outperform the current\nopen-source vision-language models across various evaluation benchmarks and support text\nrecognition and visual grounding in both Chinese and English languages. Moreover, these\nmodels enable multi-image conversations and storytelling. Further details can be found\nin Bai et al. (2023).\n",
        "font-size": 10,
        "token": 97
      },
      {
        "text": "Now, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN\nand aligned chat models QWEN-CHAT2. This release aims at providing more comprehensive and\npowerful LLMs at developer- or application-friendly scales.\n",
        "font-size": 7,
        "token": 57
      },
      {
        "text": "The structure of this report is as follows: Section 2 describes our approach to pretraining and results\nof QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic\nevaluation and human evaluation. Additionally, this section describes details about our efforts in\nbuilding chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve\ninto specialized models of coding and math and their performance. Section 6 provides an overview\nof relevant related work, and Section 7 concludes this paper and points out our future work.\n",
        "font-size": 10,
        "token": 120
      },
      {
        "text": "2 PRETRAINING\n",
        "font-size": 12,
        "token": 5
      },
      {
        "text": "The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding\nof the world and its various complexities. This includes not only basic language capabilities but also\nadvanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the\ndata, the model design and scaling, as well as the comprehensive evaluation results on benchmark\ndatasets.\n 2.1 DATA\n",
        "font-size": 10,
        "token": 80
      },
      {
        "text": "The size of data has proven to be a crucial factor in developing a robust large language model,\nas highlighted in previous research (Hoffmann et al., 2022; Touvron et al., 2023b). To create an\neffective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range\n",
        "font-size": 10,
        "token": 74
      },
      {
        "text": "2GitHub: https://github.com/QwenLM/Qwen.\n 4\n",
        "font-size": 10,
        "token": 16
      }
    ]
  },
  {
    "page_num": 5,
    "content": [
      {
        "text": "Figure 2: Performance of GPT-4, GPT-3.5, the previous 13B SOTA, as well as QWEN-14B. We\ndemonstrate the results on 12 datasets covering multiple domains, including language understanding,\nknowledge, reasoning, etc. QWEN significantly outperforms the previous SOTA of similar model\nsizes, but still lag behind both GPT-3.5 and GPT-4.\n",
        "font-size": 8,
        "token": 98
      },
      {
        "text": "of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public\nweb documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a\nsignificant portion of the data being in English and Chinese.\n",
        "font-size": 10,
        "token": 53
      },
      {
        "text": "To ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing\nprocedure. For public web data, we extract text from HTML and use language identification tools to\ndetermine the language. To increase the diversity of our data, we employ deduplication techniques,\nincluding exact-match deduplication after normalization and fuzzy deduplication using MinHash\nand LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and\nmachine-learning-based methods. Specifically, we use multiple models to score the content, including\nlanguage models, text-quality scoring models, and models for identifying potentially offensive or\ninappropriate content. We also manually sample texts from various sources and review them to ensure\ntheir quality. To further enhance the quality of our data, we selectively up-sample data from certain\nsources, to ensure that our models are trained on a diverse range of high-quality content. Finally, we\nhave built a dataset of up to 3 trillion tokens.\n 5\n",
        "font-size": 10,
        "token": 204
      }
    ]
  },
  {
    "page_num": 6,
    "content": [
      {
        "text": "Figure 3: Encoding compression rates of different models. We randomly selected 1 million\ndocument corpora of each language to test and compare the encoding compression rates of different\nmodels (with XLM-R (Conneau et al., 2019), which supports 100 languages, as the base value 1, not\nshown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and\ncode, QWEN also achieves a high compression rate for many other languages (such as th, he, ar, ko,\nvi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr, etc.), equipping the model with strong scalability as well as\nhigh training and inference efficiency in these languages.\n",
        "font-size": 8,
        "token": 164
      },
      {
        "text": "2.2 TOKENIZATION\n",
        "font-size": 8,
        "token": 6
      },
      {
        "text": "The design of vocabulary significantly impacts the training efficiency and the downstream task\nperformance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following\nGPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain, 2022), and\nselect the vocabulary cl100k base as our starting point. To enhance the performance of our model on\nmultilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly\nused Chinese characters and words, as well as those in other languages. Also, following Touvron et al.\n(2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152K.\n",
        "font-size": 10,
        "token": 163
      },
      {
        "text": "The performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this\ncomparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneau\net al., 2019), LLaMA Touvron et al. (2023a), Baichuan Inc. (2023a), and InternLM InternLM Team\n(2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors\nin most languages. This implies that the cost of serving can be significantly reduced since a smaller\nnumber of tokens from QWEN can convey more information than its competitors. Furthermore, we\nhave conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does\nnot negatively impact the downstream performance of the pretrained model. Despite the increase\nin vocabulary size, our experiments have shown that QWEN maintains its performance levels in\ndownstream evaluation.\n",
        "font-size": 10,
        "token": 200
      },
      {
        "text": "2.3 MODEL\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "2.3.1 ARCHITECTURE\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "QWEN is designed using a modified version of the Transformer architecture. Specifically, we have\nadopted the recent open-source approach of training large language models, LLaMA (Touvron et al.,\n2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture\ninclude:\n",
        "font-size": 10,
        "token": 68
      },
      {
        "text": "• Embedding and output projection. Based on preliminary experimental findings, we have\nopted for the untied embedding approach instead of tying the weights of input embedding\nand output projection. This decision was made in order to achieve better performance with\nthe price of memory costs.\n 6\n",
        "font-size": 10,
        "token": 58
      }
    ]
  },
  {
    "page_num": 7,
    "content": [
      {
        "text": "Table 1: Model sizes, architectures, and optimization hyper-parameters.\n # of Params Hidden size Heads\n Layers\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "1.8B\n7B\n14B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "2048\n4096\n5120\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "16\n32\n40\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "24\n32\n40\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "Learning rate Batch size\n3.0 × 10−4\n3.0 × 10−4\n3.0 × 10−4\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "4M\n4M\n4M\n Training tokens\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "2.2T\n2.4T\n3.0T\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "• Positional embedding. We have chosen RoPE (Rotary Positional Embedding) (Su et al.,\n2021) as our preferred option for incorporating positional information into our model. RoPE\nhas been widely adopted and has demonstrated success in contemporary large language\nmodels, notably PaLM (Chowdhery et al., 2022; Anil et al., 2023) and LLaMA (Touvron\net al., 2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency\nmatrix, rather than BF16 or FP16, in order to prioritize model performance and achieve\nhigher accuracy.\n",
        "font-size": 10,
        "token": 147
      },
      {
        "text": "• Bias. For most layers, we remove biases following Chowdhery et al. (2022), but we add\nbiases in the QKV layer of attention to enhance the extrapolation ability of the model (Su,\n2023b).\n",
        "font-size": 10,
        "token": 54
      },
      {
        "text": "• Pre-Norm & RMSNorm. In modern Transformer models, pre-normalization is the most\nwidely used approach, which has been shown to improve training stability compared to\npost-normalization. Recent research has suggested alternative methods for better training\nstability, which we plan to explore in future versions of our model. Additionally, we have\nreplaced the traditional layer normalization technique described in (Ba et al., 2016) with\nRMSNorm (Jiang et al., 2023). This change has resulted in equivalent performance while\nalso improving efficiency.\n",
        "font-size": 10,
        "token": 120
      },
      {
        "text": "• Activation function. We have selected SwiGLU (Shazeer, 2020) as our activation function,\na combination of Swish (Ramachandran et al., 2017) and Gated Linear Unit (Dauphin et al.,\n2017). Our initial experiments have shown that activation functions based on GLU generally\noutperform other baseline options, such as GeLU (Hendrycks & Gimpel, 2016). As is\ncommon practice in previous research, we have reduced the dimension of the feed-forward\nnetwork (FFN) from 4 times the hidden size to 8\n",
        "font-size": 7,
        "token": 138
      },
      {
        "text": "3 of the hidden size.\n 2.3.2 CONTEXT LENGTH EXTENSION\n",
        "font-size": 10,
        "token": 17
      },
      {
        "text": "Transformer models have a significant limitation in terms of the context length for their attention\nmechanism. As the context length increases, the quadratic-complexity computation leads to a drastic\nincrease in both computation and memory costs. In this work, we have implemented simple training-\nfree techniques that are solely applied during inference to extend the context length of the model. One\nof the key techniques we have used is NTK-aware interpolation (bloc97, 2023), which adjusts the\nscale to prevent the loss of high-frequency information in a training-free manner. To further improve\nperformance, we have also implemented a trivial extension called dynamic NTK-aware interpolation,\nwhich is later formally discussed in (Peng et al., 2023a). It dynamically changes the scale by chunks,\navoiding severe performance degradation. These techniques allow us to effectively extend the context\nlength of Transformer models without compromising their computational efficiency or accuracy.\n",
        "font-size": 10,
        "token": 192
      },
      {
        "text": "QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak, 2022;\nSu, 2023a) and window attention (Beltagy et al., 2020). LogN-Scaling rescales the dot product of\nthe query and value by a factor that depends on the ratio of the context length to the training length,\nensuring that the entropy of the attention value remains stable as the context length grows. Window\nattention restricts the attention to a limited context window, preventing the model from attending to\ntokens that are too far away.\n",
        "font-size": 10,
        "token": 128
      },
      {
        "text": "We also observed that the long-context modeling ability of our model varies across layers, with lower\nlayers being more sensitive in context length extension compared to the higher layers. To leverage\nthis observation, we assign different window sizes to each layer, using shorter windows for lower\nlayers and longer windows for higher layers.\n 7\n",
        "font-size": 10,
        "token": 65
      }
    ]
  },
  {
    "page_num": 8,
    "content": [
      {
        "text": "Table 2: Overall performance on widely-used benchmarks compared to open-source base models.\nOur largest QWEN model with 14 billion parameters outperforms previous 13B SoTA models on all\ndatasets.\n Model\n MPT\n Falcon\n ChatGLM2\n InternLM\n Baichuan2\n LLaMA\n LLAMA 2\n StableBeluga2\n QWEN\n",
        "font-size": 9,
        "token": 83
      },
      {
        "text": "Params MMLU C-Eval GSM8K MATH HumanEval MBPP\n3-shot\n 5-shot\n 8-shot\n 5-shot\n 4-shot\n 0-shot\n",
        "font-size": 9,
        "token": 40
      },
      {
        "text": "7B\n30B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n40B\n 6B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "7B\n20B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n33B\n65B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "7B\n13B\n34B\n70B\n 70B\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "1.8B\n7B\n14B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "30.8\n47.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "27.8\n57.0\n 47.9\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "51.0\n62.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.7\n59.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "35.6\n47.7\n58.7\n63.7\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "46.8\n55.0\n62.6\n69.8\n 68.6\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "44.6\n58.2\n66.3\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "23.5\n-\n",
        "font-size": 9,
        "token": 6
      },
      {
        "text": "-\n-\n 51.7\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "53.4\n58.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "56.3\n59.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "27.3\n31.8\n37.5\n40.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "32.5\n41.4\n-\n50.1\n 51.4\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "54.7\n63.5\n72.1\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "9.1\n15.2\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "6.8\n19.6\n 32.4\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "31.2\n52.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "24.6\n52.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "11.0\n20.3\n42.3\n54.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "16.7\n29.6\n42.2\n63.3\n 69.6\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "21.2\n51.7\n61.3\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "3.0\n3.1\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "2.3\n5.5\n 6.5\n",
        "font-size": 9,
        "token": 13
      },
      {
        "text": "6.3\n7.9\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "5.6\n10.1\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "2.9\n4.2\n7.1\n10.6\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "3.3\n5.0\n6.2\n13.5\n 14.6\n",
        "font-size": 9,
        "token": 23
      },
      {
        "text": "5.6\n11.6\n24.8\n",
        "font-size": 9,
        "token": 14
      },
      {
        "text": "18.3\n25.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "-\n-\n -\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "10.4\n25.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "18.3\n17.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "12.8\n15.8\n21.7\n23.7\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "12.8\n18.9\n22.6\n29.9\n 28.0\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "17.1\n29.9\n32.3\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "22.8\n32.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "11.2\n29.8\n -\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "14.0\n35.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "24.2\n30.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "17.7\n22.0\n30.2\n37.7\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "20.8\n30.3\n33.0\n45.0\n 11.4\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "14.8\n31.6\n40.8\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "BBH\n3-shot\n",
        "font-size": 9,
        "token": 6
      },
      {
        "text": "35.6\n38.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "28.0\n37.1\n 33.7\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "37.0\n52.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "41.6\n49.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "33.5\n37.9\n50.0\n58.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "38.2\n45.6\n44.1\n64.9\n 69.3\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "28.2\n45.0\n53.4\n 2.4 TRAINING\n",
        "font-size": 10,
        "token": 22
      },
      {
        "text": "To train QWEN, we follow the standard approach of autoregressive language modeling, as described\nin Radford et al. (2018). This involves training the model to predict the next token based on the\ncontext provided by the previous tokens. We train models with context lengths of 2048. To create\nbatches of data, we shuffle and merge the documents, and then truncate them to the specified context\nlengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention\nin the attention modules (Dao et al., 2022). We adopt the standard optimizer AdamW (Kingma & Ba,\n2014; Loshchilov & Hutter, 2017) for pretraining optimization. We set the hyperparameters β1 = 0.9,\nβ2 = 0.95, and ϵ = 10−8. We use a cosine learning rate schedule with a specified peak learning rate\nfor each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak\nlearning rate. All the models are trained with BFloat16 mixed precision for training stability.\n",
        "font-size": 8,
        "token": 251
      },
      {
        "text": "2.5 EXPERIMENTAL RESULTS\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "To evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thorough\nbenchmark assessment using a series of datasets. We compare QWEN with the most recent open-\nsource base models, including LLaMA (Touvron et al., 2023a), LLaMA2 (Touvron et al., 2023b),\nMPT (Mosaic ML, 2023), Falcon (Almazrouei et al., 2023), Baichuan2 (Yang et al., 2023), Chat-\nGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), XVERSE (Inc., 2023b), and\nStableBeluga2 (Stability AI, 2023). Our evaluation covers a total of 7 popular benchmarks, which\nare MMLU (5-shot) (Hendrycks et al., 2020), C-Eval (5-shot) (Huang et al., 2023), GSM8K (8-\nshot) (Cobbe et al., 2021), MATH (4-shot) (Hendrycks et al., 2021), HumanEval (0-shot) (Chen et al.,\n2021b), MBPP (0-shot) (Austin et al., 2021), and BBH (Big Bench Hard) (3 shot) (Suzgun et al.,\n2022). We aim to provide a comprehensive summary of the overall performance of our models across\nthese benchmarks.\n 8\n",
        "font-size": 10,
        "token": 359
      }
    ]
  },
  {
    "page_num": 9,
    "content": [
      {
        "text": "Table 3: Results of QWEN on long-context inference using various techniques. Our experimental\nfindings reveal that the application of our crucial techniques enables the model to consistently achieve\nlow perplexity as the context length increases. This suggests that these techniques play a significant\nrole in enhancing the model’s ability to comprehend and generate lengthy texts.\n Model\n Sequence Length\n 1024\n 2048\n 4096\n 8192\n 16384\n",
        "font-size": 9,
        "token": 105
      },
      {
        "text": "QWEN-7B\n+ dynamic ntk\n+ dynamic ntk + logn\n+ dynamic ntk + logn + window attn\n",
        "font-size": 9,
        "token": 31
      },
      {
        "text": "QWEN-14B\n+ dynamic ntk + logn + window attn\n",
        "font-size": 9,
        "token": 19
      },
      {
        "text": "4.23\n4.23\n4.23\n4.23\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "-\n-\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "3.78\n3.78\n3.78\n3.78\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "3.46\n3.46\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "39.35\n3.59\n3.58\n3.58\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "22.79\n3.29\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "469.81\n3.66\n3.56\n3.49\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "334.65\n3.18\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "2645.09\n5.71\n4.62\n4.32\n",
        "font-size": 9,
        "token": 23
      },
      {
        "text": "3168.35\n3.42\n",
        "font-size": 9,
        "token": 13
      },
      {
        "text": "In this evaluation, we focus on the base language models without alignment and collect the baselines’\nbest scores from their official results and OpenCompass (OpenCompass Team, 2023). The results are\npresented in Table 2.\n",
        "font-size": 10,
        "token": 53
      },
      {
        "text": "Our experimental results demonstrate that the three QWEN models exhibit exceptional performance\nacross all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are\noutperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-\n13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small\nnumber of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even\noutperforms larger models in some instances. The findings highlight the impressive capabilities of\nthe QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B,\ncan still achieve strong performance in certain applications.\n",
        "font-size": 10,
        "token": 185
      },
      {
        "text": "To evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv3 in\nterms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation,\nLogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of\nour models in the context of over 8192 tokens.\n",
        "font-size": 10,
        "token": 75
      },
      {
        "text": "3 ALIGNMENT\n",
        "font-size": 10,
        "token": 4
      },
      {
        "text": "Pretrained large language models have been found to be out of sync with human behavior, making\nthem unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of\nalignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human\nfeedback (RLHF), can significantly improve the ability of language models to engage in natural\nconversation. In this section, we will delve into the details of how Qwen models have been trained\nusing SFT and RLHF, and evaluate their performance in the context of chat-based assistance.\n",
        "font-size": 10,
        "token": 117
      },
      {
        "text": "3.1 SUPERVISED FINETUNING\n",
        "font-size": 8,
        "token": 11
      },
      {
        "text": "To gain an understanding of human behavior, the initial step is to carry out supervised finetuning.\nThis process fine-tunes a pre-trained model on chat-style data, which includes both human queries\nand AI responses. Supervised finetuning is similar to text-to-text transfer, but it is capable of creating\na helpful AI assistant due to the intricate and varied nature of the datasets used for finetuning. In the\nfollowing sections, we will delve into the details of data construction and training methods.\n",
        "font-size": 10,
        "token": 103
      },
      {
        "text": "3.1.1 DATA\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations\nin multiple styles. While conventional FLAN datasets (Wei et al., 2022a) contain a vast amount of\ndata prompted with questions, instructions, and answers in natural language, our approach takes\nit a step further by annotating human-style conversations. This practice, inspired by Ouyang et al.\n",
        "font-size": 10,
        "token": 83
      },
      {
        "text": "3The dataset contains academic papers from https://arxiv.org/\n",
        "font-size": 6,
        "token": 13
      },
      {
        "text": "9\n",
        "font-size": 10,
        "token": 2
      }
    ]
  },
  {
    "page_num": 10,
    "content": [
      {
        "text": "(2022), is aimed at improving the model’s helpfulness by focusing on natural language generation for\ndiverse tasks. To ensure the model’s ability to generalize to a wide range of scenarios, we specifically\nexcluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore,\nwe have prioritized the safety of the language model by annotating data related to safety concerns\nsuch as violence, bias, and pornography. This will enable the model to detect and reject malicious\nprompts or provide safe answers in such situations.\n",
        "font-size": 10,
        "token": 111
      },
      {
        "text": "In addition to data quality, we have observed that the training method can significantly impact the\nfinal performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI,\n2022), which is a versatile meta language capable of describing both the metadata (such as roles)\nand the content of a turn. This format enables the model to effectively distinguish between various\ntypes of information, including system setup, user inputs, and assistant outputs, among others. By\nleveraging this approach, we can enhance the model’s ability to accurately process and analyze\ncomplex conversational data.\n 3.1.2 TRAINING\n",
        "font-size": 10,
        "token": 131
      },
      {
        "text": "Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We apply\nthe loss masks for the system and user inputs. More details are demonstrated in Appendix A.1.1.\n",
        "font-size": 10,
        "token": 45
      },
      {
        "text": "The model’s training process utilizes the AdamW optimizer, with the following hyperparameters: β1\nset to 0.9, β2 set to 0.95, and ϵ set to 10−8. The sequence length is limited to 2048, and the batch size\nis 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased over the\nfirst 1430 steps, reaching a peak of 2 × 10−6. To prevent overfitting, weight decay is applied with a\nvalue of 0.1, dropout regularization is set to 0.1, and gradient clipping is enforced with a limit of 1.0.\n",
        "font-size": 7,
        "token": 159
      },
      {
        "text": "3.2 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK\n",
        "font-size": 8,
        "token": 16
      },
      {
        "text": "While SFT has proven to be effective, we acknowledge that its generalization and creativity capa-\nbilities may be limited, and it is prone to overfitting. To address this issue, we have implemented\nReinforcement Learning from Human Feedback (RLHF) to further align SFT models with human\npreferences, following the approaches of Ouyang et al. (2022); Christiano et al. (2017). This process\ninvolves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al.,\n2017) to conduct policy training.\n",
        "font-size": 10,
        "token": 131
      },
      {
        "text": "3.2.1 REWARD MODEL\n",
        "font-size": 8,
        "token": 9
      },
      {
        "text": "To create a successful reward model, like building a large language model (LLM), it is crucial to\nfirst undergo pretraining and then finetuning. This pretraining process, also known as preference\nmodel pretraining (PMP) (Bai et al., 2022b), necessitates a vast dataset of comparison data. This\ndataset consists of sample pairs, each containing two distinct responses for a single query and their\ncorresponding preferences. Similarly, finetuning is also conducted on this type of comparison data,\nbut with a higher quality due to the presence of quality annotations.\n",
        "font-size": 10,
        "token": 124
      },
      {
        "text": "During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on\nhuman feedback for responses from the QWEN models. To ensure the diversity and complexity of\nuser prompts are properly taken into account, we have created a classification system with around\n6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and\ncomplexity when selecting prompts for annotation by the reward model (Lu et al., 2023). To generate\na wide range of responses, we have utilized QWEN models of different sizes and sampling strategies,\nas diverse responses can help reduce annotation difficulties and enhance the performance of the\nreward model. These responses are then evaluated by annotators following a standard annotation\nguideline, and comparison pairs are formed based on their scores.\n",
        "font-size": 8,
        "token": 164
      },
      {
        "text": "In creating the reward model, we utilize the same-sized pre-trained language model QWEN and\ninitiate the PMP process. Subsequently, we fine-tune the PMP model to enhance its performance. It\nis important to mention that we have incorporated a pooling layer into the original QWEN model to\nextract the reward for a sentence based on a specific end token. The learning rate for this process has\nbeen set to a constant value of 3 × 10−6, and the batch size is 64. Additionally, the sequence length\nis set to 2048, and the training process lasts for a single epoch.\n 10\n",
        "font-size": 10,
        "token": 140
      }
    ]
  },
  {
    "page_num": 11,
    "content": [
      {
        "text": "Table 4: Test Accuracy of QWEN PMP and reward model on diverse human preference benchmark\ndatasets.\n Model\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "PMP\nRM\n QWEN\n QWEN\n Anthropic\n Anthropic\n Helpful-base Helpful-online Helpful-base Helpful-online\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "OpenAI\nSumm.\n",
        "font-size": 9,
        "token": 6
      },
      {
        "text": "Stanford\nSHP\n",
        "font-size": 9,
        "token": 6
      },
      {
        "text": "OpenAI\nPRM800K\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "62.68\n74.78\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "61.62\n69.71\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "76.52\n73.98\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "65.43\n64.57\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "69.60\n69.99\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "60.05\n60.10\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "70.59\n70.52\n 3.2.2 REINFORCEMENT LEARNING\n",
        "font-size": 10,
        "token": 25
      },
      {
        "text": "Our Proximal Policy Optimization (PPO) process involves four models: the policy model, value\nmodel, reference model, and reward model. Before starting the PPO procedure, we pause the policy\nmodel’s updates and focus solely on updating the value model for 50 steps. This approach ensures\nthat the value model can adapt to different reward models effectively.\n",
        "font-size": 10,
        "token": 75
      },
      {
        "text": "During the PPO operation, we use a strategy of sampling two responses for each query simultaneously.\nThis strategy has proven to be more effective based on our internal benchmarking evaluations. We set\nthe KL divergence coefficient to 0.04 and normalize the reward based on the running mean.\nThe policy and value models have learning rates of 1 × 10−6 and 5 × 10−6, respectively. To enhance\ntraining stability, we utilize value loss clipping with a clip value of 0.15. For inference, the policy\ntop-p is set to 0.9. Our findings indicate that although the entropy is slightly lower than when top-p is\nset to 1.0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation\nrewards under similar conditions.\n",
        "font-size": 7,
        "token": 169
      },
      {
        "text": "Additionally, we have implemented a pre-trained gradient to mitigate the alignment tax. Empirical\nfindings indicate that, with this specific reward model, the KL penalty is adequately robust to\ncounteract the alignment tax in benchmarks that are not strictly code or math in nature, such as\nthose that test common sense knowledge and reading comprehension. It is imperative to utilize\na significantly larger volume of the pretrained data in comparison to the PPO data to ensure the\neffectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly\nlarge value for this coefficient can considerably impede the alignment to the reward model, eventually\ncompromising the ultimate alignment, while an overly small value would only have a marginal effect\non alignment tax reduction.\n 3.3 AUTOMATIC AND HUMAN EVALUATION OF ALIGNED MODELS\n",
        "font-size": 10,
        "token": 169
      },
      {
        "text": "To showcase the effectiveness of our aligned models, we conduct a comparison with other aligned\nmodels on well-established benchmarks, including MMLU (Hendrycks et al., 2020), C-Eval (Huang\net al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021b), and BBH (Suzgun et al.,\n2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting\nto demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists\nof an instruction and a question without any previous examples in the context. The results of the\nbaselines are collected from their official reports and OpenCompass (OpenCompass Team, 2023).\n",
        "font-size": 10,
        "token": 183
      },
      {
        "text": "The results in Table 5 demonstrate the effectiveness of our aligned models in understanding human\ninstructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models\nexcept ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets,\nincluding MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021),\nHumanEval (Chen et al., 2021b), and BBH (Suzgun et al., 2022). In particular, QWEN’s performance\nin HumanEval, which measures the quality of generated codes, is significantly higher than that of\nother open-source models.\n",
        "font-size": 10,
        "token": 195
      },
      {
        "text": "Moreover, QWEN’s performance is consistently better than that of open-source models of similar size,\nsuch as LLaMA2 (Touvron et al., 2023b), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM\nTeam, 2023), and Baichuan2 (Yang et al., 2023). This suggests that our alignment approach, which\ninvolves fine-tuning the model on a large dataset of human conversations, has been effective in\nimproving the model’s ability to understand and generate human-like language.\n",
        "font-size": 8,
        "token": 132
      },
      {
        "text": "11\n",
        "font-size": 10,
        "token": 3
      }
    ]
  },
  {
    "page_num": 12,
    "content": [
      {
        "text": "Table 5: Performance of aligned models on widely-used benchmarks. We report both zero-shot\nand few-shot performance of the models.\n",
        "font-size": 10,
        "token": 27
      },
      {
        "text": "Model\n Params\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "MMLU\n0-shot / 5-shot\n",
        "font-size": 8,
        "token": 11
      },
      {
        "text": "C-Eval\n0-shot / 5-shot\n",
        "font-size": 8,
        "token": 11
      },
      {
        "text": "GSM8K\n0-shot / 8-shot\n",
        "font-size": 8,
        "token": 12
      },
      {
        "text": "HumanEval\n0-shot\n",
        "font-size": 8,
        "token": 6
      },
      {
        "text": "BBH\n0-shot / 3-shot\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "GPT-3.5\nGPT-4\n ChatGLM2\n InternLM-Chat\n Baichuan2-Chat\n",
        "font-size": 8,
        "token": 29
      },
      {
        "text": "LLAMA 2-CHAT\n",
        "font-size": 6,
        "token": 7
      },
      {
        "text": "QWEN-CHAT\n",
        "font-size": 6,
        "token": 6
      },
      {
        "text": "-\n-\n 6B\n 7B\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "7B\n13B\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "7B\n13B\n70B\n",
        "font-size": 8,
        "token": 11
      },
      {
        "text": "1.8B\n7B\n14B\n Proprietary models\n",
        "font-size": 8,
        "token": 17
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "/ 69.1\n/ 83.0\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "/ 52.5\n/ 69.9\n Open-source models\n 45.5 / 46.0\n 50.1 / 52.6\n -\n",
        "font-size": 8,
        "token": 43
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "-\n-\n-\n / 51.1\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "/ 52.9\n/ 57.3\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "/ 46.2\n/ 54.6\n/ 63.8\n -\n",
        "font-size": 8,
        "token": 22
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "-\n-\n-\n / 53.6\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "/ 55.6\n/ 56.7\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "/ 31.9\n/ 36.2\n/ 44.3\n",
        "font-size": 8,
        "token": 21
      },
      {
        "text": "-\n-\n -\n -\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 8,
        "token": 3
      },
      {
        "text": "/ 78.2\n/ 91.4\n / 28.8\n / 33.0\n",
        "font-size": 8,
        "token": 28
      },
      {
        "text": "/ 32.8\n/ 55.3\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "/ 26.3\n/ 37.1\n/ 59.3\n",
        "font-size": 8,
        "token": 21
      },
      {
        "text": "42.4 / 43.9\n55.8 / 57.0\n64.6 / 66.5\n",
        "font-size": 8,
        "token": 33
      },
      {
        "text": "50.7 / 50.3\n59.7 / 59.3\n69.8 / 71.7\n",
        "font-size": 8,
        "token": 33
      },
      {
        "text": "27.8 / 19.5\n50.3 / 54.1\n60.1 / 59.3\n",
        "font-size": 8,
        "token": 33
      },
      {
        "text": "73.2\n86.6\n 11.0\n 14.6\n",
        "font-size": 8,
        "token": 22
      },
      {
        "text": "13.4\n17.7\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "12.2\n18.9\n32.3\n",
        "font-size": 8,
        "token": 15
      },
      {
        "text": "14.6\n37.2\n43.9\n",
        "font-size": 8,
        "token": 15
      },
      {
        "text": "-\n-\n -\n -\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "-\n-\n",
        "font-size": 8,
        "token": 2
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 8,
        "token": 3
      },
      {
        "text": "/ 70.1\n/ 86.7\n / 32.7\n / 32.5\n",
        "font-size": 8,
        "token": 28
      },
      {
        "text": "/ 35.8\n/ 49.9\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "/ 35.6\n/ 40.1\n/ 60.8\n",
        "font-size": 8,
        "token": 21
      },
      {
        "text": "27.1 / 25.0\n39.6 / 46.7\n46.9 / 58.7\n",
        "font-size": 8,
        "token": 33
      },
      {
        "text": "Figure 4: Results of the human evaluation for chat models. We compare Qwen-7B (SFT), Qwen-\n14B (SFT), Qwen-14B (RLHF), as well as GPT-4 against GPT-3.5. Each bar segment represents the\npercentage of wins, ties, and losses, from bottom to top. On average, the RLHF model outperforms\nthe SFT model and falls behind GPT-4 by a relatively small margin.\n 12\n",
        "font-size": 10,
        "token": 111
      }
    ]
  },
  {
    "page_num": 13,
    "content": [
      {
        "text": "Despite this, we have reservations about the ability of traditional benchmark evaluation to accurately\nmeasure the performance and potential of chat models trained with alignment techniques in today’s\nlandscape. The results mentioned earlier provide some evidence of our competitive standing, but we\nbelieve that it is crucial to develop new evaluation methods specifically tailored to aligned models.\n",
        "font-size": 10,
        "token": 66
      },
      {
        "text": "We believe that human evaluation is crucial, which is why we have created a carefully curated\ndataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered a\nwide range of topics, including knowledge, language understanding, creative writing, coding, and\nmathematics. To evaluate the performance of different models, we chose the SFT version of QWEN-\nCHAT-7B and the SFT and RLHF version of QWEN-CHAT-14B, and added two strong baselines,\nGPT-3.5 and GPT-44, for comparison. For each instruction, we asked three annotators to rank the\nmodel responses by the overall score of helpfulness, informativeness, validity, and other relevant\nfactors. Our dataset and evaluation methodology provides a comprehensive and rigorous assessment\nof the capabilities of different language models in various domains.\n",
        "font-size": 10,
        "token": 182
      },
      {
        "text": "Figure 4 illustrates the win rates of the various models. For each model, we report the percentage of\nwins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing\nthese statistics. The experimental results clearly demonstrate that the RLHF model outperforms\nthe SFT models by significant margins, indicating that RLHF can encourage the model to generate\nresponses that are more preferred by humans. In terms of overall performance, we find that the\nRLHF model significantly outperforms the SFT models, slightly falling behind GPT-4. This indicates\nthe effectiveness of RLHF for aligning to human preference. To provide a more comprehensive\nunderstanding of the models’ performance, we include a case study with examples from different\nmodels in Appendix A.2.2.\n",
        "font-size": 10,
        "token": 171
      },
      {
        "text": "3.4 TOOL USE, CODE INTERPRETER, AND AGENT\n",
        "font-size": 8,
        "token": 15
      },
      {
        "text": "Table 6: Performance of QWEN on the in-house Chinese benchmark that evaluates its ability to use\nunseen tools via ReAct prompting.\n Model\n GPT-4\n GPT-3.5\n QWEN-CHAT\n Params\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "Tool Selection (Acc.↑) Tool Input (Rouge-L↑)\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "False Positive Error (%)↓\n -\n -\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "1.8B\n7B\n14B\n 95\n 85\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "92\n98\n98\n 90\n 88\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "89\n91\n93\n 15.0\n 75.0\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "19.3\n7.3\n2.4\n",
        "font-size": 9,
        "token": 13
      },
      {
        "text": "The QWEN models, which are designed to be versatile, have the remarkable ability to assist with\n(semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can\nserve as agents or copilots to help streamline various tasks. We explore QWEN’s proficiency in the\nfollowing areas:\n • Utilizing unseen tools through ReAct prompting (Yao et al., 2022) (see Table 6).\n • Using a Python code interpreter to enhance math reasoning, data analysis, and more (see\n Table 7 and Table 8).\n • Functioning as an agent that accesses Hugging Face’s extensive collection of multimodal\n models while engaging with humans (see Table 9).\n",
        "font-size": 10,
        "token": 153
      },
      {
        "text": "To enhance QWEN’s capabilities as an agent or copilot, we employ the self-instruct (Wang et al.,\n2023c) strategy for supervised fine-tuning (SFT). Specifically, we utilize the in-context learning\ncapability of QWEN for self-instruction. By providing a few examples, we can prompt QWEN to\ngenerate more relevant queries and generate outputs that follow a specific format, such as ReAct.\nWe then apply rules and involve human annotators to filter out any noisy samples. Afterwards, the\nsamples are incorporated into QWEN’s training data, resulting in an updated version of QWEN that is\nmore dependable for self-instruction. We iterate through this process multiple times until we gather\n",
        "font-size": 8,
        "token": 154
      },
      {
        "text": "4To obtain the results from the models, we use the OpenAI APIs of GPT-3.5-turbo-0613 and GPT-4-0613.\n",
        "font-size": 6,
        "token": 41
      },
      {
        "text": "13\n",
        "font-size": 10,
        "token": 3
      }
    ]
  },
  {
    "page_num": 14,
    "content": [
      {
        "text": "Table 7: The proportion of code generated by QWEN that is executable on the in-house evaluation\nbenchmark for Code Interpreter. This benchmark examines QWEN’s coding proficiency in math\nproblem solving, data visualization, and general purposes. CODE LLAMA underperforms on\nvisualization tasks because it hallucinates non-existent columns solely based on CSV file names (see\nFigure 5).\n Model\n GPT-4\n GPT-3.5\n LLAMA 2-CHAT\n CODE LLAMA-INSTRUCT\n InternLM-Chat\n QWEN-CHAT\n Params\n -\n -\n",
        "font-size": 9,
        "token": 122
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B v1.1\n20B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "1.8B\n7B\n14B\n Category\n Math (%) Visualization (%) General (%) All (%)\n 91.9\n 89.2\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "41.9\n50.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "85.1\n93.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "78.4\n70.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "33.8\n82.4\n89.2\n 85.9\n 65.0\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "33.1\n40.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.0\n55.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "44.2\n44.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "30.1\n64.4\n84.1\n 82.8\n 74.1\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "24.1\n48.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "70.7\n74.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "62.1\n65.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "8.6\n67.2\n65.5\n 86.8\n 72.9\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "33.6\n44.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "65.1\n68.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "56.3\n54.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.8\n70.2\n81.7\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "Table 8: Correctness of the final response on the in-house evaluation benchmark for Code Interpreter.\nVisualization-Hard tasks involve planning multiple steps, while Visualization-Easy tasks do not.\nVisualization-All measures both types of tasks. CODE LLAMA excels in performing Visualization-\nEasy tasks but tends to underperform in Visualization-Hard tasks, due to its inclination to hallucinate\nnon-existent columns based on the name of a CSV file (see Figure 5).\n Model\n GPT-4\n GPT-3.5\n LLAMA 2-CHAT\n",
        "font-size": 9,
        "token": 114
      },
      {
        "text": "CODE LLAMA-INSTRUCT\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "InternLM-Chat\n",
        "font-size": 9,
        "token": 5
      },
      {
        "text": "QWEN-CHAT\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "Params\n -\n -\n",
        "font-size": 9,
        "token": 4
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B v1.1\n20B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "1.8B\n7B\n14B\n Category\n Math (%) Vis.-Hard (%) Vis.-Easy (%) Vis.-All (%)\n 66.7\n 33.3\n",
        "font-size": 9,
        "token": 41
      },
      {
        "text": "14.3\n8.3\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "26.2\n27.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "4.8\n21.4\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "3.6\n40.5\n53.6\n 60.8\n 55.7\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "39.2\n40.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "60.8\n62.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "40.5\n45.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "20.3\n54.4\n59.5\n 63.8\n 44.2\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "26.4\n23.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "42.9\n44.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "22.1\n33.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "11.7\n47.2\n56.4\n 82.8\n 47.3\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "3.9\n8.3\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "14.3\n28.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "28.5\n34.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "14.7\n41.9\n58.4\n 14\n",
        "font-size": 10,
        "token": 19
      }
    ]
  },
  {
    "page_num": 15,
    "content": [
      {
        "text": "Table 9: Results of QWEN-Chat on the Hugging Face Agent benchmark.\n Task\n Model\n Params\n Metric\n Tool Selection ↑\n Tool Used ↑ Code Correctness ↑\n GPT-4\n GPT-3.5\n Run Mode\n Starcoder-Base\n Starcoder\n QWEN-CHAT\n GPT-4\n GPT-3.5\n Chat Mode\n Starcoder-Base\n Starcoder\n QWEN-CHAT\n -\n -\n 15B\n 15B\n",
        "font-size": 9,
        "token": 109
      },
      {
        "text": "1.8B\n7B\n14B\n -\n -\n 15B\n 15B\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "1.8B\n7B\n14B\n 100\n 95.4\n 86.1\n 87.0\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "85.2\n87.0\n93.5\n 97.9\n 97.3\n 97.9\n 97.9\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "93.6\n94.7\n97.9\n 100\n 96.3\n 87.0\n 88.0\n",
        "font-size": 9,
        "token": 38
      },
      {
        "text": "84.3\n87.0\n94.4\n 97.9\n 96.8\n 97.9\n 97.9\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "93.6\n94.7\n97.9\n 97.4\n 87.0\n 68.9\n 68.9\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "61.1\n71.5\n87.0\n 98.5\n 89.6\n 91.1\n 89.6\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "73.2\n85.1\n95.5\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "an ample number of samples that possess both exceptional quality and a wide range of diversity. As a\nresult, our final collection consists of around 2000 high-quality samples.\n",
        "font-size": 10,
        "token": 37
      },
      {
        "text": "During the fine-tuning process, we mix these high-quality samples with all the other general-purpose\nSFT samples, rather than introducing an additional training stage. By doing so, we are able to retain\nessential general-purpose capabilities that are also pertinent for constructing agent applications.\n",
        "font-size": 10,
        "token": 55
      },
      {
        "text": "Using Tools via ReAct Prompting We have created and made publicly available a benchmark\nfor evaluating QWEN’s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see\nQwen Team, Alibaba Cloud, 2023b). To ensure fair evaluation, we have excluded any plugins that\nwere included in QWEN’s training set from the evaluation set. The benchmark assesses the model’s\naccuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility\nof the parameters passed into the plugin and the frequency of false positives. In this evaluation, a\nfalse positive occurs when the model incorrectly invokes a plugin in response to a query, despite not\nbeing required to do so.\n",
        "font-size": 10,
        "token": 158
      },
      {
        "text": "The results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in\nidentifying the relevance of a query to the available tools as the model size increases. However,\nthe table also highlights that beyond a certain point, there is little improvement in performance\nwhen it comes to selecting the appropriate tool and providing relevant arguments. This suggests that\nthe current preliminary benchmark may be relatively easy and may require further enhancement in\nfuture iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal\nperformance on this particular benchmark. This could potentially be attributed to the fact that the\nbenchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5’s\ncapabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if\nthe query cannot be effectively addressed by the provided tools.\n",
        "font-size": 10,
        "token": 187
      },
      {
        "text": "Using Code Interpreter for Math Reasoning and Data Analysis The Python code interpreter\nis widely regarded as a powerful tool for augmenting the capabilities of an LLM agent.\nIt is\nworth investigating whether QWEN can harness the full potential of this interpreter to enhance its\nperformance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this\nexploration, we have developed and made publicly available a benchmark that is specifically tailored\nfor this purpose (see Qwen Team, Alibaba Cloud, 2023a).\n",
        "font-size": 10,
        "token": 107
      },
      {
        "text": "The benchmark encompasses three primary categories of tasks: math problem-solving, data visu-\nalization, and other general-purpose tasks like file post-processing and web crawling. Within the\n 15\n",
        "font-size": 10,
        "token": 40
      }
    ]
  },
  {
    "page_num": 16,
    "content": [
      {
        "text": "visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved\nby simply writing and executing a single code snippet without the need for advanced planning skills.\nHowever, the more challenging level requires strategic planning and executing multiple code snippets\nin a sequential manner. This is because the subsequent code must be written based on the output of\nthe previous code. For example, an agent may need to examine the structure of a CSV file using one\ncode snippet before proceeding to write and execute additional code to create a plot.\n",
        "font-size": 10,
        "token": 107
      },
      {
        "text": "Regarding evaluation metrics, we consider both the executability and correctness of the generated\ncode. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying\nif the ground truth numerical answer is present in both the code execution result and the final response.\nWhen it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al., 2023),\na powerful multimodal language model. QWEN-VL is capable of answering text questions paired\nwith images, and we rely on it to confirm whether the image generated by the code fulfills the user’s\nrequest.\n",
        "font-size": 10,
        "token": 128
      },
      {
        "text": "The results regarding executability and correctness are presented in Table 7 and Table 8, respectively.\nIt is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which\nis not surprising since this benchmark specifically requires coding skills. However, it is worth noting\nthat specialist models that are optimized for code synthesis do not necessarily outperform generalist\nmodels. This is due to the fact that this benchmark encompasses various skills beyond coding,\nsuch as abstracting math problems into equations, understanding language-specified constraints, and\nresponding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT\nsurpass all other open-source alternatives of similar scale significantly, despite being generalist\nmodels.\n",
        "font-size": 10,
        "token": 164
      },
      {
        "text": "Serving as a Hugging Face Agent Hugging Face provides a framework called the Hugging Face\nAgent or Transformers Agent (Hugging Face, 2023), which empowers LLM agents with a curated set\nof multimodal tools, including speech recognition and image synthesis. This framework allows an\nLLM agent to interact with humans, interpret natural language commands, and employ the provided\ntools as needed.\n",
        "font-size": 10,
        "token": 85
      },
      {
        "text": "To evaluate QWEN’s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks\noffered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that\nQWEN performs quite well in comparison to other open-source alternatives, only slightly behind the\nproprietary GPT-4, demonstrating QWEN’s competitive capabilities.\n",
        "font-size": 10,
        "token": 76
      },
      {
        "text": "4 CODE-QWEN: SPECIALIZED MODEL FOR CODING\n",
        "font-size": 12,
        "token": 13
      },
      {
        "text": "Training on domain-specific data has been shown to be highly effective, particularly in the case of\ncode pretraining and finetuning. A language model that has been reinforced with training on code\ndata can serve as a valuable tool for coding, debugging, and interpretation, among other tasks. In this\nwork, we have developed a series of generalist models using pretraining and alignment techniques.\nBuilding on this foundation, we have created domain-specific models for coding by leveraging the\nbase language models of QWEN. We have implemented continued pretraining on code data for\nCODE-QWEN and then applied supervised finetuning to create CODE-QWEN-CHAT. The code\nmodels, CODE-QWEN-14B and CODE-QWEN-7B, are based on base language models with 14\nbillion and 7 billion parameters.\n",
        "font-size": 10,
        "token": 176
      },
      {
        "text": "4.1 CODE PRETRAINING\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "Unlike previous approaches that focused solely on pretraining on code data (Li et al., 2022; 2023d),\nwe take a different approach (Rozi`ere et al., 2023) by starting with our base language models, CODE-\nQWEN-14B-CHAT and CODE-QWEN-7B-CHAT, and then continuing to pretrain on a combination\nof text and code data. We believe that relying solely on code data for pretraining can result in a\nsignificant loss of the ability to function as a versatile assistant. Additionally, incorporating data from\na diverse range of domains can help enhance the models’ ability to understand and generate code, as\nwell as bridge the gap between language and coding. We continue to pretrain the models on a total of\naround 90 billion tokens.\n 16\n",
        "font-size": 10,
        "token": 182
      }
    ]
  },
  {
    "page_num": 17,
    "content": [
      {
        "text": "In the pretraining process, we initialize the model by corresponding base language models, CODE-\nQWEN-14B-CHAT and CODE-QWEN-7B-CHAT. Most applications depend on specialized models\nfor coding may lead to long contextual scenarios, such as tool use and code interpreter in Section\n3.4. In that case, we train models with context lengths of 8192. Similar to base model training in\nSection 2.4, we employ Flash Attention (Dao et al., 2022) in the attention modules, and adopt the\nstandard optimizer AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017), setting β1 = 0.9,\nβ2 = 0.95, and ϵ = 10−8. We set the learning rate as 3.0×10−5 for CODE-QWEN-7B and 6.0×10−5\nCODE-QWEN-14B, with 3% warm up iterations and no learning rate decays.\n 4.2 CODE SUPERVISED FINE-TUNING\n",
        "font-size": 10,
        "token": 252
      },
      {
        "text": "After conducting a series of empirical experiments, we have determined that the multi-stage SFT\nstrategy yields the best performance compared to other methods. In the supervised fine-tuning\nstage, the models CODE-QWEN-7B-CHAT and CODE-QWEN-14B-CHAT initialized by the code\nfoundation model CODE-QWEN-7B-CHAT and CODE-QWEN-14B-CHAT are optimized by the\nAdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) optimizer (β1 = 0.9, β2 = 0.95,\nϵ = 10−8) with a learning rate of 1.0 × 10−5 and 2.0 × 10−6, respectively. The learning rate\nincreases to the peaking value with the cosine learning rate schedule (3% warm-up steps) and remains\nconstant.\n",
        "font-size": 10,
        "token": 205
      },
      {
        "text": "4.3 EVALUATION\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "Our CODE-QWEN models have been compared with both proprietary and open-source language\nmodels, as shown in Tables 11 and 10. These tables present the results of our evaluation on the test\nsets of Humaneval (Chen et al., 2021b), MBPP (Austin et al., 2021), and the multi-lingual code\ngeneration benchmark HUMANEVALPACK (Muennighoff et al., 2023). The comparison is based on\nthe pass@1 performance of the models on these benchmark datasets. The results of this comparison\nare clearly demonstrated in Tables 10 and 11.\n",
        "font-size": 10,
        "token": 141
      },
      {
        "text": "Our analysis reveals that specialized models, specifically CODE-QWEN and CODE-QWEN-CHAT, sig-\nnificantly outperform previous baselines with similar parameter counts, such as OCTOGEEX (Muen-\nnighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023d), and CodeGeeX2 (Zheng et al., 2023). In\nfact, these models even rival the performance of larger models like Starcoder (Li et al., 2023d) and\nWizardCoder-15B (Luo et al., 2023b).\n",
        "font-size": 10,
        "token": 142
      },
      {
        "text": "When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODE-\nQWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that\nthese models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with\nthe continued scaling of both model size and data size, we believe that this gap can be narrowed in\nthe near future.\n",
        "font-size": 10,
        "token": 94
      },
      {
        "text": "It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping the\nfull extent of the strengths and weaknesses of the models. In our opinion, it is crucial to develop more\nrigorous tests to enable us to accurately assess our relative performance in comparison to GPT-4.\n",
        "font-size": 10,
        "token": 59
      },
      {
        "text": "5 MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING\n",
        "font-size": 12,
        "token": 19
      },
      {
        "text": "We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is built\non top of the QWEN pretrained language models. Specifically, we have developed assistant models\nthat are specifically designed to excel in arithmetic and mathematics and are aligned with human\nbehavior. These models are referred to as MATH-QWEN-CHAT. We are releasing two versions of this\nmodel series, MATH-QWEN-14B-CHAT and MATH-QWEN-7B-CHAT, which have 14 billion and 7\nbillion parameters, respectively.\n",
        "font-size": 8,
        "token": 123
      },
      {
        "text": "5.1 TRAINING\n",
        "font-size": 8,
        "token": 6
      },
      {
        "text": "We carry out math SFT on our augmented math instructional dataset for mathematics reasoning,\nand therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average\nlengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs\n 17\n",
        "font-size": 10,
        "token": 72
      }
    ]
  },
  {
    "page_num": 18,
    "content": [
      {
        "text": "Table 10: Results of pass@1 (%) on HumanEval and MBPP. Most scores are retrieved from the\npapers of StarCoder (Li et al., 2023d), CodeT5+ (Wang et al., 2023d), WizardCoder (Luo et al.,\n2023b) and CODE LLAMA (Rozi`ere et al., 2023).\n Model\n Params HumanEval MBPP\n Proprietary models\n 26.2\n 36.0\n 37.6\n 33.5\n 47.0\n 73.2\n 86.6\n",
        "font-size": 10,
        "token": 145
      },
      {
        "text": "12.2\n20.1\n22.6\n30.5\n 18.3\n 29.3\n 35.9\n 40.8\n 30.9\n 35.0\n",
        "font-size": 10,
        "token": 56
      },
      {
        "text": "33.5\n36.0\n48.8\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": "34.8\n42.7\n41.5\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": "38.4\n43.3\n53.7\n 62.2\n",
        "font-size": 10,
        "token": 21
      },
      {
        "text": "64.0\n73.2\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "37.2\n43.9\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "40.2\n45.1\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "43.3\n66.4\n 36.8\n 47.0\n 50.0\n 45.9\n 58.1\n -\n -\n",
        "font-size": 10,
        "token": 42
      },
      {
        "text": "20.8\n27.6\n33.8\n45.4\n 20.9\n 35.3\n -\n 49.5\n -\n -\n",
        "font-size": 10,
        "token": 41
      },
      {
        "text": "41.4\n47.0\n55.0\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": "44.4\n49.4\n57.0\n",
        "font-size": 10,
        "token": 15
      },
      {
        "text": "47.6\n49.0\n56.2\n 61.2\n",
        "font-size": 10,
        "token": 21
      },
      {
        "text": "55.6\n61.2\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "35.8\n46.4\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "41.8\n51.4\n",
        "font-size": 10,
        "token": 10
      },
      {
        "text": "44.2\n52.4\n 540B\n 540B\n -\n -\n -\n -\n -\n Open-source models\n PaLM\n PaLM-Coder\n PaLM 2-S\n Code-Cushman-001\n Code-Davinci-002\n GPT-3.5\n GPT-4\n",
        "font-size": 10,
        "token": 74
      },
      {
        "text": "LLAMA 2\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "CodeGen-Multi\n CodeGen-Mono\n CodeGeeX2\n StarCoder-Prompted\n CodeT5+\n InstructCodeT5+\n CODE LLAMA\n CODE LLAMA-INSTRUCT\n",
        "font-size": 10,
        "token": 41
      },
      {
        "text": "CODE LLAMA-PYTHON\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "UNNATURAL CODE LLAMA\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "WizardCoder-Python\n",
        "font-size": 10,
        "token": 5
      },
      {
        "text": "QWEN-CHAT\n",
        "font-size": 8,
        "token": 6
      },
      {
        "text": "CODE-QWEN\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "CODE-QWEN-CHAT\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "7B\n13B\n34B\n70B\n 16B\n 16B\n 6B\n 15B\n 16B\n 16B\n",
        "font-size": 10,
        "token": 44
      },
      {
        "text": "7B\n13B\n34B\n",
        "font-size": 10,
        "token": 11
      },
      {
        "text": "7B\n13B\n34B\n",
        "font-size": 10,
        "token": 11
      },
      {
        "text": "7B\n13B\n34B\n 34B\n",
        "font-size": 10,
        "token": 16
      },
      {
        "text": "13B\n34B\n",
        "font-size": 10,
        "token": 8
      },
      {
        "text": "7B\n14B\n",
        "font-size": 10,
        "token": 7
      },
      {
        "text": "7B\n14B\n",
        "font-size": 10,
        "token": 7
      },
      {
        "text": "7B\n14B\n 18\n",
        "font-size": 10,
        "token": 11
      }
    ]
  },
  {
    "page_num": 19,
    "content": [
      {
        "text": "Table 11: Zero-shot pass@1 (%) performance on the HUMANEVALPACK (synthesize) bench-\nmark. The baseline results are partly from OCTOPACK (Muennighoff et al., 2023)\n Model\n Params\n Programming Language\n Python\n JavaScript\n Java\n Go\n C++\n Rust Avg.\n Proprietary models\n GPT-4\n -\n 86.6\n 82.9\n 81.7\n 72.6\n 78.7\n 67.1\n 78.3\n Open-source models\n InstructCodeT5+\n StarChat-β\n StarCoder\n CodeGeeX2\n OCTOGEEX\n",
        "font-size": 9,
        "token": 151
      },
      {
        "text": "OCTOCODER\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "WizardCoder\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "QWEN-CHAT\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "CODE-QWEN\n",
        "font-size": 7,
        "token": 5
      },
      {
        "text": "CODE-QWEN-CHAT\n",
        "font-size": 7,
        "token": 7
      },
      {
        "text": "16B\n 15B\n 15B\n 6B\n 6B\n 15B\n 15B\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "7B\n14B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n14B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n14B\n 37.0\n 33.5\n 33.6\n 35.9\n 44.7\n 46.2\n 59.8\n",
        "font-size": 9,
        "token": 49
      },
      {
        "text": "37.2\n43.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "40.2\n45.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "43.3\n66.4\n 18.9\n 31.4\n 30.8\n 32.2\n 33.8\n 39.2\n 49.5\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "23.2\n38.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "40.4\n51.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "41.5\n58.5\n 17.4\n 26.7\n 30.2\n 30.8\n 36.9\n 38.2\n 36.1\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "32.9\n42.7\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "40.2\n57.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "49.4\n56.1\n 9.5\n 25.5\n 17.6\n 22.5\n 21.9\n 30.4\n 36.4\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "20.7\n34.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.2\n39.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "29.3\n47.6\n 19.8\n 26.6\n 31.6\n 29.3\n 32.3\n 35.6\n 40.9\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "22.0\n24.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "20.7\n18.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "32.9\n54.2\n 0.3\n 14.0\n 21.8\n 18.1\n 15.7\n 23.4\n 20.2\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "9.1\n18.9\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "15.8\n20.7\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "20.1\n28.7\n 17.1\n 26.3\n 27.6\n 28.1\n 30.9\n 35.5\n 40.5\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "24.2\n33.7\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "30.6\n38.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "36.1\n51.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "Table 12: Results of models on mathematical reasoning. We report the accuracy of QWEN for all\nbenchmarks using greedy decoding. For MATH, we are reporting QWEN’s performances on the test\nset from Lightman et al. (2023).\n Model\n Params GSM8K MATH Math401 Math23K\n GPT-4\n GPT-3.5\n Minerva\n Proprietary models\n -\n -\n",
        "font-size": 9,
        "token": 97
      },
      {
        "text": "8B\n62B\n540B\n 92.0\n 80.8\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "16.2\n52.4\n58.8\n 42.5\n 34.1\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "14.1\n27.6\n33.6\n Open-source models\n LLaMA-1 RFT\n WizardMath\n GAIRMath-Abel\n",
        "font-size": 9,
        "token": 37
      },
      {
        "text": "QWEN-CHAT\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "MATH-QWEN-CHAT\n",
        "font-size": 7,
        "token": 8
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n70B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "7B\n13B\n70B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "7B\n14B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n14B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "5.2\n5.1\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "10.7\n14.0\n22.7\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "13.0\n17.3\n28.3\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "6.8\n18.4\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "17.2\n24.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "46.5\n52.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.9\n63.9\n81.6\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "59.7\n66.4\n83.6\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "50.3\n60.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "62.5\n69.8\n 19\n 83.5\n 75.1\n 74.0\n 60.0\n",
        "font-size": 9,
        "token": 38
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "-\n-\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "-\n-\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "-\n-\n-\n",
        "font-size": 9,
        "token": 3
      },
      {
        "text": "57.4\n70.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "80.8\n85.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "51.2\n67.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "75.4\n78.4\n",
        "font-size": 9,
        "token": 10
      }
    ]
  },
  {
    "page_num": 20,
    "content": [
      {
        "text": "in the math SFT dataset are examination questions, and it is easy for the model to predict the input\nformat and it is meaningless for the model to predict the input condition and numbers which could be\nrandom. Thus, we mask the inputs of the system and user to avoid loss computation on them and find\nmasking them accelerates the convergence during our preliminary experiments. For optimization, we\nuse the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning\nrate of 2 × 10−5 and a training step of 50 000.\n 5.2 EVALUATION\n",
        "font-size": 10,
        "token": 132
      },
      {
        "text": "We evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al., 2021), MATH\n(Challenging competition math problems) (Hendrycks et al., 2021), Math401 (Arithmetic abil-\nity) (Yuan et al., 2023b), and Math23K (Chinese grade school math) (Wang et al., 2017). We compare\nMATH-QWEN with proprietary models ChatGPT and Minerva (Lewkowycz et al., 2022) and open-\nsourced math-specialized model RFT (Yuan et al., 2023a), WizardMath (Luo et al., 2023a), and\nGAIRMath-Abel (Chern et al., 2023a) in Table 12. MATH-QWEN-CHAT models show better math\nreasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of\nsimilar sizes. Compared to proprietary models, MATH-QWEN-CHAT-7B outperforms Minerva-8B in\nMATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and\ndelivers better performance on arithmetic ability and Chinese math problems.\n",
        "font-size": 10,
        "token": 310
      },
      {
        "text": "6 RELATED WORK\n",
        "font-size": 10,
        "token": 4
      },
      {
        "text": "6.1 LARGE LANGUAGE MODELS\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "The excitement of LLM began with the introduction of the Transformer architecture (Vaswani et al.,\n2017), which was then applied to pretraining large-scale data by researchers such as Radford et al.\n(2018); Devlin et al. (2018); Liu et al. (2019). These efforts led to significant success in transfer\nlearning, with model sizes growing from 100 million to over 10 billion parameters (Raffel et al.,\n2020; Shoeybi et al., 2019).\n",
        "font-size": 10,
        "token": 124
      },
      {
        "text": "In 2020, the release of GPT-3, a massive language model that is 10 times larger than T5, demonstrated\nthe incredible potential of few-shot and zero-shot learning through prompt engineering and in-context\nlearning, and later chain-of-thought prompting (Wei et al., 2022c). This success has led to a number\nof studies exploring the possibilities of further scaling these models (Scao et al., 2022; Zhang et al.,\n2022; Du et al., 2021; Zeng et al., 2022; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022;\nBlack et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Thoppilan\net al., 2022). As a result, the community has come to view these large language models as essential\nfoundations for downstream models (Bommasani et al., 2021).\n",
        "font-size": 10,
        "token": 256
      },
      {
        "text": "The birth of ChatGPT (OpenAI, 2022) and the subsequent launch of GPT-4 (OpenAI, 2023) marked\ntwo historic moments in the field of artificial intelligence, demonstrating that large language models\n(LLMs) can serve as effective AI assistants capable of communicating with humans. These events\nhave sparked interests among researchers and developers in building language models that are aligned\nwith human values and potentially even capable of achieving artificial general intelligence (AGI) (Anil\net al., 2023; Anthropic, 2023a;b).\n",
        "font-size": 10,
        "token": 126
      },
      {
        "text": "One notable development in this area is the emergence of open-source LLMs, specifically\nLLaMA (Touvron et al., 2023a) and LLAMA 2 (Touvron et al., 2023b), which have been recognized\nas the most powerful open-source language models ever created. This has led to a surge of activity\nin the open-source community (Wolf et al., 2019), with a series of large language models being\ndeveloped collaboratively to build upon this progress (Mosaic ML, 2023; Almazrouei et al., 2023;\nChatGLM2 Team, 2023; Yang et al., 2023; InternLM Team, 2023).\n",
        "font-size": 8,
        "token": 170
      },
      {
        "text": "6.2 ALIGNMENT\n",
        "font-size": 10,
        "token": 6
      },
      {
        "text": "The community was impressed by the surprising effectiveness of alignment on LLMs. Previously,\nLLMs without alignment often struggle with issues such as repetitive generation, hallucination,\nand deviation from human preferences. Since 2021, researchers have been diligently working on\ndeveloping methods to enhance the performance of LLMs in downstream tasks (Wei et al., 2022a;\n 20\n",
        "font-size": 10,
        "token": 83
      }
    ]
  },
  {
    "page_num": 21,
    "content": [
      {
        "text": "Sanh et al., 2021; Longpre et al., 2023; Chung et al., 2022; Muennighoff et al., 2022). Furthermore,\nresearchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al.,\n2022; Askell et al., 2021; Bai et al., 2022b;c). One major challenge in alignment research is the\ndifficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or\ninstructions, it is not feasible for others to collect such data.\n",
        "font-size": 10,
        "token": 137
      },
      {
        "text": "However, there has been some progress in this area, such as the self-instruct approach proposed\nin Wang et al. (2023c). This innovative work offers a potential solution to the data collection problem\nin alignment research. As a result, there has been a surge in open-source chat data, including\nAlpaca (Taori et al., 2023), MOSS (Sun et al., 2023a), Dolly (Conover et al., 2023), Evol-Instruct (Xu\net al., 2023b), and others (Sun et al., 2023b; Xu et al., 2023a;c; Chen et al., 2023b; Ding et al.,\n2023; Ji et al., 2023; Yang, 2023). Similarly, there has been an increase in open-source chat models,\nsuch as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Guanaco (Dettmers et al., 2023),\nMOSS (Sun et al., 2023a), WizardLM (Xu et al., 2023b), and others (Xu et al., 2023c; Chen et al.,\n2023b; Ding et al., 2023; Wang et al., 2023b).\n",
        "font-size": 10,
        "token": 326
      },
      {
        "text": "To train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyang\net al., 2022). While SFT is similar to pretraining, it focuses on instruction following using the\naforementioned data. However, for many developers, the limited memory capacity is a major obstacle\nto further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al.,\n2021) and Q-LoRA (Dettmers et al., 2023), have gained popularity in the community. LoRA tunes\nonly low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and\npaged attention (Dettmers et al., 2022; Frantar et al., 2022; Kwon et al., 2023). In terms of RLHF,\nrecent methods such as PPO (Schulman et al., 2017; Touvron et al., 2023b) have been adopted, but\nthere are also alternative techniques aimed at addressing the complexity of optimization, such as\nRRHF (Yuan et al., 2023c), DPO (Rafailov et al., 2023), and PRO (Song et al., 2023). Despite the\nongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it\nenhances the intelligence of LLMs and what potential drawbacks it may have.\n",
        "font-size": 10,
        "token": 333
      },
      {
        "text": "6.3 TOOL USE AND AGENTS\n",
        "font-size": 8,
        "token": 9
      },
      {
        "text": "LLM’s planning function allows for the invocation of tools, such as APIs or agent capabilities,\nthrough in-context learning, as demonstrated by Schick et al. (2023). Yao et al. (2022) introduced\nReAct, a generation format that enables the model to generate thoughts on which tool to use, accept\ninput from API observations, and generate a response. GPT-3.5 and GPT-4, when prompted with\nfew shots, have shown consistent and impressive performance. In addition to tool usage, LLMs can\nutilize external memory sources like knowledge bases (Hu et al., 2023; Zhong et al., 2023b) or search\nengines (Nakano et al., 2021; Liu et al., 2023b) to generate more accurate and informative answers.\nThis has led to the popularity of frameworks like LangChain (LangChain, Inc., 2023). The research on\nLLMs for tool use has also sparked interest in building agents with LLM capabilities, such as agents\nthat can call different AI models (Shen et al., 2023; Li et al., 2023a), embodied lifelong learning or\nmultimodal agents (Wang et al., 2023a; Driess et al., 2023), and multiple agents interacting with each\nother and even building a micro-society (Chen et al., 2023a; Li et al., 2023b; Xu et al., 2023d; Hong\net al., 2023).\n",
        "font-size": 10,
        "token": 358
      },
      {
        "text": "6.4 LLM FOR CODING\n",
        "font-size": 8,
        "token": 9
      },
      {
        "text": "Previous research has demonstrated that LLMs possess remarkable capabilities in code understanding\nand generation, particularly those with massive numbers of parameters (Chowdhery et al., 2022;\nAnil et al., 2023; Rae et al., 2021; Hoffmann et al., 2022). Moreover, several LLMs have been pre-\ntrained, continued pre-trained, or fine-tuned on coding-related data, which has resulted in significantly\nimproved performance compared to general-purpose LLMs. These models include Codex Chen\net al. (2021a), AlphaCode (Li et al., 2022), SantaCoder (Allal et al., 2023), Starcoder-Base (Li et al.,\n2023d), InCoder (Fried et al., 2022), CodeT5 (Wang et al., 2021), CodeGeeX (Zheng et al., 2023),\nand CODE LLAMA (Rozi`ere et al., 2023). In addition to these models, recent studies have focused on\ndeveloping specialized alignment techniques for coding, such as Code Llama-Instruct (Rozi`ere et al.,\n2023) and StarCoder (Li et al., 2023d). These models can assist developers in various code-related\ntasks, including code generation (Chen et al., 2021b; Austin et al., 2021), code completion (Zhang\net al., 2023a), code translation (Szafraniec et al., 2023), bug fixing (Muennighoff et al., 2023), code\n",
        "font-size": 8,
        "token": 379
      },
      {
        "text": "21\n",
        "font-size": 10,
        "token": 3
      }
    ]
  },
  {
    "page_num": 22,
    "content": [
      {
        "text": "refinement (Liu et al., 2023c), and code question answering (Liu & Wan, 2021). In a word, LLMs\nhave the potential to revolutionize the field of coding by providing developers with powerful tools for\ncode comprehension, generation, and related tasks.\n 6.5 LLM FOR MATHEMATICS\n",
        "font-size": 10,
        "token": 78
      },
      {
        "text": "LLMs with a certain model scale have been found to possess the ability to perform mathematical\nreasoning (Wei et al., 2022b; Suzgun et al., 2022). In order to encourage LLMs to achieve better\nperformance on math-related tasks, researchers have employed techniques such as chain-of-thought\nprompting (Wei et al., 2022c) and scratchpad (Nye et al., 2021), which have shown promising results.\nAdditionally, self-consistency (Wang et al., 2022) and least-to-most prompting (Zhou et al., 2022)\nhave further improved the performance of these models on these tasks. However, prompt engineering\nis a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to\nconsistently perform well or achieve satisfactory results in solving mathematical problems. Moreover,\nsimply scaling the data and model size is not an efficient way to improve a model’s mathematical\nreasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently\nenhance these capabilities (Hendrycks et al., 2021; Lewkowycz et al., 2022; Taylor et al., 2022;\nLightman et al., 2023). Additionally, fine-tuning on math-related instruction-following datasets (Si\net al., 2023; Yuan et al., 2023a; Luo et al., 2023a; Yue et al., 2023; Chern et al., 2023a; Yu et al.,\n2023), has also been effective and more cost-effective than math-specific pretraining. Despite their\nlimitations in terms of accuracy, LLMs still have significant potential to assist users with practical\nmathematical problems. There is ample scope for further development in this area.\n",
        "font-size": 10,
        "token": 417
      },
      {
        "text": "7 CONCLUSION\n",
        "font-size": 10,
        "token": 4
      },
      {
        "text": "In this report, we present the QWEN series of large language models, which showcase the latest\nadvancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models\nhave been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using\ncutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized\nmodels for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-\nCHAT, which have been trained on domain-specific data to excel in their respective fields. Our results\ndemonstrate that the QWEN series is competitive with existing open-source models and even matches\nthe performance of some proprietary models on comprehensive benchmarks and human evaluation.\n",
        "font-size": 10,
        "token": 173
      },
      {
        "text": "We believe that the open access of QWEN will foster collaboration and innovation within the\ncommunity, enabling researchers and developers to build upon our work and push the boundaries of\nwhat is possible with language models. By providing these models to the public, we hope to inspire\nnew research and applications that will further advance the field and contribute to our understanding of\nthe variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents\na major milestone in our development of large language models, and we are excited to see how it will\nbe used to drive progress and innovation in the years to come.\n 22\n",
        "font-size": 10,
        "token": 130
      }
    ]
  },
  {
    "page_num": 23,
    "content": [
      {
        "text": "REFERENCES\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don’t\nreach for the stars! arXiv preprint arXiv:2301.03988, 2023.\n",
        "font-size": 10,
        "token": 94
      },
      {
        "text": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language\nmodel with state-of-the-art performance, 2023.\n",
        "font-size": 10,
        "token": 111
      },
      {
        "text": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n Anthropic.\n Introducing Claude, 2023a. URL https://www.anthropic.com/index/\n introducing-claude.\n Anthropic. Claude 2. Technical report, Anthropic, 2023b. URL https://www-files.\n anthropic.com/production/images/Model-Card-Claude-2.pdf.\n",
        "font-size": 10,
        "token": 155
      },
      {
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\n",
        "font-size": 10,
        "token": 80
      },
      {
        "text": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\n AutoGPT. AutoGPT: The heart of the open-source agent ecosystem, 2023. URL https://\n github.com/Significant-Gravitas/Auto-GPT.\n Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\n abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.\n",
        "font-size": 10,
        "token": 183
      },
      {
        "text": "Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng\nWang, Sinan Tan, An Yang andf Zeyu Cui, Yu Han, Shuai Bai, Wenbin Ge, Jianxin Ma, Junyang\nLin, Jingren Zhou, and Chang Zhou. OFASys: A multi-modal multi-task learning system for\nbuilding generalist models. CoRR, abs/2212.04408, 2022a. doi: 10.48550/arXiv.2212.04408. URL\nhttps://doi.org/10.48550/arXiv.2212.04408.\n",
        "font-size": 10,
        "token": 175
      },
      {
        "text": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization,\ntext reading, and beyond. CoRR, abs/2308.12966, 2023. doi: 10.48550/arXiv.2308.12966. URL\nhttps://doi.org/10.48550/arXiv.2308.12966.\n",
        "font-size": 10,
        "token": 137
      },
      {
        "text": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022b.\n",
        "font-size": 10,
        "token": 88
      },
      {
        "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022c.\n Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n arXiv preprint arXiv:2004.05150, 2020.\n",
        "font-size": 10,
        "token": 136
      },
      {
        "text": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about\nphysical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-\nference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\n 23\n",
        "font-size": 10,
        "token": 206
      }
    ]
  },
  {
    "page_num": 24,
    "content": [
      {
        "text": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022.\n bloc97.\n",
        "font-size": 10,
        "token": 93
      },
      {
        "text": "NTK-aware scaled RoPE allows LLaMA models to have extended (8k+) con-\nURL\n",
        "font-size": 10,
        "token": 22
      },
      {
        "text": "text size without any fine-tuning and minimal perplexity degradation., 2023.\nhttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\n",
        "font-size": 10,
        "token": 54
      },
      {
        "text": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n",
        "font-size": 10,
        "token": 90
      },
      {
        "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n ChatGLM2 Team. ChatGLM2-6B: An open bilingual chat LLM, 2023. URL https://github.\n com/THUDM/ChatGLM2-6B.\n",
        "font-size": 10,
        "token": 133
      },
      {
        "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021a. URL https://arxiv.\norg/abs/2107.03374.\n",
        "font-size": 10,
        "token": 327
      },
      {
        "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021b.\n",
        "font-size": 10,
        "token": 87
      },
      {
        "text": "Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia\nQin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023a.\n",
        "font-size": 10,
        "token": 97
      },
      {
        "text": "Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang,\nJuhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing ChatGPT across languages.\narXiv preprint arXiv:2304.10453, 2023b.\n Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu.\n Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023a.\n",
        "font-size": 10,
        "token": 145
      },
      {
        "text": "I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai–a tool augmented\nframework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023b.\n",
        "font-size": 10,
        "token": 96
      },
      {
        "text": "David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 7654–7664, 2022.\n",
        "font-size": 10,
        "token": 66
      },
      {
        "text": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023. URL\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\n",
        "font-size": 10,
        "token": 114
      },
      {
        "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n 24\n",
        "font-size": 10,
        "token": 92
      }
    ]
  },
  {
    "page_num": 25,
    "content": [
      {
        "text": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg,\nSamy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett\n(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-\nral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n4299–4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n-Abstract.html.\n",
        "font-size": 10,
        "token": 168
      },
      {
        "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\n",
        "font-size": 10,
        "token": 85
      },
      {
        "text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein,\nChristy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp.\n2924–2936. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1300. URL\nhttps://doi.org/10.18653/v1/n19-1300.\n",
        "font-size": 10,
        "token": 198
      },
      {
        "text": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\n",
        "font-size": 10,
        "token": 99
      },
      {
        "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\n",
        "font-size": 10,
        "token": 87
      },
      {
        "text": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised\ncross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\n",
        "font-size": 10,
        "token": 103
      },
      {
        "text": "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick\nWendell, Matei Zaharia, and Reynold Xin. Free Dolly: Introducing the world’s first truly open\ninstruction-tuned LLM, 2023. URL https://www.databricks.com/blog/2023/04/\n12/dolly-first-open-commercially-viable-instruction-tuned-llm.\n",
        "font-size": 10,
        "token": 104
      },
      {
        "text": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R´e.\n",
        "font-size": 10,
        "token": 113
      },
      {
        "text": "FlashAt-\nIn NeurIPS,\nURL http://papers.nips.cc/paper_files/paper/2022/hash/\n",
        "font-size": 10,
        "token": 27
      },
      {
        "text": "tention:\n2022.\n-Abstract-Conference.html.\n Fast and memory-efficient exact attention with io-awareness.\n",
        "font-size": 10,
        "token": 25
      },
      {
        "text": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In International conference on machine learning, pp. 933–941. PMLR,\n2017.\n Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix\n multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning\n of quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.\n",
        "font-size": 10,
        "token": 185
      },
      {
        "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
        "font-size": 10,
        "token": 60
      },
      {
        "text": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\n 25\n",
        "font-size": 10,
        "token": 87
      }
    ]
  },
  {
    "page_num": 26,
    "content": [
      {
        "text": "Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023.\n",
        "font-size": 10,
        "token": 92
      },
      {
        "text": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569.\nPMLR, 2022.\n",
        "font-size": 10,
        "token": 94
      },
      {
        "text": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\nGLM: General language model pretraining with autoregressive blank infilling. arXiv preprint\narXiv:2103.10360, 2021.\n",
        "font-size": 10,
        "token": 78
      },
      {
        "text": "William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):\n5232–5270, 2022.\n",
        "font-size": 10,
        "token": 61
      },
      {
        "text": "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n",
        "font-size": 10,
        "token": 69
      },
      {
        "text": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. ArXiv, abs/2204.05999, 2022.\n Google. An important next step on our AI journey, 2023. URL https://blog.google/\n technology/ai/bard-google-ai-search-updates/.\n",
        "font-size": 10,
        "token": 122
      },
      {
        "text": "Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with Gaussian\nerror linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.\n08415.\n",
        "font-size": 10,
        "token": 67
      },
      {
        "text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\narXiv preprint\n",
        "font-size": 10,
        "token": 33
      },
      {
        "text": "Jacob Steinhardt. Measuring massive multitask language understanding.\narXiv:2009.03300, 2020.\n",
        "font-size": 10,
        "token": 33
      },
      {
        "text": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n",
        "font-size": 10,
        "token": 73
      },
      {
        "text": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n",
        "font-size": 10,
        "token": 85
      },
      {
        "text": "Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\n Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting\n llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.\n",
        "font-size": 10,
        "token": 160
      },
      {
        "text": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021.\n",
        "font-size": 10,
        "token": 76
      },
      {
        "text": "Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra K¨ubler, and Lawrence S. Moss. OCNLI:\noriginal chinese natural language inference. In Trevor Cohn, Yulan He, and Yang Liu (eds.),\nFindings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20\nNovember 2020, volume EMNLP 2020 of Findings of ACL, pp. 3512–3526. Association for\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.314. URL https:\n//doi.org/10.18653/v1/2020.findings-emnlp.314.\n",
        "font-size": 10,
        "token": 190
      },
      {
        "text": "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-Eval: A multi-level multi-discipline chinese\nevaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n 26\n",
        "font-size": 10,
        "token": 101
      }
    ]
  },
  {
    "page_num": 27,
    "content": [
      {
        "text": "Hugging Face.\n Transformers agents, 2023. URL https://huggingface.co/docs/\n transformers/transformers_agents.\n Baichuan Inc. Baichuan-7B: A large-scale 7B pretraining language model developed by BaiChuan-\n Inc, 2023a. URL https://github.com/baichuan-inc/Baichuan-7B.\n XVERSE Technology Inc.\n",
        "font-size": 10,
        "token": 88
      },
      {
        "text": "large language model devel-\nXVERSE-13B: A multilingual\noped by XVERSE Technology Inc., 2023b. URL https://github.com/xverse-ai/\nXVERSE-13B.\n InternLM Team. InternLM: A multilingual language model with progressively enhanced capabilities,\n 2023. URL https://github.com/InternLM/InternLM.\n Shantanu Jain. tiktoken: A fast BPE tokeniser for use with OpenAI’s models, 2022. URL https:\n //github.com/openai/tiktoken/.\n",
        "font-size": 10,
        "token": 122
      },
      {
        "text": "Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang\nLi. Exploring the impact of instruction data scaling on large language models: An empirical study\non real-world use cases. arXiv preprint arXiv:2303.14742, 2023.\n",
        "font-size": 10,
        "token": 84
      },
      {
        "text": "Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\ntransformers: Equivalent and efficient pre-LN transformers. CoRR, abs/2305.14858, 2023. doi:\n10.48550/arXiv.2305.14858. URL https://doi.org/10.48550/arXiv.2305.14858.\n Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n arXiv:1412.6980, 2014.\n",
        "font-size": 10,
        "token": 162
      },
      {
        "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput.\nLinguistics, 7:452–466, 2019. doi: 10.1162/tacl\\ a\\ 00276. URL https://doi.org/10.\n1162/tacl_a_00276.\n",
        "font-size": 10,
        "token": 177
      },
      {
        "text": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\n LangChain, Inc. LangChain: Building applications with LLMs through composability, 2023. URL\n https://python.langchain.com/.\n",
        "font-size": 10,
        "token": 116
      },
      {
        "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n",
        "font-size": 10,
        "token": 95
      },
      {
        "text": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language\nmodels, 2022.\n",
        "font-size": 10,
        "token": 96
      },
      {
        "text": "Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang,\nWenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable\nagent system with open-source large language models. arXiv preprint arXiv:2309.00986, 2023a.\n",
        "font-size": 10,
        "token": 93
      },
      {
        "text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for “mind” exploration of large scale language model society. arXiv\npreprint arXiv:2303.17760, 2023b.\n",
        "font-size": 10,
        "token": 78
      },
      {
        "text": "Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv\npreprint arXiv:2306.09212, 2023c.\n 27\n",
        "font-size": 10,
        "token": 86
      }
    ]
  },
  {
    "page_num": 28,
    "content": [
      {
        "text": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\nTerry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo˜ao\nMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\nLogesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan\nDey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee,\nNadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank\nMishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜noz Ferrandis,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder:\nMay the source be with you! CoRR, abs/2305.06161, 2023d. doi: 10.48550/arXiv.2305.06161.\nURL https://doi.org/10.48550/arXiv.2305.06161.\n",
        "font-size": 10,
        "token": 454
      },
      {
        "text": "Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,\nCyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,\nSven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with AlphaCode. CoRR, abs/2203.07814, 2022.\n",
        "font-size": 10,
        "token": 176
      },
      {
        "text": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\n",
        "font-size": 10,
        "token": 80
      },
      {
        "text": "Chenxiao Liu and Xiaojun Wan. CodeQA: A question answering dataset for source code com-\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau\nprehension.\nYih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2618–2632. Associa-\ntion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.223. URL\nhttps://doi.org/10.18653/v1/2021.findings-emnlp.223.\n Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\n preprint arXiv:2304.08485, 2023a.\n",
        "font-size": 10,
        "token": 239
      },
      {
        "text": "Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong,\nand Jie Tang. WebGLM: Towards an efficient web-enhanced question answering system with\nhuman preferences. arXiv preprint arXiv:2306.07906, 2023b.\n",
        "font-size": 10,
        "token": 85
      },
      {
        "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n",
        "font-size": 10,
        "token": 91
      },
      {
        "text": "Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach Dinh\nLe, and David Lo. Refining ChatGPT-generated code: Characterizing and mitigating code\nquality issues. CoRR, abs/2307.12596, 2023c. doi: 10.48550/arXiv.2307.12596. URL https:\n//doi.org/10.48550/arXiv.2307.12596.\n",
        "font-size": 10,
        "token": 142
      },
      {
        "text": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The Flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\n Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n arXiv preprint\n arXiv:1711.05101, 2017.\n",
        "font-size": 10,
        "token": 132
      },
      {
        "text": "Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language\nmodels. CoRR, abs/2308.07074, 2023. doi: 10.48550/arXiv.2308.07074. URL https://doi.\norg/10.48550/arXiv.2308.07074.\n 28\n",
        "font-size": 10,
        "token": 134
      }
    ]
  },
  {
    "page_num": 29,
    "content": [
      {
        "text": "Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,\nQingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,\n2023a.\n",
        "font-size": 10,
        "token": 92
      },
      {
        "text": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with\nevol-instruct. arXiv preprint arXiv:2306.08568, 2023b.\n Mosaic ML. MPT-30B: Raising the bar for open-source foundation models, 2023. URL https:\n //www.mosaicml.com/blog/mpt-30b.\n",
        "font-size": 10,
        "token": 131
      },
      {
        "text": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual\ngeneralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\n",
        "font-size": 10,
        "token": 98
      },
      {
        "text": "Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction tuning\ncode large language models. CoRR, abs/2308.07124, 2023.\n",
        "font-size": 10,
        "token": 92
      },
      {
        "text": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n",
        "font-size": 10,
        "token": 91
      },
      {
        "text": "Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus\nOdena. Show your work: Scratchpads for intermediate computation with language models. ArXiv,\nabs/2112.00114, 2021.\n OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.\n OpenAI. ChatML, 2022. URL https://github.com/openai/openai-python/blob/\n /chatml.md.\n OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n OpenCompass Team. OpenCompass: A universal evaluation platform for foundation models, 2023.\n URL https://opencompass.org.cn/leaderboard-llm.\n",
        "font-size": 10,
        "token": 222
      },
      {
        "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser\nKelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback. In\nNeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n-Abstract-Conference.html.\n",
        "font-size": 10,
        "token": 138
      },
      {
        "text": "Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,\nVolume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/\np16-1144. URL https://doi.org/10.18653/v1/p16-1144.\n Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\n extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.\n",
        "font-size": 10,
        "token": 240
      },
      {
        "text": "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023b.\n Qwen Team, Alibaba Cloud. Evaluation benchmark for code intepreter, 2023a. URL https:\n //github.com/QwenLM/Qwen-Agent/tree/main/benchmark.\n 29\n",
        "font-size": 10,
        "token": 119
      }
    ]
  },
  {
    "page_num": 30,
    "content": [
      {
        "text": "Qwen Team, Alibaba Cloud. Evaluation benchmark for tool usage throught ReAct prompting, 2023b.\n URL https://github.com/QwenLM/Qwen-7B/tree/main/eval.\n",
        "font-size": 10,
        "token": 43
      },
      {
        "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\nunderstanding by generative pre-training. Technical report, OpenAI, 2018.\n Improving language\n",
        "font-size": 10,
        "token": 52
      },
      {
        "text": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n",
        "font-size": 10,
        "token": 84
      },
      {
        "text": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\n",
        "font-size": 10,
        "token": 69
      },
      {
        "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\n Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\n arXiv:1710.05941, 2017.\n",
        "font-size": 10,
        "token": 137
      },
      {
        "text": "Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio G´omez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol\nVinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Trans. Mach. Learn. Res.,\n2022, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.\n",
        "font-size": 10,
        "token": 146
      },
      {
        "text": "Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J´er´emy Rapin, et al. Code Llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\n",
        "font-size": 10,
        "token": 97
      },
      {
        "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables\nzero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\n",
        "font-size": 10,
        "token": 98
      },
      {
        "text": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Com-\nmonsense reasoning about social interactions. CoRR, abs/1904.09728, 2019. URL http:\n//arxiv.org/abs/1904.09728.\n",
        "font-size": 10,
        "token": 77
      },
      {
        "text": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, et al. BLOOM: A 176B-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n",
        "font-size": 10,
        "token": 100
      },
      {
        "text": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\n John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n",
        "font-size": 10,
        "token": 185
      },
      {
        "text": "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\ngingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace. arXiv preprint\narXiv:2303.17580, 2023.\n",
        "font-size": 10,
        "token": 79
      },
      {
        "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\n",
        "font-size": 10,
        "token": 78
      },
      {
        "text": "Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. Alpaca-CoT: An instruction-tuning\nplatform with unified interface of instruction collection, parameter-efficient methods, and large\nlanguage models, 2023. URL https://github.com/PhoebusSi/alpaca-CoT.\n 30\n",
        "font-size": 10,
        "token": 77
      }
    ]
  },
  {
    "page_num": 31,
    "content": [
      {
        "text": "Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\n Stability AI. StableBeluga2, 2023. URL https://huggingface.co/stabilityai/\n StableBeluga2.\n Jianlin Su. Improving transformer: Length extrapolation ability and position robustness, 2023a. URL\n https://spaces.ac.cn/archives/9444.\n Jianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023b.\n URL https://spaces.ac.cn/archives/9577.\n Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\n transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n",
        "font-size": 10,
        "token": 241
      },
      {
        "text": "Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu,\nYunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun\nZhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang,\nand Xipeng Qiu. MOSS: Training conversational language models from synthetic data, 2023a.\n",
        "font-size": 10,
        "token": 122
      },
      {
        "text": "Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming\nYang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with\nminimal human supervision. arXiv preprint arXiv:2305.03047, 2023b.\n",
        "font-size": 10,
        "token": 80
      },
      {
        "text": "Mirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n",
        "font-size": 10,
        "token": 98
      },
      {
        "text": "Marc Szafraniec, Baptiste Rozi`ere, Hugh Leather, Patrick Labatut, Franc¸ois Charton, and Gabriel\nSynnaeve. Code translation with compiler representations. In The Eleventh International Confer-\nence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023. URL https://openreview.net/pdf?id=XomEU3eNeSQ.\n",
        "font-size": 10,
        "token": 106
      },
      {
        "text": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question\nanswering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and\nThamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4149–\n4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL\nhttps://doi.org/10.18653/v1/n19-1421.\n",
        "font-size": 10,
        "token": 194
      },
      {
        "text": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model,\n2023. URL https://github.com/tatsu-lab/stanford_alpaca.\n",
        "font-size": 10,
        "token": 81
      },
      {
        "text": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science,\n2022.\n",
        "font-size": 10,
        "token": 62
      },
      {
        "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny\nSoraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-\nson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena\nButryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein,\nRay Kurzweil, Blaise Ag¨uera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le.\nLaMDA: Language models for dialog applications. CoRR, abs/2201.08239, 2022. URL\nhttps://arxiv.org/abs/2201.08239.\n 31\n",
        "font-size": 10,
        "token": 353
      }
    ]
  },
  {
    "page_num": 32,
    "content": [
      {
        "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n",
        "font-size": 10,
        "token": 101
      },
      {
        "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur´elien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\nCoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/\n10.48550/arXiv.2307.09288.\n",
        "font-size": 10,
        "token": 448
      },
      {
        "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\n",
        "font-size": 10,
        "token": 72
      },
      {
        "text": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023a.\n",
        "font-size": 10,
        "token": 83
      },
      {
        "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-\nconsistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171,\n2022.\n Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems.\n",
        "font-size": 10,
        "token": 84
      },
      {
        "text": "In\nConference on Empirical Methods in Natural Language Processing, 2017. URL https://api.\nsemanticscholar.org/CorpusID:910689.\n",
        "font-size": 10,
        "token": 40
      },
      {
        "text": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nExploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751,\n2023b.\n",
        "font-size": 10,
        "token": 102
      },
      {
        "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In\nAnna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023, pp. 13484–13508. Association for Computational Linguistics,\n2023c. doi: 10.18653/v1/2023.acl-long.754. URL https://doi.org/10.18653/v1/\n2023.acl-long.754.\n",
        "font-size": 10,
        "token": 212
      },
      {
        "text": "Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation. arXiv preprint\narXiv:2109.00859, 2021.\n",
        "font-size": 10,
        "token": 68
      },
      {
        "text": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H.\nHoi. CodeT5+: Open code large language models for code understanding and generation. CoRR,\nabs/2305.07922, 2023d. doi: 10.48550/arXiv.2305.07922. URL https://doi.org/10.\n48550/arXiv.2305.07922.\n",
        "font-size": 10,
        "token": 131
      },
      {
        "text": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=\ngEZrGCozdqR.\n 32\n",
        "font-size": 10,
        "token": 120
      }
    ]
  },
  {
    "page_num": 33,
    "content": [
      {
        "text": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.\nTrans. Mach. Learn. Res., 2022, 2022b. URL https://api.semanticscholar.org/\nCorpusID:249674500.\n",
        "font-size": 10,
        "token": 132
      },
      {
        "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022c.\n",
        "font-size": 10,
        "token": 84
      },
      {
        "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. HuggingFace’s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n",
        "font-size": 10,
        "token": 93
      },
      {
        "text": "Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\nMao. ExpertPrompting: Instructing large language models to be distinguished experts. arXiv\npreprint arXiv:2305.14688, 2023a.\n",
        "font-size": 10,
        "token": 74
      },
      {
        "text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. WizardLM: Empowering large language models to follow complex instructions. arXiv\npreprint arXiv:2304.12244, 2023b.\n Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with\n parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023c.\n",
        "font-size": 10,
        "token": 141
      },
      {
        "text": "Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\nExploring large language models for communication games: An empirical study on werewolf.\narXiv preprint arXiv:2309.04658, 2023d.\n",
        "font-size": 10,
        "token": 74
      },
      {
        "text": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,\nDong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou\nZhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang,\nLei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin,\nNuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng\nChen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun\nShen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying\nWu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023.\nURL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.\npdf.\n Jianxin Yang. Firefly. https://github.com/yangjianxin1/Firefly, 2023.\n",
        "font-size": 10,
        "token": 307
      },
      {
        "text": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\n",
        "font-size": 10,
        "token": 73
      },
      {
        "text": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models\nwith multimodality. arXiv preprint arXiv:2304.14178, 2023.\n",
        "font-size": 10,
        "token": 92
      },
      {
        "text": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models, 2023.\n",
        "font-size": 10,
        "token": 66
      },
      {
        "text": "Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,\nand Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language\nmodels, 2023a.\n Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large\n language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023b.\n Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank\n responses to align language models with human feedback without tears, 2023c.\n 33\n",
        "font-size": 10,
        "token": 163
      }
    ]
  },
  {
    "page_num": 34,
    "content": [
      {
        "text": "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMAmmoTH: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\n",
        "font-size": 10,
        "token": 73
      },
      {
        "text": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez\n(eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791–4800.\nAssociation for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https:\n//doi.org/10.18653/v1/p19-1472.\n",
        "font-size": 10,
        "token": 186
      },
      {
        "text": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\n",
        "font-size": 10,
        "token": 88
      },
      {
        "text": "Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\nChen. RepoCoder: Repository-level code completion through iterative retrieval and generation.\nCoRR, abs/2303.12570, 2023a. doi: 10.48550/arXiv.2303.12570. URL https://doi.org/\n10.48550/arXiv.2303.12570.\n",
        "font-size": 10,
        "token": 127
      },
      {
        "text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\n",
        "font-size": 10,
        "token": 83
      },
      {
        "text": "Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating\nthe performance of large language models on GAOKAO benchmark. CoRR, abs/2305.12474,\n2023b. doi: 10.48550/arXiv.2305.12474. URL https://doi.org/10.48550/arXiv.\n2305.12474.\n",
        "font-size": 10,
        "token": 121
      },
      {
        "text": "Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for\ncode generation with multilingual evaluations on humaneval-x. CoRR, abs/2303.17568, 2023. doi:\n10.48550/arXiv.2303.17568. URL https://doi.org/10.48550/arXiv.2303.17568.\n",
        "font-size": 10,
        "token": 152
      },
      {
        "text": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models.\nCoRR, abs/2304.06364, 2023a. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/\n10.48550/arXiv.2304.06364.\n Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. MemoryBank: Enhancing large\n language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023b.\n",
        "font-size": 10,
        "token": 204
      },
      {
        "text": "Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex\nreasoning in large language models. ArXiv, abs/2205.10625, 2022.\n 34\n",
        "font-size": 10,
        "token": 91
      }
    ]
  },
  {
    "page_num": 35,
    "content": [
      {
        "text": "A APPENDIX\n",
        "font-size": 12,
        "token": 5
      },
      {
        "text": "A.1 MORE TRAINING DETAILS\n A.1.1 DATA FORMAT FOR QWEN-CHAT\n",
        "font-size": 10,
        "token": 22
      },
      {
        "text": "Different from conventional pretraining based on autoregressive next-token prediction, despite using\na similar training task, there should be a specially design data format for SFT and RLHF to build\na conversational AI assistant model. Common formats include “human-assistant” and ChatML\nformats. As to our knowledge, the earliest example of the human-assistant format comes from\nAnthropic (Bai et al., 2022b), which adds a special phrase “\\n\\nhuman: ” in front of the user\ninput and “\\n\\nassistant: ” in front of the assistant response. It is easy for the base language\nmodel to transfer to the pattern of conversational AI. However, as the specific phrases are common\nwords, it might be hard for the model to disambiguate from these words in other contexts.\n",
        "font-size": 10,
        "token": 175
      },
      {
        "text": "Instead, we turned to the ChatML format proposed by OpenAI.5 This format allows the use of special\ntokens, i.e., “<im_start>” and “<im_end>”, that do not appear in pretraining, and thus resolve\nthe aforementioned problem. We demonstrate an example of the format below.\n",
        "font-size": 10,
        "token": 66
      },
      {
        "text": "ChatML Format\n",
        "font-size": 12,
        "token": 4
      },
      {
        "text": "<| i m s t a r t |> s y s t e m\nYou a r e a h e l p f u l\n<| i m s t a r t |> u s e r\nH e l l o ! <| i m e n d |>\n<| i m s t a r t |> a s s i s t a n t\nH e l l o ! How c a n I\n a s s i s t a n t . <| i m e n d |>\n a s s i s t you t o d a y ? <| i m e n d |>\n",
        "font-size": 9,
        "token": 130
      },
      {
        "text": "A.2 EVALUATION\n",
        "font-size": 8,
        "token": 7
      },
      {
        "text": "A.2.1 AUTOMATIC EVALUATION\n",
        "font-size": 8,
        "token": 11
      },
      {
        "text": "To provide a whole picture of the performance of our model series QWEN, here in this section we\nillustrate the detailed performance of our models as well as the baselines in the comprehensive\nbenchmark evaluation proposed by OpenCompass Team (2023). We report the results in multiple\ntables based on the officially provided categories, including examination, language, knowledge,\nunderstanding, and reasoning. In terms of the performance of the baseline models, we report the\nhigher results between the reported ones and those on the leaderboard.\n",
        "font-size": 10,
        "token": 108
      },
      {
        "text": "Examination Here we evaluate the models on a series of datasets relevant to the examination. The\ndatasets include\n • MMLU (Hendrycks et al., 2020) Massive Multi-task Language Understanding is designed\n for measuring language understanding capabilities. We report 5-shot results.\n • C-Eval (Huang et al., 2023) C-Eval is a Chinese evaluation dataset spanning 52 diverse\n disciplines. We report 5-shot results.\n • CMMLU (Li et al., 2023c) CMMLU is designed for assessing language understanding\n capabilities in Chinese. We report 5-shot results.\n",
        "font-size": 10,
        "token": 137
      },
      {
        "text": "• AGIEval (Zhong et al., 2023a) This is a benchmark consisting of human-centric examina-\ntions, including college entrance exams, law school admission tests, math competitions, and\nlawyer qualification tests. We report zero-shot results.\n • Gaokao-Bench (Zhang et al., 2023b) This is a benchmark with Gaokao (Chinese college-\n entrance examination) questions. We report zero-shot results.\n",
        "font-size": 10,
        "token": 99
      },
      {
        "text": "• ARC (Clark et al., 2018) ARC is a dataset consisting of grade-school level, multiple-choice\nscience questions. It includes an easy set and a challenge set, which are referred by ARC-e\nand ARC-c. We report zero-shot results.\n 35\n",
        "font-size": 10,
        "token": 60
      }
    ]
  },
  {
    "page_num": 36,
    "content": [
      {
        "text": "Table 13: Results on MMLU. All are tested with five-shot accuracy. We provide the reported results\nof the other models for comparison.\n Model\n Params Average\n STEM Social Sciences Humanities Others\n MPT\n Falcon\n ChatGLM2\n InternLM\n Baichuan2\n XVERSE\n LLaMA\n LLAMA 2\n QWEN\n",
        "font-size": 9,
        "token": 77
      },
      {
        "text": "7B\n30B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n40B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "6B\n12B\n 7B\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "7B\n13B\n 13B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "7B\n13B\n33B\n65B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "7B\n13B\n34B\n70B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "1.8B\n7B\n14B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "26.8\n46.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.2\n55.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "47.9\n56.2\n 51.0\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "54.2\n59.2\n 55.1\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "35.1\n46.9\n57.8\n63.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "45.3\n54.8\n62.6\n68.9\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "44.6\n58.2\n66.3\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "25.3\n39.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.2\n45.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "41.2\n48.2\n -\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "-\n-\n 44.5\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "30.5\n35.8\n46.0\n51.7\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "36.4\n44.1\n52.1\n58.0\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "39.6\n50.2\n59.4\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "27.1\n52.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "24.7\n65.4\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.4\n65.1\n -\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "-\n-\n 64.4\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "38.3\n53.8\n66.7\n72.9\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "51.2\n62.6\n71.8\n80.3\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "50.0\n68.6\n76.2\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "26.7\n44.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.4\n49.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "43.7\n52.6\n -\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "-\n-\n 50.5\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "34.0\n45.0\n55.8\n61.8\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "42.9\n52.8\n59.4\n65.0\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "40.4\n52.5\n60.9\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "28.2\n52.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "27.4\n65.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.5\n60.9\n -\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "-\n-\n 62.9\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "38.1\n53.3\n63.4\n67.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "52.2\n61.1\n69.2\n74.6\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "51.0\n64.9\n71.8\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "Table 14: Leaderboard results of C-Eval. We include the results of both proprietary models and\nopen-source models. Note that there are a number of models on the leaderboard with very few\ndetails, in terms of proprietary models, we only report the results of GPT-3.5, GPT-4, InternLM and\nChatGLM2.\n Model\n Params Avg. Avg. (Hard)\n STEM Social Sciences Humanities Others\n GPT-3.5\n GPT-4\n -\n -\n InternLM\n 123B\n ChatGLM2\n -\n ChatGLM2\n InternLM\n Baichuan2\n XVERSE\n QWEN\n 6B\n 7B\n",
        "font-size": 9,
        "token": 150
      },
      {
        "text": "7B\n13B\n 13B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "1.8B\n7B\n14B\n 54.4\n 68.7\n 68.8\n 71.1\n 51.7\n 52.8\n",
        "font-size": 9,
        "token": 48
      },
      {
        "text": "54.0\n58.1\n 54.7\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "54.7\n63.5\n72.1\n Proprietary models\n 41.4\n 54.9\n 50.0\n 50.0\n 52.9\n 67.1\n 63.5\n 64.4\n Open-source models\n 37.1\n 37.1\n",
        "font-size": 9,
        "token": 84
      },
      {
        "text": "-\n-\n 33.5\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "41.8\n46.4\n53.7\n 48.6\n 48.0\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "-\n-\n 45.6\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "50.8\n57.7\n65.7\n 61.8\n 77.6\n 81.4\n 81.6\n 60.5\n 67.4\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "-\n-\n 66.2\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "69.9\n78.1\n85.4\n 50.9\n 64.5\n 72.7\n 73.7\n 51.3\n 55.4\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "-\n-\n 58.3\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "56.3\n66.6\n75.3\n 53.6\n 67.8\n 63.0\n 71.3\n 49.8\n 45.8\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "-\n-\n 56.9\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "46.2\n57.8\n68.4\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "In terms of MMLU, we report the detailed results in Table 13. In terms of C-Eval, we report the results\nin Table 14. For the rest of the datasets, we report the results in Table 15. Note that AGIEval includes\n",
        "font-size": 10,
        "token": 60
      },
      {
        "text": "5https://github.com/openai/openai-python/blob/main/chatml.md\n",
        "font-size": 6,
        "token": 16
      },
      {
        "text": "36\n",
        "font-size": 10,
        "token": 3
      }
    ]
  },
  {
    "page_num": 37,
    "content": [
      {
        "text": "Table 15: Results on the other datasets of examination. Specifically, we report the results on\nCMMLU, AGIEval, ARC-e, and ARC-c.\n Model\n MPT\n Falcon\n ChatGLM2\n InternLM\n Baichuan2\n LLaMA\n LLAMA 2\n StableBeluga2\n QWEN\n Params CMMLU AGIEval Gaokao-Bench ARC-e ARC-c\n 7B\n 7B\n 6B\n",
        "font-size": 9,
        "token": 103
      },
      {
        "text": "7B\n20B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n33B\n65B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "7B\n13B\n70B\n 70B\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "1.8B\n7B\n14B\n 25.9\n -\n 49.3\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "51.8\n59.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "57.1\n62.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "26.8\n31.5\n36.0\n40.6\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "31.8\n38.4\n53.6\n 51.8\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "49.3\n62.2\n71.0\n 21.3\n -\n 39.0\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "36.9\n44.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "42.7\n48.2\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "20.6\n22.0\n33.5\n33.9\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "21.8\n30.9\n40.2\n 41.6\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "36.9\n45.8\n52.3\n 19.8\n -\n 46.4\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "43.0\n45.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "47.5\n54.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "21.3\n20.4\n18.9\n19.1\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "18.9\n18.2\n23.3\n 40.9\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "44.9\n52.5\n61.9\n 70.2\n 70.0\n 73.0\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "78.7\n86.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "54.7\n61.9\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "72.8\n74.8\n80.0\n80.6\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "75.2\n77.3\n85.9\n 91.2\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "71.6\n84.0\n90.3\n 42.6\n 42.4\n 61.0\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "69.5\n81.7\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "32.5\n38.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "47.6\n52.7\n67.5\n69.5\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "45.9\n60.3\n78.3\n 86.1\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "53.2\n75.3\n84.4\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "the parts of Chinese and English, while LLAMA 2 only reported the results in the English part, so we\nuse the results on OpenCompass. Additionally, while CMMLU, AGIEval, and Gaokao-Bench are\nrelated to Chinese, and MPT, Falcon, and the LLaMA series were not optimized for Chinese, these\nmodels achieved low performance on the datasets.\n",
        "font-size": 10,
        "token": 83
      },
      {
        "text": "Knowledge and Understanding Here we evaluate the models on a series of datasets relevant to\nknowledge and natural language understanding. The datasets include\n",
        "font-size": 10,
        "token": 26
      },
      {
        "text": "• BoolQ (Clark et al., 2019) This is a QA dataset, where the questions are about passages of\nWikipedia, and the model should answer yes or no to the given possible answer. We report\nzero-shot results.\n",
        "font-size": 10,
        "token": 52
      },
      {
        "text": "• CommonsenseQA (Talmor et al., 2019) This is a dataset of multiple-choice question\nanswering that asseses the understanding of commonsense knowledge. We report 8-shot\nresults.\n • NaturalQuestions (Kwiatkowski et al., 2019) It is a dataset of QA where the questions are\n from users and the answers are verified by experts. We report zero-shot results.\n",
        "font-size": 10,
        "token": 91
      },
      {
        "text": "• LAMBADA (Paperno et al., 2016) This is dataset to evaluate language understanding by\nword prediction. It consists of passages related to human subjects. We report zero-shot\nresults.\n We report the results in Table 16.\n",
        "font-size": 10,
        "token": 55
      },
      {
        "text": "Reasoning We report the evaluation results on the datasets concerning reasoning, focusing on\nnatural language reasoning. For the others, such as mathematics and coding, as we have illustrated\ndetailed results, here we do not report those results repeatedly. The datasets for evaluation include:\n",
        "font-size": 10,
        "token": 54
      },
      {
        "text": "• HellaSwag (Zellers et al., 2019) This is a commonsense natural language inference (NLI)\ndataset, where the questions are easy for humans but struggling for previous language\nmodels. We report zero-shot results.\n • PIQA (Bisk et al., 2020) This is an NLI dataset assessing the physical knowledge. We report\n zero-shot results.\n 37\n",
        "font-size": 10,
        "token": 89
      }
    ]
  },
  {
    "page_num": 38,
    "content": [
      {
        "text": "Table 16: Results on the datasets concerning knowledge and understanding. Specifically, we\nreport the results on BoolQ, CommonsenseQA, NaturalQuestions, and LAMBADA.\n Model\n MPT\n",
        "font-size": 9,
        "token": 42
      },
      {
        "text": "Falcon\nChatGLM2\n InternLM\n XVERSE\n Baichuan2\n LLaMA\n LLAMA 2\n StableBeluga2\n QWEN\n Params\n BoolQ CommonsenseQA NaturalQuestions LAMBADA\n 7B\n 7B\n 6B\n",
        "font-size": 9,
        "token": 62
      },
      {
        "text": "7B\n20B\n 13B\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n33B\n65B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "7B\n13B\n70B\n 70B\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "1.8B\n7B\n14B\n 75.0\n 67.5\n 79.0\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "64.1\n87.5\n 64.2\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "63.2\n67.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "76.5\n78.7\n84.4\n86.6\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "77.4\n82.4\n87.7\n 89.4\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "68.0\n76.4\n86.2\n 61.8\n 20.8\n 65.4\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "59.8\n70.6\n 62.2\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "63.0\n65.6\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "64.9\n67.4\n72.5\n74.1\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "66.5\n67.3\n78.5\n 72.6\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "60.1\n66.8\n70.3\n 11.6\n 15.7\n 9.7\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "8.9\n25.2\n 0.3\n",
        "font-size": 9,
        "token": 14
      },
      {
        "text": "9.4\n16.3\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "16.8\n20.2\n30.9\n33.4\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "19.1\n24.9\n34.2\n 25.1\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "3.2\n17.4\n23.9\n 70.0\n -\n 54.3\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "67.0\n71.8\n 48.2\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "73.3\n74.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "73.3\n75.2\n77.2\n77.7\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "73.3\n76.5\n78.9\n 71.3\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "58.4\n67.9\n71.1\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "Table 17: Results on the datasets related to natural language reasoning. Specifically, we report\nthe results on HellaSwag, PIQA, SIQA, and OCNLI.\n Model\n MPT\n Falcon\n ChatGLM2\n InternLM\n Baichuan2\n LLaMA\n LLAMA 2\n StableBeluga2\n",
        "font-size": 9,
        "token": 73
      },
      {
        "text": "QWEN\n",
        "font-size": 7,
        "token": 4
      },
      {
        "text": "Params HellaSwag\n PIQA SIQA OCNLI\n 7B\n 7B\n 6B\n",
        "font-size": 9,
        "token": 26
      },
      {
        "text": "7B\n20B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "7B\n13B\n33B\n65B\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "7B\n13B\n70B\n 70B\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "1.8B\n7B\n14B\n 76.4\n 74.1\n 57.0\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "70.6\n78.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "67.0\n70.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "76.1\n79.2\n82.8\n84.2\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "77.2\n80.7\n85.3\n 84.1\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "56.7\n75.1\n80.2\n 80.6\n 76.7\n 69.6\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "77.9\n80.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "76.2\n78.1\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "79.8\n80.1\n82.3\n82.8\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "78.8\n80.5\n82.8\n 83.3\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "73.3\n77.9\n79.9\n 48.5\n 47.2\n 64.3\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "60.5\n72.8\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "44.4\n44.3\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "48.9\n52.5\n57.8\n61.2\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "48.5\n54.8\n64.8\n 78.1\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "56.1\n69.9\n77.9\n 30.0\n -\n 33.1\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "37.5\n42.5\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "30.3\n30.0\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "33.6\n32.1\n30.7\n44.9\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "32.1\n34.1\n46.5\n 48.3\n",
        "font-size": 9,
        "token": 21
      },
      {
        "text": "39.0\n47.4\n57.9\n • SIQA (Sap et al., 2019) This is an NLI dataset evaluating social commonsense intelligence.\n We report zero-shot results.\n • OCNLI (Hu et al., 2020) This is an NLI dataset focusing on Chinese. We report zero-shot\n results.\n 38\n",
        "font-size": 10,
        "token": 84
      }
    ]
  },
  {
    "page_num": 39,
    "content": [
      {
        "text": "We report the results in Table 17.\n 39\n",
        "font-size": 10,
        "token": 14
      }
    ]
  },
  {
    "page_num": 40,
    "content": [
      {
        "text": "A.2.2 HUMAN EVALUATION\n",
        "font-size": 10,
        "token": 11
      },
      {
        "text": "In this section, we demonstrate the cases of human analysis. In our self-constructed evaluation dataset,\nthe instructions are either manually written data or manual revised from public datasets, such as\nCLiB6, C-Eval (Huang et al., 2023), FacTool (Chern et al., 2023b), LeetCode7), etc.\n",
        "font-size": 10,
        "token": 78
      },
      {
        "text": "In terms of each case, we demonstrate the responses and Elo ratings8 of all models for comparison.\nSpecifically, as the data in our human evaluation are in Chinese, we also provide their translations in\nEnglish.\n",
        "font-size": 10,
        "token": 43
      },
      {
        "text": "Knowledge\n北京大学的前身是什么学校？\nWhat was the predecessor of Peking University?\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Elo\n1037\n 1011\n 1090\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 北京大学的前身是京师大学堂，它创建于1898年，是中国近代\n Response\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "第一所国立大学，也是最早以“大学” 命名的高等学府。\nThe predecessor of Peking University was the Imperial University\nof Peking, established in 1898. It was the first national university in\nmodern China and also the earliest higher education institution to be\nnamed a “university”.\n",
        "font-size": 9,
        "token": 70
      },
      {
        "text": "Qwen-14B-Chat (SFT) 北京大学的前身是京师大学堂，创建于1898年，是中国近代第\n",
        "font-size": 9,
        "token": 36
      },
      {
        "text": "一所国立大学。\nThe predecessor of Peking University is the Imperial University of\nPeking, founded in 1898 and the first national university in modern\nChina.\n",
        "font-size": 9,
        "token": 36
      },
      {
        "text": "Qwen-14B-Chat (RLHF)北京大学的前身是京师大学堂，创建于1898年，是中国近代第\n一所国立综合性大学。京师大学堂最初设立的学科有经学、文\n学、史学、哲学、法律、教育、农学、工学、商学、医学等，\n后来逐渐扩大到社会科学、自然科学、工程技术、艺术等多个\n领域。1912年，京师大学堂改名为北京大学，成为中国最高学\n府之一，也是中国现代高等教育的发源地之一。\nPeking University, originally known as the Imperial University of\nPeking, was founded in 1898 and is the first comprehensive national\nuniversity in modern China. The initial disciplines established at\nthe Imperial University of Peking included classics, literature, his-\ntory, philosophy, law, education, agriculture, engineering, business,\nmedicine, etc., gradually expanding to encompass social sciences,\nnatural sciences, engineering and technology, arts, and other fields. In\n1912, the Imperial University of Peking changed its name to Peking\nUniversity, becoming one of the top universities in China and also\none of the birthplaces of modern higher education in China.\n GPT-3.5\n",
        "font-size": 9,
        "token": 267
      },
      {
        "text": "北京大学的前身是国立北京大学，建于1898年，是中国近代第\n一所国立大学。其成立初衷是为了培养国家的政治、经济、军\n事和外交人才，为中国的现代化进程做出贡献。\nThe predecessor of Peking University was National Peking University,\nestablished in 1898. It is the first national university in modern China.\nIts original purpose was cultivating talents in politics, economics,\nmilitary, and diplomacy and contributing to China’s modernization\nprocess.\n 910\n",
        "font-size": 9,
        "token": 109
      },
      {
        "text": "6https://github.com/jeinlee1991/chinese-llm-benchmark\n7https://leetcode.cn/\n8https://en.wikipedia.org/wiki/Elo rating system\n",
        "font-size": 6,
        "token": 39
      },
      {
        "text": "40\n",
        "font-size": 10,
        "token": 3
      },
      {
        "table": "|北京大学的前身是什么学校？ WhatwasthepredecessorofPekingUniversity?|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 北京大学的前身是京师大学堂，它创建于1898年，是中国近代 1037 第一所国立大学，也是最早以“大学”命名的高等学府。 ThepredecessorofPekingUniversitywastheImperialUniversity ofPeking,establishedin1898.Itwasthefirstnationaluniversityin modernChinaandalsotheearliesthighereducationinstitutiontobe nameda“university”. Qwen-14B-Chat(SFT) 北京大学的前身是京师大学堂，创建于1898年，是中国近代第 1011 一所国立大学。 ThepredecessorofPekingUniversityistheImperialUniversityof Peking,foundedin1898andthefirstnationaluniversityinmodern China. Qwen-14B-Chat(RLHF)北京大学的前身是京师大学堂，创建于1898年，是中国近代第 1090 一所国立综合性大学。京师大学堂最初设立的学科有经学、文 学、史学、哲学、法律、教育、农学、工学、商学、医学等， 后来逐渐扩大到社会科学、自然科学、工程技术、艺术等多个 领域。1912年，京师大学堂改名为北京大学，成为中国最高学 府之一，也是中国现代高等教育的发源地之一。 PekingUniversity,originallyknownastheImperialUniversityof Peking,wasfoundedin1898andisthefirstcomprehensivenational universityinmodernChina. Theinitialdisciplinesestablishedat theImperialUniversityofPekingincludedclassics,literature,his- tory,philosophy,law,education,agriculture,engineering,business, medicine,etc.,graduallyexpandingtoencompasssocialsciences, naturalsciences,engineeringandtechnology,arts,andotherfields.In 1912,theImperialUniversityofPekingchangeditsnametoPeking University,becomingoneofthetopuniversitiesinChinaandalso oneofthebirthplacesofmodernhighereducationinChina. GPT-3.5 北京大学的前身是国立北京大学，建于1898年，是中国近代第 910 一所国立大学。其成立初衷是为了培养国家的政治、经济、军 事和外交人才，为中国的现代化进程做出贡献。 ThepredecessorofPekingUniversitywasNationalPekingUniversity, establishedin1898.ItisthefirstnationaluniversityinmodernChina. Itsoriginalpurposewascultivatingtalentsinpolitics,economics, military,anddiplomacyandcontributingtoChina’smodernization process.|",
        "token": 623
      }
    ]
  },
  {
    "page_num": 41,
    "content": [
      {
        "text": "GPT-4\n",
        "font-size": 9,
        "token": 5
      },
      {
        "text": "北京大学的前身是京师大学堂。\nThe predecessor of Peking University is the Imperial University of\nPeking.\n 955\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "Knowledge\n世界上现存最高的木塔是中国什么塔？\nWhat is the tallest wooden tower in the world that still exists in China?\nResponses\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "Elo\n987\n 1055\n 1060\n",
        "font-size": 9,
        "token": 19
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 世界上现存最高的木塔是中国山西应县木塔。\n Response\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "The tallest existing wooden tower in the world is the Yinxian\nWooden Tower in Shanxi, China.\n",
        "font-size": 9,
        "token": 23
      },
      {
        "text": "Qwen-14B-Chat (SFT) 现存最高的木塔是中国山西应县木塔，全名为佛宫寺释迦\n塔。应县木塔位于山西省朔州市应县县城内西北角的佛宫寺\n内，建于辽代清宁二年（1056年），金明昌六年（1195年）\n增修完毕，是中国现存最高最古的一座木构塔式建筑，也是\n世界现存最高的木结构建筑。\nThe tallest existing wooden tower in the world is the Yinxian\nWooden Tower, also known as the Shijia Tower of the Fogong\nTemple. It is inside the Fogong Temple in Yinxian, Shanxi Province,\nChina. The tower was originally built in the second year of the\nQingning era of the Liao Dynasty (1056 AD) and completed its\nexpansion in the sixth year of the Jin Dynasty’s Mingchang era\n(1195 AD). It is the tallest and oldest existing wooden tower in\nChina and the tallest wooden structure in the world.\n",
        "font-size": 9,
        "token": 236
      },
      {
        "text": "Qwen-14B-Chat (RLHF) 世界上现存最高的木塔是中国的应县木塔，位于山西省应\n县。该塔建于公元1056年，高67.31米，共有5层，是中国古\n代建筑技术的杰出代表之一。应县木塔的建筑风格独特，采\n用了许多古代建筑技术，如榫卯结构、斗拱、悬挑等，具有\n很高的艺术价值和历史价值。应县木塔也是中国文化遗产保\n护的重点之一，被列为全国重点文物保护单位和世界文化遗\n产。\nThe tallest existing wooden tower in the world is the Yinxian\nWooden Tower in Yinxian, Shanxi Province, China. It was built in\nthe year 1056 AD and stands at a height of 67.31 meters, consisting\nof 5 floors. This tower is an outstanding representative of ancient\nChinese architectural technology. The Yinxian Wooden Tower\nshowcases a unique architectural style and incorporates various\nancient construction techniques such as mortise and tenon joints,\nbracket sets, and cantilever beams, possessing high artistic and\nhistorical value. It is also a key cultural heritage site in China,\nlisted as a national key cultural relic protection unit and a UNESCO\nWorld Cultural Heritage site.\n GPT-3.5\n",
        "font-size": 9,
        "token": 289
      },
      {
        "text": "世界上现存最高的木塔是中国的普陀山普济寺塔，也被称为\n古佛顶宝塔。该塔建于1056年，高约70.28米。\nThe tallest existing wooden tower in the world is the Putuoshan\nPujisi Pagoda, also known as the Ancient Baoding Pagoda, located\nin Putuoshan, China. It was built in the year 1056 AD and stands\nat a height of approximately 70.28 meters.\n 864\n GPT-4\n",
        "font-size": 9,
        "token": 122
      },
      {
        "text": "世界上现存最高的木塔是中国的应县木塔，也被称为佛宫寺\n大佛塔。\n 1040\n 41\n",
        "font-size": 10,
        "token": 32
      },
      {
        "table": "|世界上现存最高的木塔是中国什么塔？ WhatisthetallestwoodentowerintheworldthatstillexistsinChina?|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 世界上现存最高的木塔是中国山西应县木塔。 987 The tallest existing wooden tower in the world is the Yinxian WoodenTowerinShanxi,China. Qwen-14B-Chat(SFT) 现存最高的木塔是中国山西应县木塔，全名为佛宫寺释迦 1055 塔。应县木塔位于山西省朔州市应县县城内西北角的佛宫寺 内，建于辽代清宁二年（1056年），金明昌六年（1195年） 增修完毕，是中国现存最高最古的一座木构塔式建筑，也是 世界现存最高的木结构建筑。 The tallest existing wooden tower in the world is the Yinxian Wooden Tower, also known as the Shijia Tower of the Fogong Temple.ItisinsidetheFogongTempleinYinxian,ShanxiProvince, China. Thetowerwasoriginallybuiltinthesecondyearofthe QingningeraoftheLiaoDynasty(1056AD)andcompletedits expansioninthesixthyearoftheJinDynasty’sMingchangera (1195AD).Itisthetallestandoldestexistingwoodentowerin Chinaandthetallestwoodenstructureintheworld. Qwen-14B-Chat(RLHF)世界上现存最高的木塔是中国的应县木塔，位于山西省应 1060 县。该塔建于公元1056年，高67.31米，共有5层，是中国古 代建筑技术的杰出代表之一。应县木塔的建筑风格独特，采 用了许多古代建筑技术，如榫卯结构、斗拱、悬挑等，具有 很高的艺术价值和历史价值。应县木塔也是中国文化遗产保 护的重点之一，被列为全国重点文物保护单位和世界文化遗 产。 The tallest existing wooden tower in the world is the Yinxian WoodenTowerinYinxian,ShanxiProvince,China.Itwasbuiltin theyear1056ADandstandsataheightof67.31meters,consisting of5floors.Thistowerisanoutstandingrepresentativeofancient Chinese architectural technology. The Yinxian Wooden Tower showcasesauniquearchitecturalstyleandincorporatesvarious ancientconstructiontechniquessuchasmortiseandtenonjoints, bracket sets, and cantilever beams, possessing high artistic and historical value. It is also a key cultural heritage site in China, listedasanationalkeyculturalrelicprotectionunitandaUNESCO WorldCulturalHeritagesite. GPT-3.5 世界上现存最高的木塔是中国的普陀山普济寺塔，也被称为 864 古佛顶宝塔。该塔建于1056年，高约70.28米。 ThetallestexistingwoodentowerintheworldisthePutuoshan PujisiPagoda,alsoknownastheAncientBaodingPagoda,located inPutuoshan,China.Itwasbuiltintheyear1056ADandstands ataheightofapproximately70.28meters. GPT-4 世界上现存最高的木塔是中国的应县木塔，也被称为佛宫寺 1040 大佛塔。|",
        "token": 772
      }
    ]
  },
  {
    "page_num": 42,
    "content": [
      {
        "text": "The tallest wooden pagoda currently in existence in the world is\nthe Yingxian Wooden Pagoda in China, also known as the Pagoda\nof Fogong Temple.\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "Language Understanding\n吃葡萄不吐葡萄皮的拼音怎么写？\nWhat is the Pingyin for “吃葡萄不吐葡萄皮”?\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 吃葡萄不吐葡萄皮的拼音是：“ch¯ı p´u b`u tˇu p´u p´ı”。\n Response\n",
        "font-size": 9,
        "token": 46
      },
      {
        "text": "Elo\n914\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "To eat grapes without spitting out the grape skins, the pinyin is “ch¯ı\np´u b`u tˇu p´u p´ı”.\n",
        "font-size": 9,
        "token": 37
      },
      {
        "text": "Qwen-14B-Chat (SFT) 拼音：吃葡萄不吐葡萄皮“ch¯ı p´u t´ao b`u tˇu p´u t´ao p´ı”\n 1049\n",
        "font-size": 9,
        "token": 55
      },
      {
        "text": "Pingyin: To eat grapes without spitting out the grape skins, ‘ch¯ı p´u\nb`u tˇu p´u p´ı”.\n",
        "font-size": 9,
        "token": 37
      },
      {
        "text": "Qwen-14B-Chat (RLHF) 吃葡萄不吐葡萄皮的拼音是：ch¯ı p´u t´ao b`u tˇu p´u t´ao p´ı。拼音\n是一种用拉丁字母表示汉字发音的方法，它可以帮助人们学\n习和记忆汉字的发音。在这个拼音中，“吃”表示“ch¯ı”，“葡\n萄”表示“p´u t´ao”，“不”表示“b`u”，“吐”表示“tˇu”，“葡萄皮”表\n示“p´u t´ao p´ı”。\nThe pinyin for ’吃葡萄不吐葡萄皮’ is “ch¯ı p´u t´ao b`u tˇu p´u t´ao p´ı”.\nPinyin is a method of representing the pronunciation of Chinese\ncharacters using Latin letters, which can help people learn and\nremember the pronunciation of Chinese characters. In this pinyin,\n“吃” is represented as ’ch¯ı’, “葡萄” is represented as “p´u t´ao”, “不”\nis represented as “b`u”, “吐” is represented as “tˇu”, and “葡萄皮”\nis represented as “p´u t´ao p´ı”.\n GPT-3.5\n GPT-4\n",
        "font-size": 9,
        "token": 289
      },
      {
        "text": "ch¯ı p´u t´ao b´u tˇu p´u t´ao p´ı de p¯ın y¯ın zˇen me xiˇe？\nch¯ı p´u t´ao b´u tˇu p´u t´ao p´ı de p¯ın y¯ın zˇen me xiˇe？\n",
        "font-size": 9,
        "token": 78
      },
      {
        "text": "ch¯ı p´u t´ao b`u tˇu p´u t´ao p´ı\nch¯ı p´u t´ao b`u tˇu p´u t´ao p´ı\n 1068\n 946\n 1040\n",
        "font-size": 9,
        "token": 67
      },
      {
        "text": "Language Understanding\n父亲：儿子，你高中毕业了，现在打算上哪个大学？\n儿子：爸爸，我已经准备好了申请资料，想申请一个好的本科专业。\n父亲：那你已经想好申请哪些大学了吗？\n儿子：是的，我计划申请几所排名靠前的大学。我已经做了很多研究，准备在学业、社交和职\n业规划等方面全面提升自己。\n父亲：很好！你一直都很努力，爸爸妈妈一直都很自豪。上大学是一个新的起点，你需要面对\n更多的挑战和机会。你有什么打算？\n儿子：我计划尽早适应大学生活，参加各种社团和课外活动，拓宽自己的视野和人际圈。同\n时，我会认真学习专业知识和技能，努力取得好成绩，争取获得奖学金和其他荣誉。\n父亲：非常好！你的计划很明智，爸爸相信你一定会有一个充实而有意义的大学生活。爸爸会\n一直支持你的决定，希望你取得好成绩，为你的未来打下坚实的基础。\n儿子：谢谢爸爸！我会加倍努力，不辜负您对我的期望。\n这段对话中，儿子打算上大学后有哪些计划和打算？\n",
        "font-size": 9,
        "token": 229
      },
      {
        "text": "Father: Son, you have graduated from high school. Which university are you planning to attend now?\nSon: Dad, I have already prepared my application materials, and I want to apply for an excellent under-\ngraduate program.\nFather: Have you already decided which universities you want to apply to?\nSon: Yes, I plan to apply to several top-ranked universities. I have done a lot of research, and I am ready to\nimprove myself in academics, social life, and career planning.\n 42\n",
        "font-size": 10,
        "token": 104
      },
      {
        "table": "|吃葡萄不吐葡萄皮的拼音怎么写？ WhatisthePingyinfor“吃葡萄不吐葡萄皮”?|\n|Responses|",
        "token": 31
      }
    ]
  },
  {
    "page_num": 43,
    "content": [
      {
        "text": "Father: That’s great! You have always been hardworking and your mom and I are proud of you. Going to\ncollege is a new starting point, and you will face more challenges and opportunities. What are your plans?\nSon: I plan to adapt to college life as soon as possible and participate in various clubs and extracurricular\nactivities to broaden my horizons and social circle. At the same time, I will study my major knowledge\nand skills seriously, strive for good grades, and try to win scholarships and other honors.\nFather: Very good! Your plans are wise. Dad believes that you will have a fulfilling and meaningful college\nlife. Dad will always support your decisions and hopes you achieve good results, laying a solid foundation\nfor your future.\nSon: Thank you, Dad! I will work even harder and not let your expectations down.\nWhat are the son’s plans and intentions after going to college in this conversation?\n",
        "font-size": 9,
        "token": 191
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Response\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 儿子打算尽早适应大学生活，参加各种社团和课外活动，拓\n宽自己的视野和人际圈。同时，他会认真学习专业知识和技\n能，努力取得好成绩，争取获得奖学金和其他荣誉。\nMy son plans to adapt to university life as soon as possible, par-\nticipate in various clubs and extracurricular activities, and expand\nhis horizons and social circle. At the same time, he will study\nprofessional knowledge and skills seriously, work hard to achieve\ngood grades, and strive to obtain scholarships and other honors.\n",
        "font-size": 9,
        "token": 130
      },
      {
        "text": "Qwen-14B-Chat (SFT) 儿子打算在学业、社交和职业规划等方面全面提升自己，尽\n早适应大学生活，参加各种社团和课外活动，拓宽自己的视\n野和人际圈。同时，他会认真学习专业知识和技能，努力取\n得好成绩，争取获得奖学金和其他荣誉。\nThe son plans to comprehensively improve himself in his academic,\nsocial, and career planning aspects to adapt to university life as early\nas possible. He will participate in various clubs and extracurricular\nactivities to broaden his horizons and expand his social circle. At\nthe same time, he will diligently study professional knowledge and\nskills, strive for good grades, and obtain scholarships and other\nhonors.\n",
        "font-size": 9,
        "token": 154
      },
      {
        "text": "Qwen-14B-Chat (RLHF) 儿子打算在大学生活中参加各种社团和课外活动，拓宽自己\n的视野和人际圈。同时，他会认真学习专业知识和技能，努\n力取得好成绩，争取获得奖学金和其他荣誉。他的目标是尽\n早适应大学生活，为自己的未来打下坚实的基础。\nMy son plans to participate in various clubs and extracurricular\nactivities during his college life, in order to broaden his horizons\nand expand his social circle. At the same time, he will study\nprofessional knowledge and skills seriously, strive for good grades,\nand aim to obtain scholarships and other honors. His goal is to\nadapt to college life as soon as possible and lay a solid foundation\nfor his future.\n GPT-3.5\n",
        "font-size": 9,
        "token": 168
      },
      {
        "text": "儿子计划适应大学生活，参加各种社团和课外活动，拓宽自\n己的视野和人际圈。同时，他还计划认真学习专业知识和技\n能，努力取得好成绩，争取获得奖学金和其他荣誉。他希望\n在大学打下坚实的基础，为自己的未来打好基础。\nThe son plans to adapt to college life participate in various clubs\nand extracurricular activities to broaden his horizons and social\ncircle. At the same time, he plans to study professional knowledge\nand skills seriously, strive for good grades, and obtain scholarships\nand other honors. He hopes to lay a solid foundation in college and\nbuild a good foundation for his future.\n",
        "font-size": 9,
        "token": 138
      },
      {
        "text": "Elo\n973\n 1049\n 975\n 976\n 43\n",
        "font-size": 10,
        "token": 27
      },
      {
        "table": "|Father:That’sgreat!YouhavealwaysbeenhardworkingandyourmomandIareproudofyou.Goingto collegeisanewstartingpoint,andyouwillfacemorechallengesandopportunities.Whatareyourplans? Son:Iplantoadapttocollegelifeassoonaspossibleandparticipateinvariousclubsandextracurricular activitiestobroadenmyhorizonsandsocialcircle.Atthesametime,Iwillstudymymajorknowledge andskillsseriously,striveforgoodgrades,andtrytowinscholarshipsandotherhonors. Father:Verygood!Yourplansarewise.Dadbelievesthatyouwillhaveafulfillingandmeaningfulcollege life.Dadwillalwayssupportyourdecisionsandhopesyouachievegoodresults,layingasolidfoundation foryourfuture. Son:Thankyou,Dad!Iwillworkevenharderandnotletyourexpectationsdown. Whataretheson’splansandintentionsaftergoingtocollegeinthisconversation?|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 儿子打算尽早适应大学生活，参加各种社团和课外活动，拓 973 宽自己的视野和人际圈。同时，他会认真学习专业知识和技 能，努力取得好成绩，争取获得奖学金和其他荣誉。 Mysonplanstoadapttouniversitylifeassoonaspossible,par- ticipateinvariousclubsandextracurricularactivities,andexpand his horizons and social circle. At the same time, he will study professionalknowledgeandskillsseriously,workhardtoachieve goodgrades,andstrivetoobtainscholarshipsandotherhonors. Qwen-14B-Chat(SFT) 儿子打算在学业、社交和职业规划等方面全面提升自己，尽 1049 早适应大学生活，参加各种社团和课外活动，拓宽自己的视 野和人际圈。同时，他会认真学习专业知识和技能，努力取 得好成绩，争取获得奖学金和其他荣誉。 Thesonplanstocomprehensivelyimprovehimselfinhisacademic, social,andcareerplanningaspectstoadapttouniversitylifeasearly aspossible.Hewillparticipateinvariousclubsandextracurricular activitiestobroadenhishorizonsandexpandhissocialcircle.At thesametime,hewilldiligentlystudyprofessionalknowledgeand skills, striveforgoodgrades, andobtainscholarshipsandother honors. Qwen-14B-Chat(RLHF)儿子打算在大学生活中参加各种社团和课外活动，拓宽自己 975 的视野和人际圈。同时，他会认真学习专业知识和技能，努 力取得好成绩，争取获得奖学金和其他荣誉。他的目标是尽 早适应大学生活，为自己的未来打下坚实的基础。 Mysonplanstoparticipateinvariousclubsandextracurricular activitiesduringhiscollegelife,inordertobroadenhishorizons and expand his social circle. At the same time, he will study professionalknowledgeandskillsseriously,striveforgoodgrades, andaimtoobtainscholarshipsandotherhonors. Hisgoalisto adapttocollegelifeassoonaspossibleandlayasolidfoundation forhisfuture. GPT-3.5 儿子计划适应大学生活，参加各种社团和课外活动，拓宽自 976 己的视野和人际圈。同时，他还计划认真学习专业知识和技 能，努力取得好成绩，争取获得奖学金和其他荣誉。他希望 在大学打下坚实的基础，为自己的未来打好基础。 Thesonplanstoadapttocollegelifeparticipateinvariousclubs andextracurricularactivitiestobroadenhishorizonsandsocial circle.Atthesametime,heplanstostudyprofessionalknowledge andskillsseriously,striveforgoodgrades,andobtainscholarships andotherhonors.Hehopestolayasolidfoundationincollegeand buildagoodfoundationforhisfuture.|",
        "token": 868
      }
    ]
  },
  {
    "page_num": 44,
    "content": [
      {
        "text": "GPT-4\n",
        "font-size": 9,
        "token": 5
      },
      {
        "text": "儿子的大学计划和打算包括：\n1. 尽早适应大学生活。\n2. 参加各种社团和课外活动，以拓宽自己的视野和人际圈。\n3. 认真学习专业知识和技能，努力取得好成绩。\n4. 争取获得奖学金和其他荣誉。\n5. 为未来打下坚实的基础。\nMy son’s university plans and aspirations include:\n 1048\n",
        "font-size": 9,
        "token": 85
      },
      {
        "text": "1. Adjusting to university life as soon as possible.\n2. Joining various clubs and extracurricular activities to broaden\nhis horizons and social circle.\n3. Studying professional knowledge and skills seriously and striv-\ning for excellent grades.\n4. Striving to obtain scholarships and other honors.\n5. Building a solid foundation for the future.\n",
        "font-size": 9,
        "token": 73
      },
      {
        "text": "Creative Writing\n请编写一个以超市为背景的，内容涉及到水果误会的笑话。\nPlease write a joke set in a supermarket involving a fruit misunderstanding.\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Elo\n849\n Response\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 有一天，小明去超市买水果，他买了一篮子苹果。当他回到\n家，他发现苹果都是红色的，他想：这些苹果一定很好吃。\n但是，当他开始吃的时候，他发现这些苹果都是酸的。他很\n惊讶，因为他买的是甜苹果。他去超市问，店员说：“对不\n起，我们没有甜苹果，只有红色的苹果。”小明很生气，他\n想：我买的是甜苹果，为什么给我红色的苹果呢？他回去告\n诉了妈妈，妈妈笑着说：“你买的是红苹果... [[The original\ncontent is long and the follow up is truncated]]\nOne day, Xiaoming went to the supermarket to buy fruits. He\nbought a basket of apples. When he got home, he found that all the\napples were red. He thought to himself: These apples must be very\ndelicious. However, when he started eating them, he found that\nthese apples were sour. He was surprised because he bought sweet\napples. He went to the supermarket to ask, and the shopkeeper said,\n“I’m sorry, we don’t have sweet apples, only red apples.” Xiaoming\nwas angry and thought to himself: I bought sweet apples, why did\nthey give me red apples? He went back and told his mother, and his\nmother smiled and said, “You bought red delicious apples.... [[The\noriginal content is long and the follow up is truncated]]\n 44\n",
        "font-size": 10,
        "token": 328
      },
      {
        "table": "|请编写一个以超市为背景的，内容涉及到水果误会的笑话。 Pleasewriteajokesetinasupermarketinvolvingafruitmisunderstanding.|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 有一天，小明去超市买水果，他买了一篮子苹果。当他回到 849 家，他发现苹果都是红色的，他想：这些苹果一定很好吃。 但是，当他开始吃的时候，他发现这些苹果都是酸的。他很 惊讶，因为他买的是甜苹果。他去超市问，店员说：“对不 起，我们没有甜苹果，只有红色的苹果。”小明很生气，他 想：我买的是甜苹果，为什么给我红色的苹果呢？他回去告 诉了妈妈，妈妈笑着说：“你买的是红苹果... [[Theoriginal contentislongandthefollowupistruncated]] One day, Xiaoming went to the supermarket to buy fruits. He boughtabasketofapples.Whenhegothome,hefoundthatallthe appleswerered.Hethoughttohimself:Theseapplesmustbevery delicious. However,whenhestartedeatingthem,hefoundthat theseapplesweresour.Hewassurprisedbecauseheboughtsweet apples.Hewenttothesupermarkettoask,andtheshopkeepersaid, “I’msorry,wedon’thavesweetapples,onlyredapples.”Xiaoming wasangryandthoughttohimself:Iboughtsweetapples,whydid theygivemeredapples?Hewentbackandtoldhismother,andhis mothersmiledandsaid,“Youboughtreddeliciousapples....[[The originalcontentislongandthefollowupistruncated]]|",
        "token": 390
      }
    ]
  },
  {
    "page_num": 45,
    "content": [
      {
        "text": "Qwen-14B-Chat (SFT) 一天，一位顾客走进超市，对店员说：“我要买一个香蕉。”\n 980\n",
        "font-size": 9,
        "token": 36
      },
      {
        "text": "店员回答：“好的，先生，我们这里有各种各样的香蕉，您\n想要哪一种？”\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "顾客回答：“我要一个黄色的香蕉。”\n 店员有些疑惑，但还是给他拿了一个黄色的香蕉。\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "顾客又说：“我要一个弯曲的香蕉。”\n 店员更加疑惑了，但他还是给他拿了一个弯曲的香蕉。\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "最后，顾客说：“我要一个会说话的香蕉。”\n",
        "font-size": 9,
        "token": 13
      },
      {
        "text": "店员忍不住笑了，回答：“先生，我们这里没有会说话的香\n蕉，那只是个误会。”\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "顾客也笑了，回答：“我知道，我只是想和你开个玩笑。”\nOne day, a customer walked into a supermarket and said to the shop\nassistant, “I want to buy a banana.”\n",
        "font-size": 9,
        "token": 42
      },
      {
        "text": "The assistant replied, “Okay, sir, we have all kinds of bananas here.\nWhich one would you like?”\n The customer answered, “I want a yellow banana.”\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "The assistant was a little confused but still gave him a yellow\nbanana.\n The customer then said, “I want a curved banana.”\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "The assistant was even more confused, but he still gave him a\ncurved banana.\n Finally, the customer said, “I want a talking banana.”\n",
        "font-size": 9,
        "token": 31
      },
      {
        "text": "The assistant couldn’t help but laugh and replied, ”Sir, we don’t\nhave bananas that can talk here, that’s just a misunderstanding.”\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "The customer also laughed and replied, “I know, I was just joking\nwith you.”\n 45\n",
        "font-size": 10,
        "token": 23
      }
    ]
  },
  {
    "page_num": 46,
    "content": [
      {
        "text": "Qwen-14B-Chat (RLHF) 有一天，小明去超市买水果。他挑选了一篮子新鲜的苹果和\n 香蕉，然后走向收银台。\n 986\n",
        "font-size": 9,
        "token": 48
      },
      {
        "text": "收银员小王看到小明手中的水果，突然大声喊道：“哇，你\n的水果好新鲜啊！你是从哪里买的？”\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "小明愣了一下，心想：“难道我买水果的技巧被发现了？”于\n是他得意地回答：“我刚刚从果园里摘的，保证新鲜！”\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "小王听了，忍不住笑了起来：“你可真逗，这是超市里的水\n果，不是你家果园里的。”\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "小明听了，顿时傻眼了。他尴尬地解释道：“啊，对不起，\n我太紧张了，以为这是我的水果。”\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "小王笑着安慰道：“没事没事，我们都搞错了。不过，你的\n水果确实很好，我们超市里的水果也都是新鲜的。”\n",
        "font-size": 9,
        "token": 31
      },
      {
        "text": "小明听了，松了一口气，心情也好了起来。他付了钱，提着\n水果离开了超市，心里想着：“下次再买水果，一定要更加\n小心，不要闹出笑话了！”\nOne day, Xiao Ming went to the supermarket to buy fruits. He\npicked a basket of fresh apples and bananas, and then headed\ntowards the cashier.\n",
        "font-size": 9,
        "token": 77
      },
      {
        "text": "The cashier, Xiao Wang, saw the fruits in Xiao Ming’s hands and\nsuddenly shouted, ”Wow, your fruits are so fresh! Where did you\nbuy them from?”\n",
        "font-size": 9,
        "token": 38
      },
      {
        "text": "Xiao Ming was stunned for a moment and thought to himself,\n”Could it be that my fruit-buying skills have been discovered?” So\nhe proudly replied, ”I just picked them from the orchard, guaranteed\nfresh!”\n",
        "font-size": 9,
        "token": 48
      },
      {
        "text": "Upon hearing this, Xiao Wang couldn’t help but laugh and said,\n“You’re really funny. These are fruits from the supermarket, not\nfrom your own orchard.”\n",
        "font-size": 9,
        "token": 34
      },
      {
        "text": "Xiao Ming, upon listening, was immediately dumbfounded. He\nawkwardly explained, “Oh, I’m sorry, I was too nervous, and I\nthought these were my fruits.”\n",
        "font-size": 9,
        "token": 41
      },
      {
        "text": "Xiao Wang smiled and comforted him, saying, “It’s alright, it’s\nalright. We both made a mistake. However, your fruits are indeed\nvery good, and the fruits in our supermarket are also fresh.”\n",
        "font-size": 9,
        "token": 48
      },
      {
        "text": "Upon hearing this, Xiao Ming breathed a sigh of relief, and his\nmood improved. He paid the money, carried the fruits, and left\nthe supermarket, thinking, “Next time I buy fruits, I must be more\ncareful and avoid making such a funny mistake!”\n GPT-3.5\n",
        "font-size": 9,
        "token": 66
      },
      {
        "text": "在超市的水果区，小明看到了一盒红苹果和一盒绿苹果。他\n拿着两盒苹果走到收银台前排队结账。\n 964\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "收银员看着小明手里的两盒苹果说：“这位小朋友，你知道\n红苹果和绿苹果有什么区别吗？”\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "小明想了一下，然后笑着回答：“当然知道了！红苹果是情\n侣，绿苹果是单身狗。”\n 收银员被小明的回答逗乐了，忍不住笑了出来。\n 46\n",
        "font-size": 10,
        "token": 46
      }
    ]
  },
  {
    "page_num": 47,
    "content": [
      {
        "text": "In the supermarket’s fruit section, Xiaoming saw a box of red apples\nand a box of green apples. He held the two boxes of apples and\nwalked to the cash register to queue up for payment.\n",
        "font-size": 9,
        "token": 44
      },
      {
        "text": "The cashier looked at the two boxes of apples in Xiaoming’s hands\nand said, “Hey, little friend, do you know the difference between\nred apples and green apples?”\n",
        "font-size": 9,
        "token": 38
      },
      {
        "text": "Xiaoming thought momentarily and then smiled, “Of course, I\nknow! Red apples are couples, and green apples are single dogs.”\n Xiaoming’s answer amused the cashier, and couldn’t help but laugh.\n GPT-4\n",
        "font-size": 9,
        "token": 49
      },
      {
        "text": "“一个非常善良的男孩，在超市里发现了一个老太太在苹果\n堆里挑选苹果。他看到老太太似乎找不到满意的苹果，就走\n上前去帮忙。\n 1144\n",
        "font-size": 9,
        "token": 41
      },
      {
        "text": "男孩说：“老奶奶，这箱子的苹果我之前都看过了，里面没\n有一个好的。但是我知道柜台上有一个我保留的极好的苹\n果，我去为您拿过来。”\n",
        "font-size": 9,
        "token": 42
      },
      {
        "text": "老太太感动地点点头，男孩兴冲冲地跑去拿苹果，只见他笑\n呵呵地拿过一个巨大的，艳丽的苹果放在老太太面前。\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "老太太疑惑地看看男孩，再看看手中的“苹果”，然后微笑着\n说：“孩子，很感谢你的好意，但这是一个番茄。””\nA very kind boy found an old lady picking apples in a pile at the\nsupermarket. Seeing that she could not find a satisfactory apple, he\napproached to help.\n",
        "font-size": 9,
        "token": 67
      },
      {
        "text": "The boy said, “Granny, I’ve checked all the apples in this box\nbefore, and there isn’t a good one. But I know I saved an excellent\napple on the counter for you. Let me get it for you.”\n",
        "font-size": 9,
        "token": 50
      },
      {
        "text": "The old lady nodded gratefully, and the boy eagerly ran to get the\napple. He joyfully handed a huge, vibrant apple to the old lady.\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "Confused, the old lady looked at the boy, then at the ”apple” in her\nhand, and smiled. She said, “Child, thank you for your kindness,\nbut this is a tomato.”\n",
        "font-size": 9,
        "token": 43
      },
      {
        "text": "Mathematics\n一个正方形，被分成5个相等的长方形，每个长方形的周长是60厘米，正方形的周长是多少厘米\nA square is divided into five equal rectangles, each with a perimeter of 60 centimeters. What is the\nperimeter of the square in centimeters?\n",
        "font-size": 9,
        "token": 69
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 设正方形的边长为x，那么每个长方形的边长就是x/5。根据\n Response\n",
        "font-size": 9,
        "token": 38
      },
      {
        "text": "题意，我们知道每个长方形的周长是60厘米，所以有：\n2(x/5) + 2(x/5) + 2(x/5) + 2(x/5) = 60\n化简得：x = 60/(2 + 2 + 2 + 2) = 15\n所以正方形的周长是4x = 4 ∗ 15 = 60厘米\nIf the side length of a square is x, then the side length of each\nrectangle is x/5. According to the question, we know that the\nperimeter of each rectangle is 60 centimeters, so we have:\n2(x/5) + 2(x/5) + 2(x/5) + 2(x/5) = 60\nSimplifying, we get: x = 60/(2 + 2 + 2 + 2) = 15\nSo the perimeter of the square is 4x = 4 ∗ 15 = 60 centimeters.\n",
        "font-size": 9,
        "token": 232
      },
      {
        "text": "Elo\n976\n 47\n",
        "font-size": 10,
        "token": 11
      },
      {
        "table": "|一个正方形，被分成5个相等的长方形，每个长方形的周长是60厘米，正方形的周长是多少厘米 A square is divided into five equal rectangles, each with a perimeter of 60 centimeters. What is the perimeterofthesquareincentimeters?|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 设正方形的边长为x，那么每个长方形的边长就是x/5。根据 976 题意，我们知道每个长方形的周长是60厘米，所以有： 2(x/5)+2(x/5)+2(x/5)+2(x/5)=60 化简得：x=60/(2+2+2+2)=15 所以正方形的周长是4x=4∗15=60厘米 If the side length of a square is x, then the side length of each rectangle is x/5. According to the question, we know that the perimeterofeachrectangleis60centimeters,sowehave: 2(x/5)+2(x/5)+2(x/5)+2(x/5)=60 Simplifying,weget:x=60/(2+2+2+2)=15 Sotheperimeterofthesquareis4x=4∗15=60centimeters.|",
        "token": 300
      }
    ]
  },
  {
    "page_num": 48,
    "content": [
      {
        "text": "976\n 1139\n",
        "font-size": 9,
        "token": 10
      },
      {
        "text": "Qwen-14B-Chat (SFT) 设正方形边长为x，长方形边长为y\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "根据题意，有：x = 5y\n2y + 2y = 60\n4y = 60\ny = 15\nx = 5 ∗ 15 = 75\n正方形周长= 4x = 4 ∗ 75 = 300\nLet the side length of the square be x and the length of the rectangle\nbe y. According to the problem: x = 5y 2y + 2y = 60 4y = 60\ny = 15 x = 5 ∗ 15 = 75 Perimeter of the square = 4x = 4 ∗ 75 =\n300\n",
        "font-size": 9,
        "token": 160
      },
      {
        "text": "Qwen-14B-Chat (RLHF) 首先，我们设正方形的边长为x，则每个长方形的长为x，宽\n",
        "font-size": 9,
        "token": 37
      },
      {
        "text": "为x/5。\n根据长方形的周长公式，可得每个长方形的周长为2(x +\nx/5) = 12x/5。\n已知每个长方形的周长为60厘米，代入上式得：\n12x/5 = 60\n解得x = 25。\n因此，正方形的周长为4x = 100厘米。\n答案：正方形的周长是100厘米。\nFirst, let’s assume that the square’s side length is x. Therefore, the\nlength of each rectangle is x and the width is x/5.\nAccording to the formula for the perimeter of a rectangle, we can\ndetermine that the perimeter of each rectangle is 2(x + x/5) =\n12x/5.\nGiven that the perimeter of each rectangle is 60 centimeters, we\ncan substitute this value into the equation:\n12x/5 = 60\nSolving for x, we find x = 25.\nTherefore, the perimeter of the square is 4x = 100 centimeters.\nAnswer: The perimeter of the square is 100 centimeters.\n GPT-3.5\n GPT-4\n 897\n",
        "font-size": 9,
        "token": 272
      },
      {
        "text": "设正方形的边长为x厘米，则每个长方形的长为x厘米，宽\n为5x/4厘米。\n每个长方形的周长为2(x + 5x/4) = 60厘米。\n化简得13x/2 = 60\n解方程得到x = (60 ∗ 2)/13 = 120/13H9.23\n正方形的周长为4x = 4 ∗ 9.23 ≈ 36.92厘米。\nIf the side length of a square is x centimeters, then the length of\neach rectangle is x centimeters, and the width is 5x/4 centimeters.\nThe perimeter of each rectangle is 2(x + 5x/4) = 60 centimeters.\nSimplifying, we get 13x/2 = 60.\nSolving the equation, we find x = (60 * 2) / 13 = 120 / 13 ≈ 9.23.\nThe perimeter of the square is 4x = 4 * 9.23 ≈ 36.92 centimeters.\n",
        "font-size": 9,
        "token": 263
      },
      {
        "text": "由题意可知，每个长方形的一条长边等于正方形的边长，设\n为a。设每个长方形的一条短边为b，那么有5b = a。根据每\n个长方形的周长可得到2 ∗ (a + b) = 60厘米。将5b = a代\n入到长方形的周长公式中得：2 ∗ a + 2 ∗ 5b = 60，化简\n得2 ∗ a + 10b = 60，由于a = 5b，所以2 ∗ a = 2 ∗ 5b = 10b\n那么有10b + 10b = 60，得b = 3厘米，a = 5 ∗ b = 15厘米。\n所以，正方形的周长是4 ∗ a = 4 ∗ 15 = 60厘米。\n 1010\n 48\n",
        "font-size": 10,
        "token": 217
      }
    ]
  },
  {
    "page_num": 49,
    "content": [
      {
        "text": "From the given problem, it can be inferred that the length of one\nside of each rectangle is equal to that of one side of the square,\ndenoted as a. Let the length of one side of each rectangle be denoted\nas b; then, we have 5b = a. The perimeter of each rectangle can be\nobtained as 2 ∗ (a + b) = 60 centimeters.\nSubstituting 5b = a into the formula for the perimeter of the\nrectangle, we have 2a+25b = 60. Simplifying it gives 2a+10b =\n60. Since a = 5b, it follows that 2a = 25b = 10b. Therefore,\n10b + 10b = 60, which gives b = 3 centimeters and a = 5b = 15\ncentimeters.\nThus, the perimeter of the square is 4a = 415 = 60 centimeters.\n",
        "font-size": 9,
        "token": 221
      },
      {
        "text": "Mathematics\n同学们进行广播操比赛，全班正好排成相等的6行。小红排在第二行，从头数，她站在第5个位\n置，从后数她站在第3个位置，这个班共有多少人\nThe students are having a radio exercise competition, and the whole class is neatly arranged into 6 equal\nrows. Xiaohong is in the second row, counting from the front, she is in the 5th position. Counting from the\nback, she is in the 3rd position. How many people are there in this class?\n",
        "font-size": 9,
        "token": 127
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Response\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 小红前面的人数= 5 - 1 = 4，小红后面的人数= 3 - 1 = 2，全班\n人数= 小红前面的人数+ 小红后面的人数+ 小红自己= 4 + 2 +\n1 = 7\nThe number of people in front of Xiaohong = 5 - 1 = 4, the number\nof people behind Xiaohong = 3 - 1 = 2, the total number of people\nin the class = the number of people in front of Xiaohong + the\nnumber of people behind Xiaohong + Xiaohong herself = 4 + 2 + 1\n= 7.\n Qwen-14B-Chat (SFT)\n",
        "font-size": 9,
        "token": 179
      },
      {
        "text": "5+3-1=7（人）\n7×6=42（人）\n这个班共有42人。\n5+3-1=7 (people)\n7×6=42 (people)\nThere are a total of 42 people in this class.\n",
        "font-size": 9,
        "token": 58
      },
      {
        "text": "Qwen-14B-Chat (RLHF) 小 红 前 面 有4个 人 ， 后 面 有2个 人 ， 所 以 这 一 行 共\n",
        "font-size": 9,
        "token": 55
      },
      {
        "text": "有4+2+1=7个人。\n全班共有6行，所以全班共有6*7=42个人。\nThere are 4 people in front of Xiaohong and 2 people behind her,\nso there are a total of 4+2+1=7 people in this row.\nThere are a total of 6 rows in the class, so there are a total of 6*7=42\npeople in the whole class.\n",
        "font-size": 9,
        "token": 97
      },
      {
        "text": "Elo\n976\n 976\n 1139\n 897\n GPT-3.5\n GPT-4\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "根据题目信息，小红排在第二行第5个位置，也即第11个位\n置。而小红从后数是第3个位置，也即倒数第11个位置。所\n以这个班共有11个人。\nAccording to the information given, Xiaohong is in the second row,\nfifth position, which is also the 11th position. And Xiaohong is\nin the third position from the end, which is also the 11th position\ncounting from the back. So there are a total of 11 people in this\nclass.\n",
        "font-size": 9,
        "token": 131
      },
      {
        "text": "如 果 小 红 从 头 数 是 第5个 ， 从 后 数 是 第3个 ， 那 么 这 一 排\n就 有5+3-1=7人 。 全 班 正 好 排 成6行 ， 所 以 班 级 里 有 人 数\n为7*6=<<7*6=42>>=42人。\n 1010\n 49\n",
        "font-size": 10,
        "token": 114
      },
      {
        "table": "|同学们进行广播操比赛，全班正好排成相等的6行。小红排在第二行，从头数，她站在第5个位 置，从后数她站在第3个位置，这个班共有多少人 Thestudentsarehavingaradioexercisecompetition,andthewholeclassisneatlyarrangedinto6equal rows.Xiaohongisinthesecondrow,countingfromthefront,sheisinthe5thposition.Countingfromthe back,sheisinthe3rdposition.Howmanypeoplearethereinthisclass?|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 小红前面的人数=5-1=4，小红后面的人数=3-1=2，全班 976 人数=小红前面的人数+小红后面的人数+小红自己=4+2+ 1=7 ThenumberofpeopleinfrontofXiaohong=5-1=4,thenumber ofpeoplebehindXiaohong=3-1=2,thetotalnumberofpeople intheclass=thenumberofpeopleinfrontofXiaohong+the numberofpeoplebehindXiaohong+Xiaohongherself=4+2+1 =7. Qwen-14B-Chat(SFT) 5+3-1=7（人） 976 7×6=42（人） 这个班共有42人。 5+3-1=7(people) 7×6=42(people) Thereareatotalof42peopleinthisclass. Qwen-14B-Chat(RLHF)小红前面有4个人，后面有2个人，所以这一行共 1139 有4+2+1=7个人。 全班共有6行，所以全班共有6*7=42个人。 Thereare4peopleinfrontofXiaohongand2peoplebehindher, sothereareatotalof4+2+1=7peopleinthisrow. Thereareatotalof6rowsintheclass,sothereareatotalof6*7=42 peopleinthewholeclass. GPT-3.5 根据题目信息，小红排在第二行第5个位置，也即第11个位 897 置。而小红从后数是第3个位置，也即倒数第11个位置。所 以这个班共有11个人。 Accordingtotheinformationgiven,Xiaohongisinthesecondrow, fifthposition,whichisalsothe11thposition. AndXiaohongis inthethirdpositionfromtheend,whichisalsothe11thposition countingfromtheback. Sothereareatotalof11peopleinthis class. GPT-4 如果小红从头数是第5个，从后数是第3个，那么这一排 1010 就有5+3-1=7人。全班正好排成6行，所以班级里有人数 为7*6=<<7*6=42>>=42人。|",
        "token": 702
      }
    ]
  },
  {
    "page_num": 50,
    "content": [
      {
        "text": "If Xiaohong is the 5th person when counting from the front and the\n3rd person when counting from the back, then there are a total of\n5+3-1=7 people in this row. The whole class is perfectly arranged\nin 6 rows, so there are a total of 7*6=42 people in the class.\n",
        "font-size": 9,
        "token": 76
      },
      {
        "text": "Code\npython实现一个正则表达式匹配，要求能匹配到所有以139开头，以00结尾的11位数字\nImplement a regular expression matching in Python, which should be able to match all 11-digit numbers\nthat start with ”139” and end with ”00”.\nResponses\n",
        "font-size": 9,
        "token": 71
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 你可以使用Python的re模块来实现这个正则表达式匹配。以\n Response\n 下是一个示例代码：\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "Elo\n941\n import re\n def match phone numbers(phone numbers):\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "pattern = r ’ˆ139d9$’\nreturn re . findall ( pattern , phone numbers)\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "phone numbers = [’13900000000’, ’13912345678’, ’\n13998765432’, ’13987654321’]\nprint (match phone numbers(phone numbers))\n",
        "font-size": 9,
        "token": 66
      },
      {
        "text": "在这个代码中，我们首先定义了一个正则表达式模式’ˆ139\nd9$’，这个模式表示以139开头，以00结尾，且长度为11的\n数字。然后我们使用re . findall 函数来查找所有匹配这个模\n式的字符串，最后返回这些匹配的字符串。\n",
        "font-size": 9,
        "token": 73
      },
      {
        "text": "注意，这个代码假设phone numbers是一个包含多个字符串的\n列表。如果你的phone numbers是一个字符串，你需要先将其\n转换为列表。\nYou can use the re module in Python to implement this regular\nexpression matching. Here is an example code:\n import re\n def match phone numbers(phone numbers):\n",
        "font-size": 9,
        "token": 62
      },
      {
        "text": "pattern = r ’ˆ139d9$’\nreturn re . findall ( pattern , phone numbers)\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "phone numbers = [’13900000000’, ’13912345678’, ’\n13998765432’, ’13987654321’]\nprint (match phone numbers(phone numbers))\n",
        "font-size": 9,
        "token": 66
      },
      {
        "text": "This code defines a regular expression pattern ˆ139d9$. This pat-\ntern represents a string that starts with ’139’, ends with ’00’, and\nhas a length of 11 digits. Then, we use the re.findall function to\nfind all the strings that match this pattern, and finally, we return\nthese matching strings.\n",
        "font-size": 9,
        "token": 76
      },
      {
        "text": "Note that this code assumes that phone numbers is a list containing\nmultiple strings. If your phone numbers is a string, you must first\nconvert it to a list.\n 50\n",
        "font-size": 10,
        "token": 38
      },
      {
        "table": "|python实现一个正则表达式匹配，要求能匹配到所有以139开头，以00结尾的11位数字 ImplementaregularexpressionmatchinginPython,whichshouldbeabletomatchall11-digitnumbers thatstartwith”139”andendwith”00”.|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 你可以使用Python的re模块来实现这个正则表达式匹配。以 941 下是一个示例代码： importre def match phone numbers(phone numbers): pattern = r’ˆ139d9$’ return re. findall (pattern, phone numbers) phone numbers=[’13900000000’,’13912345678’,’ 13998765432’,’13987654321’] print(match phone numbers(phone numbers)) 在这个代码中，我们首先定义了一个正则表达式模式’ˆ139 d9$’，这个模式表示以139开头，以00结尾，且长度为11的 数字。然后我们使用re. findall函数来查找所有匹配这个模 式的字符串，最后返回这些匹配的字符串。 注意，这个代码假设phone numbers是一个包含多个字符串的 列表。如果你的phone numbers是一个字符串，你需要先将其 转换为列表。 YoucanusetheremoduleinPythontoimplementthisregular expressionmatching.Hereisanexamplecode: importre def match phone numbers(phone numbers): pattern = r’ˆ139d9$’ return re. findall (pattern, phone numbers) phone numbers=[’13900000000’,’13912345678’,’ 13998765432’,’13987654321’] print(match phone numbers(phone numbers)) Thiscodedefinesaregularexpressionpatternˆ139d9$.Thispat- ternrepresentsastringthatstartswith’139’,endswith’00’,and hasalengthof11digits. Then,weusethere.findallfunctionto findallthestringsthatmatchthispattern,andfinally,wereturn thesematchingstrings. Notethatthiscodeassumesthatphone numbersisalistcontaining multiplestrings.Ifyourphone numbersisastring,youmustfirst convertittoalist.|",
        "token": 536
      }
    ]
  },
  {
    "page_num": 51,
    "content": [
      {
        "text": "Qwen-14B-Chat (SFT) 你可以使用Python的re模块来实现这个正则表达式匹配。以\n 下是一个简单的例子：\n 941\n import re\n def match phone numbers(phone numbers):\n",
        "font-size": 9,
        "token": 50
      },
      {
        "text": "pattern = r ’ˆ139d9$’\nreturn [phone number for phone number in\nphone numbers if re .match( pattern , phone number)]\n",
        "font-size": 9,
        "token": 34
      },
      {
        "text": "phone numbers = [’13912345678’, ’13923456789’, ’\n13934567890’, ’13945678901’, ’13956789012’, ’13967890123\n’, ’13978901234’, ’13989012345’, ’13990123456’, ’\n13900000000’]\nprint (match phone numbers(phone numbers))\n",
        "font-size": 9,
        "token": 146
      },
      {
        "text": "在这个例子中，我们定义了一个正则表达式模式’ˆ139d9$’，\n它匹配以139开头，后面跟着9个任意数字，最后以0结尾的\n字符串。然后我们使用re .match()函数来检查每个电话号码\n是否匹配这个模式。如果匹配，我们就将这个电话号码添加\n到结果列表中。\nYou can use the re module of Python to implement this regular\nexpression matching. Here is a simple example:\n import re\n def match phone numbers(phone numbers):\n",
        "font-size": 9,
        "token": 111
      },
      {
        "text": "pattern = r ’ˆ139d9$’\nreturn [phone number for phone number in\nphone numbers if re .match( pattern , phone number)]\n",
        "font-size": 9,
        "token": 34
      },
      {
        "text": "phone numbers = [’13912345678’, ’13923456789’, ’\n13934567890’, ’13945678901’, ’13956789012’, ’13967890123\n’, ’13978901234’, ’13989012345’, ’13990123456’, ’\n13900000000’]\nprint (match phone numbers(phone numbers))\n",
        "font-size": 9,
        "token": 146
      },
      {
        "text": "In this example, we define a regular expression pattern ’ˆ139d9$’,\nwhich matches a string starting with 139, followed by nine arbitrary\ndigits and ending with 0. Then we use the re .match() function to\ncheck if each phone number matches this pattern. If it does, we\nadd this phone number to the result list.\n 51\n",
        "font-size": 10,
        "token": 82
      }
    ]
  },
  {
    "page_num": 52,
    "content": [
      {
        "text": "Qwen-14B-Chat (RLHF) 可以使用Python的re模块来实现正则表达式匹配。以下是一\n 个示例代码：\n 1090\n import re\n pattern = r ’ˆ139d9$’\n",
        "font-size": 9,
        "token": 59
      },
      {
        "text": "# 测试匹配\ntext = ’13912345678’\nmatch = re .match( pattern ,\nif match:\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "print ( ’匹配成功’ )\n else :\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "print ( ’匹配失败’ )\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "# 测试不匹配\ntext = ’139123456780’\nmatch = re .match( pattern ,\nif match:\n",
        "font-size": 9,
        "token": 34
      },
      {
        "text": "print ( ’匹配成功’ )\n else :\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "print ( ’匹配失败’ )\n text )\n text )\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "其中，ˆ表示匹配字符串的开头，d表示匹配数字，9表示匹\n配9个数字，$表示匹配字符串的结尾。将这些符号组合在一\n起，就可以得到一个匹配11位数字的正则表达式。\n",
        "font-size": 9,
        "token": 52
      },
      {
        "text": "在测试匹配时，使用re .match()函数来尝试匹配文本，如果\n匹配成功，返回一个Match对象，否则返回None。在本例\n中，第一个测试匹配成功，第二个测试匹配失败。\nYou can use the re module in Python to implement regular expres-\nsion matching. Here is an example code:\n import re\n pattern = r ’ˆ139d9$’\n",
        "font-size": 9,
        "token": 85
      },
      {
        "text": "# Match Test\ntext = ’13912345678’\nmatch = re .match( pattern ,\nif match:\n print ( ’Success’ )\n else :\n print ( ’Fail’ )\n",
        "font-size": 9,
        "token": 45
      },
      {
        "text": "# Unmatch Test\ntext = ’139123456780’\nmatch = re .match( pattern ,\nif match:\n print ( ’Success’ )\n else :\n print ( ’Fail’ )\n text )\n text )\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "Among them, ˆ represents matching the beginning of a string, d\nrepresents matching a digit, 9 represents matching 9 digits, and $\nrepresents matching the end of a string. Combining these symbols,\nwe can obtain a regular expression that matches 11 digits.\nWhen testing for a match, use the re .match() function to attempt\nto match the text. A successful match returns a Match object;\notherwise, it returns None. In this example, the first test matches\nsuccessfully, while the second test fails to match.\n 52\n",
        "font-size": 10,
        "token": 116
      }
    ]
  },
  {
    "page_num": 53,
    "content": [
      {
        "text": "GPT-3.5\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "使用Python的re模块可以很方便地实现正则表达式匹配。以\n下是一个匹配所有以139开头，以00结尾的11位数字的示例\n代码：\n 1043\n import re\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "text = ”139123456700”\npattern = r”ˆ139d800$”\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "match = re .match( pattern ,\nif match:\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "print (”匹配成功”)\n text )\n else :\n",
        "font-size": 9,
        "token": 11
      },
      {
        "text": "print (”匹配失败”)\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "输出结果为：\n匹配成功\n代码中，首先定义了一个文本字符串 text ，用于测试正则表\n达式的匹配。接着定义了一个正则表达式 pattern ，其中ˆ139\n表示以139开头，d8表示任意8位数字，00$表示以00结尾。\n然 后 使 用re .match()函 数 进 行 匹 配 ， 如 果 匹 配 成 功 ， 则 输\n出”匹配成功”，否则输出”匹配失败”。\nThe re module in Python can conveniently accomplish regular\nexpression matching. Below is an example code that matches all\n11-digit numbers beginning with 139 and ending with 00.\n import re\n",
        "font-size": 9,
        "token": 157
      },
      {
        "text": "text = ”139123456700”\npattern = r”ˆ139d800$”\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "match = re .match( pattern ,\nif match:\n text )\n print (”Success”)\n else :\n print (”Fail”)\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "Output:\nSuccess\nIn the code, a text string ”text” is first defined for testing regular\nexpression matching. Then, a regular expression ”pattern” is de-\nfined, where ”ˆ139” means starting with 139, ”d8” means any eight\ndigits, and ”00$” means ending with 00. Then, the re .match()\nfunction is used for matching. If the match is successful, ”Success”\nis outputted; otherwise, ”Fail” is outputted.\n 53\n",
        "font-size": 10,
        "token": 115
      }
    ]
  },
  {
    "page_num": 54,
    "content": [
      {
        "text": "GPT-4\n",
        "font-size": 9,
        "token": 5
      },
      {
        "text": "下面是你需要的python代码，这段代码中使用了python的re模\n块。\n 1134\n import re\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "def match phone number(string) :\npattern = ”ˆ139d600$”\nreturn re .match( pattern , string ) is not None\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "# 测试\nprint (match phone number(”13912345600”)) # 输出: True\nprint (match phone number(”15912345600”)) # 输出: False\nprint (match phone number(”13912345601”)) # 输出: False\nprint (match phone number(”139123456000”)) # 输出: False\n",
        "font-size": 9,
        "token": 105
      },
      {
        "text": "在这里，”ˆ139d600$”是正则表达式。”ˆ”表示匹配字符串的\n开 始 ，”139”匹 配 字 符 串 中 的”139”，”d6”匹 配 任 意 六 个 数\n字，”00”匹配字符串中的”00”，“$”表示匹配字符串的结束。\n因此，这个正则表达式可以匹配到所有以139开头，以00结\n尾的11位数字。\nBelow is the python code you need. This code uses the python re\nmodule.\n import re\n",
        "font-size": 9,
        "token": 140
      },
      {
        "text": "def match phone number(string) :\npattern = ”ˆ139d600$”\nreturn re .match( pattern , string ) is not None\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "# Test\nprint (match phone number(”13912345600”)) # Output: True\nprint (match phone number(”15912345600”)) # Output: False\nprint (match phone number(”13912345601”)) # Output: False\nprint (match phone number(”139123456000”)) # Output: False\n",
        "font-size": 9,
        "token": 104
      },
      {
        "text": "In here, ”ˆ139d600$” is a regular expression. ”ˆ” represents the\nbeginning of the string, ”139” matches the ”139” in the string, ”d6”\nmatches any six digits, ”00” matches the ”00” in the string, and ”$”\nrepresents the end of the string. Therefore, this regular expression\ncan match any 11-digit number that starts with 139 and ends with\n00.\n",
        "font-size": 9,
        "token": 110
      },
      {
        "text": "Code\n我用下面代码求二叉树的最大深度，但是答案不对，怎么回事呢？\n d e f maxDepth ( s e l f ,\n r o o t : TreeNode ) −> i n t :\n",
        "font-size": 9,
        "token": 44
      },
      {
        "text": "r o o t : r e t u r n 0\nr e s = [ r o o t ] , 0\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "i f n o t\nqueue ,\nw h i l e q u e u e :\ntmp = [ ]\nf o r node i n q u e u e :\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "i f node . l e f t :\ni f node . r i g h t :\n tmp . a p p e n d ( node . l e f t )\n tmp . a p p e n d ( node . l e f t )\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "q u e u e = tmp\nr e s += 1\n r e t u r n r e s\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "I use the following code to find the maximum depth of a binary tree, but the answer is incorrect. What\ncould be the problem?\n 54\n",
        "font-size": 10,
        "token": 32
      }
    ]
  },
  {
    "page_num": 55,
    "content": [
      {
        "text": "d e f maxDepth ( s e l f ,\n r o o t : TreeNode ) −> i n t :\n",
        "font-size": 9,
        "token": 24
      },
      {
        "text": "r o o t : r e t u r n 0\nr e s = [ r o o t ] , 0\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "i f n o t\nqueue ,\nw h i l e q u e u e :\ntmp = [ ]\nf o r node i n q u e u e :\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "i f node . l e f t :\ni f node . r i g h t :\n tmp . a p p e n d ( node . l e f t )\n tmp . a p p e n d ( node . l e f t )\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "q u e u e = tmp\nr e s += 1\n r e t u r n r e s\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "Responses\n",
        "font-size": 12,
        "token": 2
      },
      {
        "text": "Response\n",
        "font-size": 9,
        "token": 2
      },
      {
        "text": "Model\nQwen-7B-Chat (SFT) 你的代码是正确的，答案是正确的。在你的代码中，你首\n先将根节点添加到队列中，然后进入一个循环，直到队列为\n空。在每次循环中，你都会将队列中的所有节点的左子节点\n和右子节点添加到队列中。然后，你增加深度计数器res。\n最后，你返回深度计数器res。\nYour code is correct, the answer is correct. In your code, you first\nadd the root node to the queue, then enter a loop until the queue\nis empty. In each loop, you add the left child node and right child\nnode of all nodes in the queue to the queue. Then, you increase the\ndepth counter res. Finally, you return the depth counter res.\n",
        "font-size": 9,
        "token": 180
      },
      {
        "text": "Elo\n876\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "Qwen-14B-Chat (SFT) 你的代码会返回二叉树的节点数量，而不是最大深度。这是\n因为你在每次循环中都增加了res的值，而不是只在找到新\n的最深节点时增加。你可以通过在找到新的最深节点时才增\n加res的值来修复这个问题。以下是修复后的代码：\ndef maxDepth(self, root : TreeNode) −> int:\n 949\n",
        "font-size": 9,
        "token": 97
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nif len(queue) > res :\nres = len(queue)\n return res\n",
        "font-size": 9,
        "token": 19
      },
      {
        "text": "Your code will return the number of nodes in the binary tree, not\nthe maximum depth. This is because you increase the value of ”res”\nin every loop, instead of only increasing it when a new deepest\nnode is found. You can fix this problem by increasing the value of\n”res” only when a new deepest node is found. Here is the modified\ncode:\ndef maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 91
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nif len(queue) > res :\nres = len(queue)\n return res\n 55\n",
        "font-size": 10,
        "token": 23
      },
      {
        "table": "|def maxDepth(self , root: TreeNode) −> int: if not root: return 0 queue , res = [root], 0 while queue: tmp = [] for node in queue: if node. left : tmp.append(node. left ) if node. right : tmp.append(node. left ) queue = tmp res += 1 return res|\n|Responses|\n|Model Response Elo Qwen-7B-Chat(SFT) 你的代码是正确的，答案是正确的。在你的代码中，你首 876 先将根节点添加到队列中，然后进入一个循环，直到队列为 空。在每次循环中，你都会将队列中的所有节点的左子节点 和右子节点添加到队列中。然后，你增加深度计数器res。 最后，你返回深度计数器res。 Yourcodeiscorrect,theansweriscorrect.Inyourcode,youfirst addtherootnodetothequeue,thenenteraloopuntilthequeue isempty.Ineachloop,youaddtheleftchildnodeandrightchild nodeofallnodesinthequeuetothequeue.Then,youincreasethe depthcounter res.Finally,youreturnthedepthcounter res. Qwen-14B-Chat(SFT) 你的代码会返回二叉树的节点数量，而不是最大深度。这是 949 因为你在每次循环中都增加了res的值，而不是只在找到新 的最深节点时增加。你可以通过在找到新的最深节点时才增 加res的值来修复这个问题。以下是修复后的代码： def maxDepth(self, root: TreeNode)−>int: if not root: return 0 queue, res = [root], 0 while queue: tmp= [] for node in queue: if node.left: tmp.append(node.left) if node.right: tmp.append(node.right) queue= tmp if len(queue) >res: res = len(queue) return res Yourcodewillreturnthenumberofnodesinthebinarytree,not themaximumdepth.Thisisbecauseyouincreasethevalueof”res” ineveryloop, insteadofonlyincreasingitwhenanewdeepest nodeisfound.Youcanfixthisproblembyincreasingthevalueof ”res”onlywhenanewdeepestnodeisfound.Hereisthemodified code: def maxDepth(self, root: TreeNode)−>int: if not root: return 0 queue, res = [root], 0 while queue: tmp= [] for node in queue: if node.left: tmp.append(node.left) if node.right: tmp.append(node.right) queue= tmp if len(queue) >res: res = len(queue) return res|",
        "token": 567
      }
    ]
  },
  {
    "page_num": 56,
    "content": [
      {
        "text": "Qwen-14B-Chat (RLHF) 这个代码的问题在于它没有正确地处理左右子树的深度。具\n体来说，每次从队列中取出一个节点时，它会将该节点的左\n右子树都加入到队列中，然后将队列长度加一。但是，如果\n该节点的左右子树中有一个为空，那么这个节点的深度就应\n该是它所在队列的长度减一，而不是队列长度加一。\n 992\n 以下是修改后的代码：\n def maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 134
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres = max(res, len(queue))\n return res\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "在这个代码中，每次取出一个节点时，我们只将该节点的左\n右子树中非空的部分加入到队列中，并计算队列的长度。然\n后，我们将队列长度与当前最大深度进行比较，并更新最大\n深度。这样就可以正确地计算出二叉树的最大深度了。\nThe problem with this code is that it does not correctly handle the\ndepth of the left and right subtrees. Specifically, when a node is\ntaken out from the queue, it adds both its left and right subtrees to\nthe queue and increases the length of the queue by one. However,\nif either the left or right subtree of the node is empty, then the depth\nof this node should be the length of the queue minus one, not the\nlength of the queue plus one.\n",
        "font-size": 9,
        "token": 171
      },
      {
        "text": "Here is the modified code:\ndef maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 19
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres = max(res, len(queue))\n return res\n",
        "font-size": 9,
        "token": 15
      },
      {
        "text": "In this code, when each node is extracted, we only add the non-\nempty parts of its left and right subtrees to the queue, and calculate\nthe length of the queue. Then, we compare the length of the queue\nwith the current maximum depth and update the maximum depth.\nThis way, we can correctly calculate the maximum depth of the\nbinary tree.\n 56\n",
        "font-size": 10,
        "token": 79
      }
    ]
  },
  {
    "page_num": 57,
    "content": [
      {
        "text": "GPT-3.5\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "问题出在第11行代码，在将node. right 添加到ttmp列表时，\n应 该 是tmp.append(node. right )而 不 是tmp.append(node. left )\n。修改后的代码如下：\n 1090\n def maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 64
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres += 1\n return res\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "The problem lies in line 11 of the code. When adding node. right\nto the tmp list, it should be tmp.append(node. right ) instead of tmp\n.append(node. left ). The modified code is as follows:\ndef maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 61
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres += 1\n return res\n GPT-4\n 你的代码中有一个错误：\n 1095\n",
        "font-size": 9,
        "token": 30
      },
      {
        "text": "你在这一行代码 if node. right : tmp.append(node. left ) 中应\n该是想把右子节点加入到tmp列表中，但是你写的是node.\nleft ，这样就只会把左子节点加入到tmp列表中。正确的应\n该是node. right 。\n 正确的代码如下：\n def maxDepth(self, root : TreeNode) −> int:\n",
        "font-size": 9,
        "token": 82
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres += 1\n return res\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "这种解法的思路是：每次迭代时，都把当前层的所有节点\n遍历一遍，同时把下一层的所有节点都加到下一次迭代的队\n列中。所以每次迭代完，深度res就加1。直到遍历的队列为\n空，此时的深度就是二叉树的最大深度。\n 57\n",
        "font-size": 10,
        "token": 74
      }
    ]
  },
  {
    "page_num": 58,
    "content": [
      {
        "text": "There is an error in your code:\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "In this line of code if node. right : tmp.append(node. left ), you\nshould be adding the right child node to the tmp list, but you wrote\nnode. left instead, so only the left child node will be added to the\ntmp list. The correct code should be node. right .\n",
        "font-size": 9,
        "token": 63
      },
      {
        "text": "The correct code is as follows: def maxDepth(self, root :\nTreeNode) −> int:\n",
        "font-size": 9,
        "token": 20
      },
      {
        "text": "if not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\n",
        "font-size": 9,
        "token": 29
      },
      {
        "text": "if node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\n",
        "font-size": 9,
        "token": 22
      },
      {
        "text": "queue = tmp\nres += 1\n return res\n",
        "font-size": 9,
        "token": 12
      },
      {
        "text": "The idea of this solution is: in each iteration, traverse all the nodes\nin the current level, and add all the nodes in the next level to the\nqueue for the next iteration. So after each iteration, the depth res is\nincreased by 1. Continue this process until the queue for traversal\nis empty, and the depth at that time is the maximum depth of the\nbinary tree.\n A.3 ANALYSIS OF CODE INTERPRETER\n",
        "font-size": 10,
        "token": 94
      },
      {
        "text": "Here we provide a case of comparison between CODE LLAMA and QWEN-CHAT. This case\ndemonstrates the advantages of QWEN-CHAT in processing tabular data and performing complex\ntasks.\n 58\n",
        "font-size": 10,
        "token": 48
      }
    ]
  },
  {
    "page_num": 59,
    "content": [
      {
        "text": "Figure 5: Example showcasing QWEN-CHAT’s ability in using a code interpreter via ReAct prompting.\nThe ReAct instruction is omitted for clarity. QWEN creates a two-step plan and first investigates\nthe columns present in the CSV file before proceeding to draw the plot, as shown in the top-left\nfigure. CODE LLAMA, however, attempts to draw the plot based on non-existent columns in its\ninitial attempt, as seen in the bottom figure. CODE LLAMA can only reliably perform the task if the\ncolumns are provided in the user query, as shown in the top-right figure.\n 59\n",
        "font-size": 10,
        "token": 128
      }
    ]
  }
]