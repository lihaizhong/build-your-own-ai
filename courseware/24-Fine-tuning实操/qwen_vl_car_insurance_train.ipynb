{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a69d31-b2a7-4ad0-94b7-387a7bbfa679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 22:10:53.819040: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-03 22:10:53.840407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756908653.865471   52134 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756908653.873280   52134 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756908653.892974   52134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756908653.892994   52134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756908653.892997   52134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756908653.892999   52134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-03 22:10:53.899794: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 09-03 22:11:01 __init__.py:183] Automatically detected platform cuda.\n",
      "æ­£åœ¨åŠ è½½Qwen2.5-VL-3Bæ¨¡å‹...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2_5_Vl patching. Transformers: 4.51.3. vLLM: 0.7.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.65 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c233af2a737045f4b4228acf7a385c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®LoRAå‚æ•°...\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Qwen2-VL 3B è§†è§‰æ¨¡å‹å¾®è°ƒè„šæœ¬ - æ±½è½¦ä¿é™©æ‰¿ä¿ä¸“å®¶\n",
    "åŸºäºUnslothæ¡†æ¶è¿›è¡Œé«˜æ•ˆå¾®è°ƒ\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from unsloth import FastVisionModel  \n",
    "import torch\n",
    "\n",
    "# æ”¯æŒçš„4bité¢„é‡åŒ–æ¨¡å‹åˆ—è¡¨\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\", \n",
    "#     \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "# ]\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½Qwen2.5-VL-3Bæ¨¡å‹...\")\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"/root/autodl-tmp/models/Qwen/Qwen2.5-VL-3B-Instruct\",  # ä½¿ç”¨Qwen2.5-VL-3Bæ¨¡å‹\n",
    "    #load_in_4bit = True,  # ä½¿ç”¨4bité‡åŒ–å‡å°‘æ˜¾å­˜ä½¿ç”¨\n",
    "    use_gradient_checkpointing = \"unsloth\",  # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜\n",
    ")\n",
    "\n",
    "print(\"é…ç½®LoRAå‚æ•°...\")\n",
    "# é…ç½®LoRAé€‚é…å™¨è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers = True,  # å¾®è°ƒè§†è§‰å±‚\n",
    "    finetune_language_layers = True,  # å¾®è°ƒè¯­è¨€å±‚  \n",
    "    finetune_attention_modules = True,  # å¾®è°ƒæ³¨æ„åŠ›æ¨¡å—\n",
    "    finetune_mlp_modules = True,  # å¾®è°ƒMLPæ¨¡å—\n",
    "    \n",
    "    r = 16,  # LoRAç§©ï¼Œè¶Šå¤§ç²¾åº¦è¶Šé«˜ä½†å¯èƒ½è¿‡æ‹Ÿåˆ\n",
    "    lora_alpha = 16,  # LoRA alphaå‚æ•°ï¼Œå»ºè®®ç­‰äºr\n",
    "    lora_dropout = 0,  # LoRA dropout\n",
    "    bias = \"none\",  # åç½®è®¾ç½®\n",
    "    random_state = 3407,  # éšæœºç§å­\n",
    "    use_rslora = False,  # æ˜¯å¦ä½¿ç”¨rank stabilized LoRA\n",
    "    loftq_config = None,  # LoftQé…ç½®\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430d06d-52d1-4886-b776-c3e8b8294922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½è®­ç»ƒæ•°æ®...\n",
      "Excelæ–‡ä»¶åˆ—å: ['id', 'prompt', 'image', 'response']\n",
      "æ•°æ®é›†å½¢çŠ¶: (2, 4)\n",
      "æˆåŠŸå¤„ç†æ ·æœ¬ 1: images/1-vehicle-odometer-reading.jpg\n",
      "æˆåŠŸå¤„ç†æ ·æœ¬ 2: images/2-vehicle-odometer-reading.jpg\n",
      "æˆåŠŸåŠ è½½ 2 ä¸ªè®­ç»ƒæ ·æœ¬\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
    "# åŠ è½½Excelæ ¼å¼çš„è®­ç»ƒæ•°æ®\n",
    "def load_excel_dataset(file_path):\n",
    "    \"\"\"åŠ è½½Excelæ ¼å¼çš„æ•°æ®é›†\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Excelæ–‡ä»¶åˆ—å: {list(df.columns)}\")\n",
    "        print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "# æ•°æ®è½¬æ¢å‡½æ•° - å¤„ç†Excelæ ¼å¼æ•°æ®\n",
    "def convert_excel_to_training_format(df):\n",
    "    \"\"\"å°†Excelæ ¼å¼è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼ŒåŠ è½½æœ¬åœ°å›¾ç‰‡\"\"\"\n",
    "    converted_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Excelæ ¼å¼: id, prompt, image, response\n",
    "        image_path = row[\"image\"]\n",
    "        prompt = row[\"prompt\"]\n",
    "        response = row[\"response\"]\n",
    "        \n",
    "        if pd.notna(image_path) and os.path.exists(image_path):\n",
    "            try:\n",
    "                # åŠ è½½å›¾ç‰‡\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # åˆ›å»ºè®­ç»ƒæ ·æœ¬\n",
    "                conversation = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": prompt},\n",
    "                                {\"type\": \"image\", \"image\": image}\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\", \n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": response}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                converted_data.append(conversation)\n",
    "                print(f\"æˆåŠŸå¤„ç†æ ·æœ¬ {idx + 1}: {image_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"å¤„ç†å›¾ç‰‡ {image_path} æ—¶å‡ºé”™: {e}\")\n",
    "        else:\n",
    "            print(f\"è­¦å‘Šï¼šå›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸ºç©º {image_path}\")\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "# åŠ è½½å¹¶è½¬æ¢æ•°æ®é›†\n",
    "train_df = load_excel_dataset(\"qwen-vl-train.xlsx\")\n",
    "if train_df is not None:\n",
    "    converted_dataset = convert_excel_to_training_format(train_df)\n",
    "else:\n",
    "    print(\"æ— æ³•åŠ è½½æ•°æ®é›†ï¼Œç¨‹åºé€€å‡º\")\n",
    "    exit()\n",
    "\n",
    "print(f\"æˆåŠŸåŠ è½½ {len(converted_dataset)} ä¸ªè®­ç»ƒæ ·æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa5b3ea-846c-47b7-aff0-553e7d6b8cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'ä½ æ˜¯ä¸€åæ±½è½¦ä¿é™©æ‰¿ä¿ä¸“å®¶ã€‚è¿™é‡Œæœ‰ä¸€å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ã€‚è¯·ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚'},\n",
       "     {'type': 'image',\n",
       "      'image': <PIL.Image.Image image mode=RGB size=426x320>}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'è¿™å¼ è½¦è¾†é‡Œç¨‹è¡¨å›¾ç‰‡æ˜¾ç¤ºäº†ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š\\n\\n1. **æ€»é‡Œç¨‹**ï¼š528,915å…¬é‡Œ\\n2. **å½“å‰é€Ÿåº¦**ï¼š0å…¬é‡Œ/å°æ—¶\\n3. **å½“å‰æ—¶é—´**ï¼š19:18\\n4. **å½“å‰æ¸©åº¦**ï¼š+4.5Â°C\\n5. **å½“å‰æŒ¡ä½**ï¼šåœè½¦æŒ¡ï¼ˆPï¼‰'}]}]},\n",
       " {'messages': [{'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'ä½ æ˜¯ä¸€åæ±½è½¦ä¿é™©æ‰¿ä¿ä¸“å®¶ã€‚è¿™é‡Œæœ‰ä¸€å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ã€‚è¯·ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚'},\n",
       "     {'type': 'image',\n",
       "      'image': <PIL.Image.Image image mode=RGB size=800x445>}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'ä»è¿™å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ä¸­ï¼Œå¯ä»¥æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š\\n\\n1. **å½“å‰é€Ÿåº¦**ï¼š56.6è‹±é‡Œ/å°æ—¶ã€‚\\n2. **æ€»é‡Œç¨‹**ï¼š37756è‹±é‡Œã€‚\\n3. **å½“å‰æ—¶é—´**ï¼š11:14ã€‚\\n4. **å½“å‰æ¸©åº¦**ï¼š10.5Â°Cã€‚\\n5. **å½“å‰æŒ¡ä½**ï¼šPï¼ˆåœè½¦æŒ¡ï¼‰ã€‚'}]}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58574836-f35e-4330-acac-239063c1937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ç»“æ„ï¼š\n",
      "ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬: ä½ æ˜¯ä¸€åæ±½è½¦ä¿é™©æ‰¿ä¿ä¸“å®¶ã€‚è¿™é‡Œæœ‰ä¸€å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ã€‚è¯·ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚\n",
      "åŠ©æ‰‹å›å¤: è¿™å¼ è½¦è¾†é‡Œç¨‹è¡¨å›¾ç‰‡æ˜¾ç¤ºäº†ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š\n",
      "\n",
      "1. **æ€»é‡Œç¨‹**ï¼š528,915å…¬é‡Œ\n",
      "2. **å½“å‰é€Ÿåº¦**ï¼š0å…¬é‡Œ/å°æ—¶\n",
      "3. **å½“å‰æ—¶é—´**ï¼š19:18\n",
      "4. **å½“å‰æ¸©åº¦**ï¼š+4.5Â°C\n",
      "5. **å½“å‰æŒ¡ä½**ï¼šåœè½¦æŒ¡ï¼ˆPï¼‰\n",
      "\n",
      "è®­ç»ƒå‰æ¨¡å‹æ¨ç†æµ‹è¯•...\n",
      "è®­ç»ƒå‰æ¨¡å‹è¾“å‡º:\n",
      "ä»æä¾›çš„å›¾ç‰‡ä¸­æå–çš„å…³é”®ä¿¡æ¯å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. **é€Ÿåº¦è¡¨**ï¼š\n",
      "   - å½“å‰é€Ÿåº¦æ˜¾ç¤ºä¸º20å…¬é‡Œ/å°æ—¶ã€‚\n",
      "\n",
      "2. **é‡Œç¨‹æ•°**ï¼š\n",
      "   - ç°åœ¨è½¦è¾†çš„æ€»è¡Œé©¶é‡Œç¨‹æ˜¯529891.5å…¬é‡Œã€‚\n",
      "\n",
      "3. **è½¦å†µ**ï¼š\n",
      "   - è¡Œé©¶è·ç¦»ï¼š2600000.7å…¬é‡Œã€‚\n",
      "   - åˆ¶åŠ¨æµ‹è¯•ï¼š19,181.3å…¬é‡Œã€‚\n",
      "\n",
      "4. **æ²¹è€—**ï¼š\n",
      "   - é«˜é€Ÿæ®µç‡ƒæ²¹æ¶ˆè€—ä¸º44å‡ï¼ˆå¹³å‡ï¼‰ã€‚\n",
      "   - åŸå¸‚\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æ„\n",
    "print(\"ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ç»“æ„ï¼š\")\n",
    "print(f\"ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬: {converted_dataset[0]['messages'][0]['content'][0]['text']}\")\n",
    "print(f\"åŠ©æ‰‹å›å¤: {converted_dataset[0]['messages'][1]['content'][0]['text']}\")\n",
    "\n",
    "# è®­ç»ƒå‰æ¨ç†æµ‹è¯•\n",
    "print(\"\\nè®­ç»ƒå‰æ¨¡å‹æ¨ç†æµ‹è¯•...\")\n",
    "FastVisionModel.for_inference(model)  # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼\n",
    "\n",
    "# åŠ è½½æµ‹è¯•å›¾ç‰‡\n",
    "test_image = Image.open(\"images/1-vehicle-odometer-reading.jpg\").convert('RGB')\n",
    "test_instruction = \"ä½ æ˜¯ä¸€åæ±½è½¦ä¿é™©æ‰¿ä¿ä¸“å®¶ã€‚è¿™é‡Œæœ‰ä¸€å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ã€‚è¯·ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚\"\n",
    "\n",
    "# æ„å»ºæ¨ç†è¾“å…¥\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": test_instruction}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    test_image,\n",
    "    input_text, \n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# ç”Ÿæˆå›å¤\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "print(\"è®­ç»ƒå‰æ¨¡å‹è¾“å‡º:\")\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                   use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b5e8b6-d43b-4fb7-96c8-57ffbdb8834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è®­ç»ƒæ¨¡å‹...\n",
      "Unsloth: Model does not have a default image size - using 512\n",
      "GPU = NVIDIA GeForce RTX 4090. æœ€å¤§æ˜¾å­˜ = 23.65 GB.\n",
      "3.867 GB æ˜¾å­˜å·²ä½¿ç”¨.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2 | Num Epochs = 30 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928/3,000,000,000 (1.37% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:27, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "è®­ç»ƒç”¨æ—¶: 50.2063 ç§’\n",
      "è®­ç»ƒç”¨æ—¶: 0.84 åˆ†é’Ÿ\n",
      "å³°å€¼æ˜¾å­˜ä½¿ç”¨: 3.867 GB\n",
      "LoRAè®­ç»ƒæ˜¾å­˜ä½¿ç”¨: 0.0 GB\n",
      "æ˜¾å­˜ä½¿ç”¨ç‡: 16.351%\n",
      "LoRAæ˜¾å­˜ä½¿ç”¨ç‡: 0.0%\n",
      "\n",
      "è®­ç»ƒåæ¨¡å‹æ¨ç†æµ‹è¯•...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2_5_VLForConditionalGeneration(\n",
       "      (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "        (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "        (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "        (blocks): ModuleList(\n",
       "          (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "            (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (attn): Qwen2_5_VLVisionSdpaAttention(\n",
       "              (qkv): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2_5_VLMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (merger): Qwen2_5_VLPatchMerger(\n",
       "          (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (model): Qwen2_5_VLModel(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "            (self_attn): Qwen2_5_VLSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "# é…ç½®è®­ç»ƒå™¨\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # å¿…é¡»ä½¿ç”¨è§†è§‰æ•°æ®æ•´ç†å™¨\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,  # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°\n",
    "        gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "        warmup_steps=5,  # é¢„çƒ­æ­¥æ•°\n",
    "        max_steps=30,  # è®­ç»ƒæ­¥æ•°ï¼Œå¯æ”¹ä¸º num_train_epochs=1 è¿›è¡Œå®Œæ•´è®­ç»ƒ\n",
    "        learning_rate=2e-4,  # å­¦ä¹ ç‡\n",
    "        logging_steps=1,  # æ—¥å¿—è®°å½•æ­¥æ•°\n",
    "        optim=\"adamw_8bit\",  # ä½¿ç”¨8bit AdamWä¼˜åŒ–å™¨\n",
    "        weight_decay=0.01,  # æƒé‡è¡°å‡\n",
    "        lr_scheduler_type=\"linear\",  # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦\n",
    "        seed=3407,  # éšæœºç§å­\n",
    "        output_dir=\"outputs\",  # è¾“å‡ºç›®å½•\n",
    "        report_to=\"none\",  # ä¸ä½¿ç”¨wandbç­‰è®°å½•å·¥å…·\n",
    "        \n",
    "        # è§†è§‰å¾®è°ƒå¿…éœ€é…ç½®\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    ),\n",
    "    max_seq_length=2048,  # åºåˆ—æœ€å¤§é•¿åº¦å‚æ•°ç§»åˆ°è¿™é‡Œ\n",
    ")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. æœ€å¤§æ˜¾å­˜ = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB æ˜¾å­˜å·²ä½¿ç”¨.\")\n",
    "\n",
    "# æ‰§è¡Œè®­ç»ƒ\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# æ˜¾ç¤ºè®­ç»ƒå®Œæˆåçš„æ˜¾å­˜å’Œæ—¶é—´ç»Ÿè®¡\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"è®­ç»ƒç”¨æ—¶: {trainer_stats.metrics['train_runtime']} ç§’\")\n",
    "print(f\"è®­ç»ƒç”¨æ—¶: {round(trainer_stats.metrics['train_runtime']/60, 2)} åˆ†é’Ÿ\")\n",
    "print(f\"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {used_memory} GB\")\n",
    "print(f\"LoRAè®­ç»ƒæ˜¾å­˜ä½¿ç”¨: {used_memory_for_lora} GB\")\n",
    "print(f\"æ˜¾å­˜ä½¿ç”¨ç‡: {used_percentage}%\")\n",
    "print(f\"LoRAæ˜¾å­˜ä½¿ç”¨ç‡: {lora_percentage}%\")\n",
    "\n",
    "# è®­ç»ƒåæ¨ç†æµ‹è¯•\n",
    "print(\"\\nè®­ç»ƒåæ¨¡å‹æ¨ç†æµ‹è¯•...\")\n",
    "FastVisionModel.for_inference(model)  # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e18fec4-6e41-47a6-a648-cbdaa07b48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒåæ¨¡å‹è¾“å‡º:\n",
      "ä»è¿™å¼ è½¦è¾†é‡Œç¨‹è¡¨çš„å›¾ç‰‡ä¸­ï¼Œå¯ä»¥æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š\n",
      "\n",
      "1. **æ€»é‡Œç¨‹**ï¼š528,915å…¬é‡Œã€‚\n",
      "2. **å½“å‰é€Ÿåº¦**ï¼š0å…¬é‡Œ/å°æ—¶ã€‚\n",
      "3. **å½“å‰æ—¶é—´**ï¼š19:18ï¼ˆæ™šï¼‰ã€‚\n",
      "4. **å½“å‰æ¸©åº¦**ï¼š+4.6Â°Cã€‚\n",
      "5. **å½“å‰æŒ¡ä½**ï¼šåœè½¦æŒ¡ã€‚<|im_end|>\n",
      "\n",
      "ä¿å­˜LoRAé€‚é…å™¨...\n",
      "è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° car_insurance_lora_model ç›®å½•\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ ·æœ¬\n",
    "inputs = tokenizer(\n",
    "    test_image,\n",
    "    input_text,\n",
    "    add_special_tokens=False, \n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"è®­ç»ƒåæ¨¡å‹è¾“å‡º:\")\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                   use_cache=True, temperature=1.5, min_p=0.1)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "print(\"\\nä¿å­˜LoRAé€‚é…å™¨...\")\n",
    "model.save_pretrained(\"car_insurance_lora_model\")  # æœ¬åœ°ä¿å­˜\n",
    "tokenizer.save_pretrained(\"car_insurance_lora_model\")\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° car_insurance_lora_model ç›®å½•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fa76f-acbf-4952-b63c-3a6a61d9fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastVisionModel\n",
    "# model, tokenizer = FastVisionModel.from_pretrained(\n",
    "#     model_name=\"car_insurance_lora_model\",\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "# FastVisionModel.for_inference(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
