{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3ad4f-9492-45c4-a693-570ecb300582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "å±äº InternVL 2.5ç³»åˆ—\n",
    "è§†é¢‘ç†è§£ä¸ç”Ÿæˆï¼šå¯ä»¥ç”¨äºè§†é¢‘å†…å®¹çš„åˆ†æã€æ€»ç»“å’Œç”Ÿæˆç›¸å…³çš„æ–‡æœ¬æè¿°ã€‚\n",
    "è§†è§‰é—®ç­”ï¼šèƒ½å¤Ÿå›ç­”ä¸å›¾åƒæˆ–è§†é¢‘å†…å®¹ç›¸å…³çš„é—®é¢˜ã€‚\n",
    "å¤šæ¨¡æ€å¯¹è¯ï¼šæ”¯æŒä¸ç”¨æˆ·è¿›è¡ŒåŒ…å«è§†è§‰ä¿¡æ¯çš„å¯¹è¯ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cd027c-59e9-467e-b198-db28f1a11397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B\n"
     ]
    }
   ],
   "source": [
    "#æ¨¡å‹ä¸‹è½½ï¼Œéœ€è¦ä¸‹è½½3ä¸ªå¤§æ¨¡å‹\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('OpenGVLab/InternVideo2_5_Chat_8B', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('internlm/internlm2_5-7b-chat', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('LLM-Research/Mistral-7B-Instruct-v0.3', cache_dir='/root/autodl-tmp/models')\n",
    "#model_dir = snapshot_download('AI-ModelScope/bert-base-uncased', cache_dir='/root/autodl-tmp/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0cfc4c-d3b1-47d0-ba82-8c4754ad5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:49:09.167664: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 08:49:09.185359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747270149.210169  914572 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747270149.216043  914572 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747270149.232877  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232894  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232895  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747270149.232897  914572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 08:49:09.239558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b0826d7220431ebb241282f1aafffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from modelscope import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "model_path = '/root/autodl-tmp/models/OpenGVLab/InternVideo2_5_Chat_8B'\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)\n",
    "\n",
    "# ImageNet æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"\n",
    "    æ„å»ºå›¾åƒè½¬æ¢pipeline\n",
    "    \n",
    "    å‚æ•°:\n",
    "        input_size: è¾“å…¥å›¾åƒå¤§å°\n",
    "    \n",
    "    è¿”å›:\n",
    "        transform: è½¬æ¢pipeline\n",
    "    \"\"\"\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), \n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), \n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"\n",
    "    å¯»æ‰¾æœ€æ¥è¿‘åŸå§‹å›¾åƒå®½é«˜æ¯”çš„ç›®æ ‡æ¯”ä¾‹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        aspect_ratio: åŸå§‹å›¾åƒçš„å®½é«˜æ¯”\n",
    "        target_ratios: ç›®æ ‡æ¯”ä¾‹åˆ—è¡¨\n",
    "        width: åŸå§‹å›¾åƒå®½åº¦\n",
    "        height: åŸå§‹å›¾åƒé«˜åº¦\n",
    "        image_size: ç›®æ ‡å›¾åƒå¤§å°\n",
    "        \n",
    "    è¿”å›:\n",
    "        best_ratio: æœ€ä½³æ¯”ä¾‹\n",
    "    \"\"\"\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    åŠ¨æ€é¢„å¤„ç†å›¾åƒï¼Œæ ¹æ®å®½é«˜æ¯”å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªå—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image: åŸå§‹å›¾åƒ\n",
    "        min_num: æœ€å°å—æ•°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        image_size: ç›®æ ‡å›¾åƒå¤§å°\n",
    "        use_thumbnail: æ˜¯å¦ä½¿ç”¨ç¼©ç•¥å›¾\n",
    "        \n",
    "    è¿”å›:\n",
    "        processed_images: å¤„ç†åçš„å›¾åƒåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # è®¡ç®—ç°æœ‰å›¾åƒå®½é«˜æ¯”\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡çš„å®½é«˜æ¯”\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # è®¡ç®—ç›®æ ‡å®½åº¦å’Œé«˜åº¦\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # è°ƒæ•´å›¾åƒå¤§å°\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, \n",
    "               ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n",
    "        # åˆ†å‰²å›¾åƒ\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=6):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¹¶å¤„ç†å›¾åƒ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image: è¾“å…¥å›¾åƒ\n",
    "        input_size: è¾“å…¥å¤§å°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        pixel_values: å¤„ç†åçš„å›¾åƒå¼ é‡\n",
    "    \"\"\"\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    \"\"\"\n",
    "    è·å–è§†é¢‘å¸§ç´¢å¼•\n",
    "    \n",
    "    å‚æ•°:\n",
    "        bound: æ—¶é—´è¾¹ç•Œ [å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´]\n",
    "        fps: è§†é¢‘å¸§ç‡\n",
    "        max_frame: æœ€å¤§å¸§æ•°\n",
    "        first_idx: ç¬¬ä¸€å¸§ç´¢å¼•\n",
    "        num_segments: åˆ†æ®µæ•°é‡\n",
    "        \n",
    "    è¿”å›:\n",
    "        frame_indices: å¸§ç´¢å¼•æ•°ç»„\n",
    "    \"\"\"\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n",
    "    return frame_indices\n",
    "\n",
    "def get_num_frames_by_duration(duration):\n",
    "    \"\"\"\n",
    "    æ ¹æ®è§†é¢‘æ—¶é•¿è®¡ç®—å¸§æ•°\n",
    "    \n",
    "    å‚æ•°:\n",
    "        duration: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰\n",
    "        \n",
    "    è¿”å›:\n",
    "        num_frames: è®¡ç®—å‡ºçš„å¸§æ•°\n",
    "    \"\"\"\n",
    "    local_num_frames = 4        \n",
    "    num_segments = int(duration // local_num_frames)\n",
    "    if num_segments == 0:\n",
    "        num_frames = local_num_frames\n",
    "    else:\n",
    "        num_frames = local_num_frames * num_segments\n",
    "    \n",
    "    num_frames = min(512, num_frames)\n",
    "    num_frames = max(128, num_frames)\n",
    "\n",
    "    return num_frames\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¹¶å¤„ç†è§†é¢‘\n",
    "    \n",
    "    å‚æ•°:\n",
    "        video_path: è§†é¢‘è·¯å¾„\n",
    "        bound: æ—¶é—´è¾¹ç•Œ\n",
    "        input_size: è¾“å…¥å¤§å°\n",
    "        max_num: æœ€å¤§å—æ•°\n",
    "        num_segments: åˆ†æ®µæ•°é‡\n",
    "        get_frame_by_duration: æ˜¯å¦æ ¹æ®æ—¶é•¿è·å–å¸§æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        pixel_values: å¤„ç†åçš„è§†é¢‘å¸§å¼ é‡\n",
    "        num_patches_list: æ¯å¸§çš„å—æ•°åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if get_frame_by_duration:\n",
    "        duration = max_frame / fps\n",
    "        num_segments = get_num_frames_by_duration(duration)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e992d5-6e66-469a-8f21-062c00967c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video shows a person inspecting the damage to a white car's fender after a minor collision. The car's paint is scratched and dented, and the person is discussing the extent of the damage and potential repair options. They mention that the dent is not too deep and that the paint can be buffed out. The person also notes that the car's bumper is damaged and will need to be replaced. The scene takes place in an outdoor parking area with a green wall in the background.\n",
      "Two people appear in the video.\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°è®¾ç½®\n",
    "max_num_frames = 512\n",
    "generation_config = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.1,\n",
    "    num_beams=1\n",
    ")\n",
    "video_path = \"car.mp4\"\n",
    "num_segments=128\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # åŠ è½½è§†é¢‘å¹¶å¤„ç†\n",
    "  pixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\n",
    "  pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "  video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
    "  \n",
    "  # å•è½®å¯¹è¯ï¼šè§†é¢‘è¯¦ç»†æè¿°\n",
    "  question1 = \"Describe this video in detail.\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # å¤šè½®å¯¹è¯ï¼šè¯¢é—®è§†é¢‘ä¸­çš„äººæ•°\n",
    "  question2 = \"How many people appear in the video?\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d691952-7126-4dce-8ded-b61af1ab9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4528ec18-0b11-4bd9-9c86-446bf345a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½¦çš„é—¨å’Œè½®èƒéƒ½æŸä¼¤äº†ï¼Œé—¨å‡¹é™·äº†ï¼Œè½®èƒçš„æ¼†é¢è¢«åˆ®æ‰äº†ã€‚\n",
      "è½¦æ’åˆ°äº†å¢™è§’ã€‚\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  # å•è½®å¯¹è¯ï¼šè¯¢é—®è½¦è¾†æŸä¼¤éƒ¨ä½ï¼ˆä¸­æ–‡ï¼‰\n",
    "  question1 = \"è½¦çš„å“ªä¸ªéƒ¨ä½æŸä¼¤äº†ï¼Ÿ\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  \n",
    "  # å¤šè½®å¯¹è¯ï¼šè¯¢é—®è½¦è¾†ç¢°æ’ä½ç½®ï¼ˆä¸­æ–‡ï¼‰\n",
    "  question2 = \"è½¦æ’åˆ°å“ªé‡Œäº†ï¼Ÿ\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "  \n",
    "  print(output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
