[
    {
        "type": "text",
        "text": "INTERNVIDEO2.5: EMPOWERING VIDEO MLLMS WITH LONG AND RICH CONTEXT MODELING ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yi Wang∗1, Xinhao $\\mathbf { L i } ^ { * 2 , 1 }$ , Ziang $\\mathbf { Y a n ^ { * 1 } }$ , Yinan $\\mathbf { H e ^ { * 1 } }$ , Jiashuo $\\mathbf { Y } \\mathbf { u } ^ { * 1 }$ Xiangyu Zeng2,1, Chenting $\\mathbf { W a n g ^ { 1 } }$ , Changlian $\\mathbf { M } \\mathbf { a } ^ { 2 , 1 }$ , Haian Huang1 Jianfei Gao1, Min $\\mathbf { D o u } ^ { 1 }$ , Kai Chen1, Wenhai Wang1 Yu Qiao†1, Yali Wang†3,1, Limin Wang†2,1 1Shanghai AI Laboratory 2Nanjing University 3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/e16db609f585c24c8cb5e44702bf4581946f9ca8fe3cefde7c92536e7e535296.jpg",
        "img_caption": [
            "Figure 1: Demonstrations of InternVideo2.5. Left: open-source model (8B) performance on MVBench and VideoMME; right: an example of InternVideo2.5 about it monitors the target requested by users and analyzes it. "
        ],
        "img_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "ABSTRACT ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs’ ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM’s innate abilites (focus and memory), providing new insights for future research on video MLLM. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Multimodal large language models (MLLM) make milestones in reaching artificial general intelligence. They integrate different sensing signals into a unified LLM-based framework, reformulating most perception and cognition problems into the multimodal next token prediction task. This successfully addresses applications ranging from multimodal document analysis [OpenAI, 2024, Reid et al., 2024, Chen et al., 2024c], video understanding [Li et al., 2023, Wang et al., 2024f, Li et al., 2024e], agent interactions, to scientific discovery [Chen et al., 2023a], world modeling [Agarwal et al., 2025], autonomous driving [Hu et al., 2023], and live assistance [Mu et al., 2024, Driess et al., 2023]. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Despite its advances, MLLMs still underperform humans in fundamental vision related tasks, impeding its understanding and reasoning performance. They frequently exhibit difficulties in accurately recognizing, localizing, and recall objects, scenes, and motions in common scenarios - limitations that users find difficult to accept. Although research has demonstrated that scaling laws apply to vision-language modeling, showing consistent improvements in multimodal understanding benchmarks through increased visual-related data and model size, this trend doesn’t provide a clear timeline for MLLMs to achieve human-level visual understanding. Current research emphasizes intelligent document analysis and resolution adaptation for high-definition processing. While these approaches show promising performance improvements, they don’t conclusively demonstrate emerging capabilities in systematically solving these problems. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In this paper, we investigate how the length and fineness of the multimodal context influences MLLMs’ vision-centric abilities and performance, rather than focusing on directly scaling MLLMs through model size or data volume. Here length and fineness refers to the model’s ability to process and interpret multimodal inputs (e.g., video frames, audio, text) in long-term context with fine-grained details. Intuitively, this directly impacts the model’s understanding and reasoning. Longer context allows the model to capture extended temporal dependencies and coherence, such as story arcs in videos or multi-step events. This is crucial for understanding complex narratives or reasoning over them. Meanwhile, fine-grained context, such as object details, spatiotemporal relationships, enables the model to perceive subtle details. It improves understanding specific actions, interactions, or scenes, and short-term reasoning on inferring causality or predicting future actions. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "As higher context resolution and richness (both length and fineness) improves the model’s ability to perceive and encode multimodal inputs accurately, we explore enhancing MLLMs by explicitly modeling fine-grained and extended multimodal context in a scalable manner. For accurate spatiotemporal understanding, we transfer extensive dense visual annotations into MLLMs using direct preference optimization (DPO) [Rafailov et al., 2024], utilizing vision expert models as preference models [Yan et al., 2024]. To process extended multimodal context, we employ compact and compatible spatiotemporal representations by adaptively compressing multimodal tokens both visually and semantically [Li et al., 2024e]. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Generally, we show that improving multimodal context length and fineness is a workable and easily implemented solution with quick returns in both chat-based and fundamental vision perception performance, as given in Figure. 1. Considering whether the online interaction of MLLM more or less replies on its memory (how long its multimodal context can handle) and focus (how accurate its context can capture), we believe InternVideo2.5 builds capability foundations for these advanced functions and applications. Concretely, our contributions lie in: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• To our best knowledge, we give a first comprehensive study on how to realize long and rich context (LRC) for improving MLLM’s memory and focus. By unifying hierarchical token compression (HiCo) and task preference optimization (TPO) into a schema, we can enhance current MLLMs at scale with highly efficient learning and rich vision annotations. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• InternVideo2.5 can improve existing MLLMs with notable performance in video understanding as well as granting them with expert-level visual perception abilities. Specifically, InternVideo2.5 achieves the leading performance in several short and long video benchmarks. The video memory capacity of InternVideo2.5 has been significantly enhanced, allowing it to retain inputs at least six times longer than the original. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 Related Work ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Multimodal Large Language Models. Multimodal Large Language Models (MLLMs) usually combine vision encoder, LLM, and their connectors through instruction-tuning, capable of handling sophisticated tasks such as generating accurate picture descriptions and responding to visual queries. Progresses have been made to explore its model architecture [Zohar et al., 2024, Liu et al., 2025, Li et al., 2023, Wang et al., 2022, Ye et al., 2023, 2024], size [Chen et al., 2024c, Zhang et al., 2024c] and capability [Wang et al., 2024d], training corpus [Li et al., 2024d, Wang et al., 2023, GLM et al., 2024], preference optimization [Yan et al., 2024, Yu et al., 2024], and more. Contemporary video-capable MLLMs have demonstrated the ability to process sequential visuals, comprehending spatiotemporal variations. Works discuss different how different vision encoders [Zohar et al., 2024] and connectors [Liu et al., 2025] affect MLLMs’ performance, and find video encoders are still irreplaceable with image ones. Also, though MLPs related connectors are proven as effective as Q-Former, pooling, and other compression-preferred ones, the latter ones can deliver more efficient solutions, benefiting potential long visuals processing [Li et al., 2024e]. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Long Video Understanding. Long video understanding has seen rapid progress through multimodal large language models (MLLMs). It faces challenges in handling extended video content, leading researchers to pursue three primary approaches. The first method [Reid et al., 2024, Wei and Chen, 2024, Xue et al., 2024, Zhang et al., 2024a] focuses on expanding the context window capacity of MLLM to handle longer sequences. The second emphasizes efficient token compression to reduce compute [Li et al., 2024f, Fei et al., 2024b, Weng et al., 2025, Tan et al., 2024, Song et al., 2024, Shu et al., 2024]. The third leverages agents to manage compute complexity, minaly disentangling long video understanding into temporal grounding, spatiotemporal perception, and the reasoning over the observed evidence [Fan et al., 2025, Wang et al., 2025]. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Context window extension has demonstrated promising results in processing longer video sequences. For instance, advanced training systems like parallel computing [Contributors, 2023] have been developed to optimize compute distribution across temporal and tensor dimensions, improving computational scalability by using more devices. However, while these approaches [Reid et al., 2024, Wei and Chen, 2024, Xue et al., 2024, Zhang et al., 2024a] enable the processing of longer videos, they often encounter barriers due to communication costs and real video length. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Token compression creates compact video representations while attempting to preserve essentials. These methods [Li et al., 2024f, Fei et al., 2024b, Weng et al., 2025, Tan et al., 2024, Song et al., 2024, Shu et al., 2024] achieve high compression ratios, making them more computationally efficient. However, their performance in detailed video understanding tasks often falls short, sometimes performing below image-focused MLLMs, suggesting room for improvement in maintaining semantics during compression. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Agent-based methods decompose long video comprehension into several sub-tasks for existing expert models and LLMs. These methods [Fan et al., 2025, Wang et al., 2025, Fei et al., 2024a] are often integrated with self-reflection or chain-of-thought to enhance their answering performance and interpretability. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The assessment of long video understanding requires multiple aspects including object recognition, temporal reasoning, and memory retention. Recent benchmark developments have primarily centered on question-answering (QA) tasks. These [Rawal et al., 2024, Song et al., 2024, Zhang et al., 2023b, Huang et al., 2024b, Zhang et al., 2025, Chandrasegaran et al., 2024, Hong et al., 2023, Fang et al., 2024, Li et al., $2 0 2 4 \\mathrm { g } ]$ include evaluations of egocentric videos, online content, movies, TV shows, surveillance footage, and so on. Some benchmarks focus on specific aspects like temporal reasoning [Zhou et al., 2024] and plot comprehension [Wu et al., 2024a], while others emphasize fine-grained information retrieval from extended sequences [Zhao et al., 2024, Fu et al., 2024]. These diverse evaluation methods help assess both perceptual accuracy and reasoning capabilities in processing long-form content. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "MLLMs for Specific Vision Problems. While conventional MLLMs have demonstrated strong performance in tasks like visual captioning and question answering, they are challenged in handling fine-grained visual tasks such as segmentation and temporal grounding that require precise predictions. To address these limitations, researchers have developed two main approaches: 1) The pixel-to-sequence (P2S) methodology [Chen et al., 2023b, Ren et al., 2024, Wang et al., 2024a,g, Ye et al., 2023] enables MLLMs to generate textual predictions directly. These systems incorporate specialized components like time-aware encoders and sliding video processors to enhance temporal information understanding. 2) The pixel-to-embedding (P2E) approach [Bai et al., 2024, Lai et al., 2024, Wang et al., 2024d, Wu et al., 2024b, Zhang et al., 2023a] focuses on compressing visual information before passing it to specialized downstream decoders for final predictions. Recent implementations leverage advanced segmentation tools through prompt-based mechanisms, using special tokens to bridge MLLMs with segmentation modules. Some systems employ multiple routing tokens and enhanced query mechanisms to facilitate connections between MLLMs and various decoder components. These developments represent significant progress in creating more capable and versatile visual understanding systems. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3 InternVideo2.5: Long and Rich Context Modeling ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "To enable long and accurate video understanding with MLLM, we build InternVideo2.5 to enhance the MLLM context length and fineness, employing a video length adaptive token representation and task preference optimization, as given in Figure 2. The whole model is learnt with three stages leveraging both short and long video data, as well as classical vision task data. The whole method is detailed below. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.1 Video Length Adaptive Token Representation For Long Multimodal Context ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Our approach introduces a practical length-adaptive token representation approach for processing video sequences of any lengths efficiently. This method is built upon a typical MLLM architecture comprising three key components: a vision encoder, a vision-language connector, and a language model. After a dynamic frame sampling, the given pipeline implements a hierarchical token compression (HiCo) with two distinctive stages: 1) spatiotemporal-aware compression during the visual encoding, and 2) adaptive multimodal context consolidation during language model processing. ",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/98aa150c903719740a1aaab883522a4fdc356116fb6fbdea964009707a1a9630.jpg",
        "img_caption": [
            "Figure 2: Framework of InternVideo2.5 with the long and rich context (LRC) modeling. "
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Adaptive Temporal Sampling. We implement a context-aware sampling mechanism that adjusts according to video duration and content characteristics. For shorter sequences where motion granularity is crucial, we employ dense temporal sampling (15 frames per second). Conversely, for long sequences (e.g. minute / hour-level videos) focused on event-level understanding, we utilize sparse sampling (1 frame per second). This adaptive approach ensures proper motion capture across varying temporal scales. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Hierarchical Token Compression. We compress long visual signals via their spatiotemporal redundancies in events and semantic redundancies between events. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "• Spatiotemporal Token Merging. To address the inherent temporal redundancy in video sequences, we process the input through a hierarchical compression scheme. Given a video sequence divided into $T$ temporal segments, each segment is processed by a vision encoder $\\mathcal { E }$ to generate $M$ initial tokens: $[ \\mathbf { v } _ { i } ^ { j } ] _ { i = 1 , 2 , . . , M }$ for the $j$ -th segment. These tokens undergo adaptive compression through a token connector $\\mathcal { C }$ , producing $N$ compressed tokens $[ \\mathbf { s } _ { i } ^ { j } ] _ { i = 1 , 2 , . . , N }$ where $N < M$ : ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n[ \\mathbf { s } _ { i } ^ { j } ] _ { i = 1 , 2 , . . , N } = \\mathcal { C } ( [ \\mathbf { v } _ { i } ^ { j } ] _ { i = 1 , 2 , . . , M } ) .\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "To keep the essences of spatiotemporal tokens, token merging [Bolya et al., 2022] can be treated as a semantic pooling operation, pooling tokens with high similarities instead of close locations. Such relation calculation is realized by a bipartite soft matching with a few of iterations. Our empirical analysis (in Sec. X) across various compression configurations (pooling, MLP, Q-Former, and more) reveal that exploiting semantic similarity-based token merging [Bolya et al., 2022] (ToMe) across spatiotemporal scales as $\\mathcal { C }$ shows the superiority in visual compression while preserving essential details. Meanwhile, we argue query-conditioned visual token compression by Q-Former are no longer performant competitive and user-friendly in the context of up-to-date LLMs. A Q-Former is usually with 300M learanble parameters demanding several epochs of learning on at least 10M visual-text pairs. Such learning is independent of the pretraining or supervised finetuning (SFT) of vision or language models, while ToMe is plugable in training or testing without extra tuning just like pooling. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "• Multimodal Token Dropout. We introduce the token dropout that operates during language model processing to further optimize long-range visual understanding. It implements a two-phase token reduction strategy: (1) uniform token pruning in early layers to maintain structural integrity while reducing computational overhead, and (2) attention-guided token selection in deeper layers to retain task-relevant essences. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "For $N T$ compressed tokens $[ \\mathbf { s } _ { i } ^ { j } ]$ from the video sequence, we apply this pruning during language model computation. Denoting $[ \\mathbf { t } _ { i } ^ { ( l ) } ]$ as the token representations at layer $l$ , the pruning operation is defined as: ",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n[ \\mathbf { t } _ { i } ^ { ( l ) } ] = [ \\mathbf { p } _ { i } ^ { ( l ) } \\odot \\mathbf { t } _ { i } ^ { ( l - 1 ) } ] \\quad \\mathrm { w h e r e } \\quad \\mathbf { p } _ { i } ^ { ( l ) } \\sim \\mathrm { B e r n o u l l i } ( p ) ,\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "where $p$ denotes the token preservation probability. This adaptive pruning mechanism not only enhances computational efficiency but also improves model performance by reducing irrelevant visual information in the token representation. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Note all three steps in the given video length adaptive token representation are non-learnable. Yet, training MLLMs with HiCo would yield much better performance as HiCo benefits from training in more context-adaptive features. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.2 Enhancing Visual Precision in Multimodal Context through Task Preference Optimization ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We enhance multimodal language models’ (MLLMs) precise visual understanding capabilities through Multi-task Preference Learning (MPL). Our approach integrates specialized visual perception modules with a base MLLM architecture to enable fine-grained visual analysis capabilities such as precise localization and temporal understanding. The framework consists of a core MLLM $( \\Phi )$ containing a visual encoder $( \\mathcal { E } )$ , cross-modal connector $( \\mathcal { C } )$ , and language model $( \\mathcal { L } )$ , augmented with a specialized visual perception module $( \\mathcal { P } )$ comprising task-specific heads $H _ { k }$ (where $k = 1 , 2 , . . . , K )$ . These heads interface with the MLLM through learned task embeddings $\\mathbf { e } _ { k }$ derived from trainable task tokens $\\mathbf { v } _ { k }$ . ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Visual Perception Preference. The visual perception module incorporates two fundamental capabilities essential for precise visual understanding: ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Temporal Understanding. For processing dynamic visual content, we develop a temporal component that combines video feature extraction with temporal alignment capabilities. The module takes in both visual sequences and textual queries, incorporating task-specific temporal embeddings to predict precise temporal boundaries and relevance scores. • Instance Segmentation. To enable pixel-precise understanding and instance-level differentiation, we design a segmentation module building upon recent advances in foundation models for segmentation. The module consists of an image encoder, mask decoder, and an adaptive projection layer that bridges MLLM embeddings with pixel-level predictions. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To effectively integrate these specialized capabilities while preserving the MLLM’s general capabilities, we optimize the MLLM $\\Phi$ along with visual perception module $\\mathcal { P }$ as: ",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\mathcal { L } _ { \\mathrm { t o t a l } } = \\mathcal { L } _ { \\mathrm { b a s e } } + \\lambda _ { 1 } \\mathcal { L } _ { \\mathrm { t a s k } } ( G ( \\mathbf { Q } ) , \\mathbf { y } ) + \\lambda _ { 2 } \\sum _ { k = 1 } ^ { K } \\mathcal { L } _ { \\mathrm { s p e c } } ( \\mathbf { Y } _ { k } , H _ { k } ( G ( \\mathbf { v } _ { k } ) ) ) ,\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "where $\\mathbf { Q }$ represents input queries, y denotes task labels, and $\\mathbf { Y } _ { k }$ represents task-specific annotations. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Our framework improves the MLLM’s general capabilities by significantly enhancing its precision in specific visual analysis tasks. The modular design allows for flexible extension to additional visual understanding capabilities while maintaining efficient computation and training dynamics. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.3 Training Video Corpus for Multimodal Context Modeling ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The training process has three stages, in which visual-text aligned data, long video data and task-specific visual data are used. The training data can be seen in Table 1. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Visual-Text Data For Crossmodal Alignment. We curate a collection of visual-text data with 7M image-text pairs and 3.7M video-text pairs, as well as 143K text data for language capability enhancement. For visual-caption pairs, we convert them into a question-answering (QA) format for training convenience. Specifically, ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Image-Text. We utilize 3.5 million detailed image descriptions recaptioned with LLava-NeXT-34B [Zhang et al., 2024c] from COCO118K, BLIP558K, and CC3M datasets [Li et al., 2024a]. For instruction data, we use singleimage instructions from LLava-NeXT [Zhang et al., 2024c], Allava [Chen et al., 2024a], and ShareGPT4O [Chen et al., 2024c, Wang et al., 2024f], along with multi-image instructions from LLaVA-Interleave [Li et al., 2024b]. Additionally, we incorporate 558K image-text pairs from LCS-558K [Liu et al., 2024]. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Video-Text. Our video-text data consists of WebVid2M [Bain et al., 2021] descriptions recaptioned with VideoChat2 [Li et al., 2024c], and 323K detailed descriptions from WebVid [Bain et al., 2021] and Kinetics [Kay et al., 2017] recaptioned with Gemini [Reid et al., 2024][Share, 2024]. For instruction fine-tuning, we use short video data from VideoChat2[Li et al., 2024c] and InternVideo2 [Wang et al., 2024f], complemented by GPT4oannotated data from ShareGPT4o [Chen et al., 2024c, Wang et al., 2024f], VideoChatGPT-Plus [Maaz et al., 2024], LLaVA-Video-178K [Zhang et al., 2024d], and LLava-Hound [Zhang et al., 2024b]. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "• Text. We incorporate 143K samples from the Evo-Instruct dataset [Chen et al., 2024a]. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Long Video Corpus for Context Extension. We primarily utilized long video instruction data from MoiveChat [Song et al., 2024], Cineplie [Rawal et al., 2024], Vript [Yang et al., 2024] and our LongVid. A significant challenge in training long video models is the scarcity of large-scale, high-quality data. Although recent progress has alleviated this issue to some extent through long-form datasets of video-text pairs, these datasets lack the instruction-following paradigm, such as (video, instruction, answer) triplets, which is essential for multimodal reasoning. To tackle this problem, we employ a large-scale long video instruction-tuning dataset named LongVid from [Li et al., 2024e]. This dataset consists of 114,228 long videos and 3,444,849 question-answering (QA) pairs across five distinct task types, enabling models to handle various long video scenarios. ",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/19e5b6a2a10753e536c1c1353ebd063d1b0112a6dc704374c8a26fcd210d364c.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 1: The training data specifications encompass more than the standard public video question-answering datasets In addition to common data, we incorporate annotations from lengthy videos and typical vision task data. "
        ],
        "table_body": "\n\n<html><body><table><tr><td>Stage</td><td>Task</td><td>Samples</td><td>Datasets</td></tr><tr><td rowspan=\"4\">Stage 1</td><td>Segmentation</td><td>50K</td><td>SAMv2,MeViS</td></tr><tr><td>Toempoa ronding</td><td>50K</td><td>RefCOCD, eMoCug, RefCOCO+</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Alignment data</td><td>1M</td><td>LCS-558K,S-MiT</td></tr><tr><td rowspan=\"4\">Stage 2</td><td>Segmentation</td><td>114.6K</td><td>SAMv2,MeViS</td></tr><tr><td>Tempoa Gronding</td><td>146.5K </td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Visual Concept</td><td>6M</td><td>VideoChat2-IT, WebVid2M, Share-Gemini, LLaVA-NexT, Evo-Instruct</td></tr><tr><td rowspan=\"4\">Stage 3</td><td>Temporal Grounding Segmentation Temporal Reasoning</td><td>7.5K 116.5K</td><td>QVHighlight MeViS, SAMv2</td></tr><tr><td rowspan=\"3\">Spatial Grounding Conversation</td><td>40K</td><td>YouCook2, ActivityNet</td></tr><tr><td>400K</td><td>AS-V2, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg</td></tr><tr><td>3.5M</td><td>LLaVA-NexT, Evo-Instruct,ShareGPT4o,LLaVA-Interleave, VideoChat2-IT, VideoChatGPT-Plus, LLaVA-Video, LLaVA-Hound, MovieChat, Vript, LongVid</td></tr></table></body></html>\n\n",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Task-Specific Data for Accurate Perception. ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "• Segmentation. We use MeViS [Ding et al., 2023] and SAMv2 [Ravi et al., 2024] for referring segmentation task. • Spatial Grounding. We use AS-V2 [Wang et al., 2024c], Visual Genome [Krishna et al., 2017], RefCOCO [Yu et al., 2016], RefCOCOg [Yu et al., 2016], RefCOCO $+$ [Yu et al., 2016] for one epoch with a total batch size of 128 to train region head and token. • Temporal Grounding. We utilize DiDeMo [Hendricks et al., 2017], QuerYD [Oncescu et al., 2021], HiRest [Zala et al., 2023], ActivityNet [Caba Heilbron et al., 2015], TACoS [Regneri et al., 2013], NLQ [Grauman et al., 2022]. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.4 Progressive Multi-stage Training ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "We propose a unified progressive training scheme that jointly enhances MLLM’s fine-grained perception and temporal understanding abilities. Our approach consists of three main stages that gradually increase both the complexity of tasks and the temporal length of video inputs. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Stage 1: Foundation Learning. This initial stage focuses on two parallel objectives: (a) task recognition instruction tuning for LLM using diverse dialogue templates, enabling the model to identify and route different visual tasks; and (b) video-language alignment training where we freeze the visual encoder and LLM while optimizing the compressor and MLP to establish basic visual-linguistic connections. We utilize $0 . 5 \\mathbf { M }$ image-text pairs and $0 . 5 \\mathbf { M }$ short video-text pairs with 4 frames per video in this stage. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Stage 2: Fine-grained Perception Training. This stage enhances the model’s visual understanding capabilities through: (a) integration and training of task-specific components including task tokens, region head, temporal head, and mask adapter using task-specific datasets; and (b) visual concept pre-training using $3 . 5 \\mathrm { M }$ images and 2.5M short video-text pairs with 8 frames per video. The LLM is updated using LoRA during this stage to maintain its general capabilities while acquiring new visual skills. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Stage 3: Integrated Accurate and Long-form Context Training. The final stage jointly optimizes all model components through: (a) multi-task training on mixed corpus combining multimodal conversations and specific task data, allowing task-supervised gradients to flow from specialized heads to MLLM; and (b) instruction tuning on a comprehensive dataset of $3 . 5 \\mathrm { M }$ samples, including 1.1M images, 1.7M short videos $( < 6 0 \\mathrm { s } )$ , and $0 . 7 \\mathbf { M }$ long videos (60-3600s). We employ dynamic video sampling with 64-512 frames and tune the entire model including the vision encoder, connector, task tokens, specialized heads, and LLM with LoRA. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "This progressive training strategy enables the model to develop both fine-grained perception and long-form video understanding while mitigating potential degradation of general abilities. Unlike previous approaches that rely on long-form text for extending context windows, our direct training on long-form videos minimizes the gap between training and deployment scenarios. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.5 Implementation ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Distributed System. We develop a multimodal sequence parallelism system based on XTuner to train and test on millions of multimodal tokens (mostly in visual). Built upon several open-sourced solutions [Xue et al., 2024, Fang and Zhao, 2024, Jacobs et al., 2023, Rasley et al., 2020, Liu et al., 2023], we enable scalable computing of long videos by integrating both sequence and tensor distributed processing as well as multimodal dynamic (soft) data packing. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "To efficiently distribute multimodal token sequence, we divide it in head processing in multi-head self-attention (MHSA) and input sequence length. Specifically, we employ All-to-All (A2A) communication for tensor parallel computing from DeepSpeed-Ulysses [Jacobs et al., 2023]. As it is limited to head number in attention, we further incorporate ring-attention [Liu et al., 2023] to realize concurrent computing on the sequence for rising parallelism degree in Peer-to-Peer (P2P) communication. Regarding high transfer speed in inter-node and low speed in intra-node settings, we utilize the 2D-attention strategy [Fang and Zhao, 2024] that assigns A2A to inter-node computing and P2P to intra-node, as any device is required to communicate with any others for A2A while any device is with the two neighbors in the ring structure for P2P. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In regard to the variance in training video length, we need unify the data size for batch training by packing them (or sharding them in some studies). A simple padding strategy widely adopted in mainstream MLLM learning is padding the short sequence and clip the long one to the target length in the same batch. Despite its simplicity, its data utilization effectency has room to improve as it fills too many placeholders in inputs. In contrast, we employ a dynamic packing strategy. Given a fixed sequence length in each training iteration, it dynamically merge input sequences by order into a new one (ensuring its total length do not exceed the target). Note it can maximize the GPU memory usage to achieve speedup ratio, especially when the distribution of training video lengths is beyond even. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Model Configuration. In our multimodal architecture, we utilize a comprehensive framework that combines advanced video processing and language modeling capabilities. The system implements dynamic video sampling, processing 64- 512 frames, with each 8-frame clip compressed to 128 tokens, yielding approximately 16 tokens per frame representation. The architecture integrates InternViT [Chen et al., 2024b] for visual encoding, along with an MLP-based token merging mechanism and InternLM2.5-7B [Cai et al., 2024] as the language model. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The framework incorporates multiple specialized components: a Temporal Head based on CG-DETR architecture [Moon et al., 2023], and a Mask Head utilizing SAM2’s pre-trained weights. For temporal processing, the system leverages InternVideo2 [Wang et al., 2024f] for video feature extraction, while query features are processed through the language model. To enhance spatiotemporal capabilities, we implement two-layer MLPs for both positioning prompts and spatial input encoding into the multimodal language model. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4 Experiments ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "We evaluate InternVideo2.5-7B along with its base model and other comparisons on mainstream multimodal video understanding and classical vision tasks. With the incorporation of HiCo and TPO to MLLMs, InternVideo2.5 can address short and long video dialogue (in Table 2) and typical vision tasks like tracking (in Table 3). ",
        "page_idx": 6
    },
    {
        "type": "table",
        "img_path": "images/64f25ce078b277c9f0339a544906e11f1c006a619cbfe4a6b6e979f60b484f84.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 2: Performance on video question-answering benchmarks, covering both short and long videos. The models and their corresponding results, displayed in gray font, are based on LLMs of over 7 billion parameters in size. "
        ],
        "table_body": "\n\n<html><body><table><tr><td>Model Average duration (sec)</td><td>Size</td><td>#Tokens</td><td>MVBench 16</td><td>PerceptionTest 23</td><td>EgoSchema 180</td><td>LongVideoBench 473</td><td>MLVU 651</td><td>VideoMME 1010</td><td>LVBench 4101</td></tr><tr><td colspan=\"10\"></td></tr><tr><td>ProprietaryModels</td><td></td><td></td><td></td><td></td><td></td><td>59.1</td><td>49.2</td><td>59.9</td><td></td></tr><tr><td>GPT4-V [OpenAI, 2023] GPT4-o [OpenAI, 2024]</td><td></td><td>－－-</td><td>43.7 64.6</td><td></td><td>72.2</td><td>66.7</td><td>64.6</td><td>71.9</td><td>30.8</td></tr><tr><td>Gemini-1.5-Pro [Reid et al., 2024]</td><td></td><td></td><td>60.5</td><td></td><td>71.2</td><td>64.0</td><td></td><td>75.0</td><td>33.1</td></tr><tr><td colspan=\"10\">Open-SourceMLLMs</td></tr><tr><td>InternVL2 [Chen et al., 2024c]</td><td>8B</td><td>256</td><td>66.4</td><td></td><td></td><td></td><td></td><td>54.0</td><td></td></tr><tr><td>Intern VL2 [Chen et al., 2024c]</td><td>76B</td><td>256</td><td>69.6</td><td></td><td></td><td>-</td><td></td><td>61.2</td><td></td></tr><tr><td>LLaVA-NeXT-Video [Zhang et al., 2024c]</td><td>7B</td><td>144</td><td>53.1</td><td>48.8</td><td></td><td>49.1</td><td></td><td>46.5</td><td></td></tr><tr><td>LLaVA-One Vision [Li et al., 2024a]</td><td>7B</td><td>196</td><td>56.7</td><td>57.1</td><td>60.1</td><td>56.3</td><td>64.7</td><td>58.2</td><td></td></tr><tr><td>LLaVA-OneVision [Li et al., 2024a]</td><td>72B</td><td>196</td><td>59.4</td><td>66.9</td><td></td><td>61.3</td><td>68.0</td><td>66.2</td><td>26.9</td></tr><tr><td>VideoLLaMA2 [Cheng et al., 2024]</td><td>7B</td><td>72</td><td>54.6</td><td>51.4</td><td>51.7</td><td></td><td>48.5</td><td>47.9</td><td></td></tr><tr><td>VideoLLaMA2 [Cheng et al., 2024]</td><td>72B</td><td>72</td><td>62.0</td><td>57.5</td><td>63.9</td><td></td><td></td><td>62.4</td><td></td></tr><tr><td>mPLUG-Owl3 [Ye et al., 2024]</td><td>7B</td><td></td><td>54.5</td><td>-</td><td></td><td>52.1</td><td></td><td>53.5</td><td>43.5</td></tr><tr><td>QwenVL2 [Bai et al., 2023]</td><td>7B</td><td></td><td>67.0</td><td>62.3</td><td>66.7</td><td></td><td></td><td>63.3</td><td></td></tr><tr><td>QwenVL2 [Bai et al., 2023]</td><td>72B</td><td></td><td>73.6</td><td>68.0</td><td>77.9</td><td></td><td></td><td>71.2</td><td>41.3</td></tr><tr><td>VideoChat2-HD [Li et al., 2024c]</td><td>7B</td><td>72</td><td>62.3</td><td></td><td></td><td></td><td>47.9</td><td>45.3</td><td></td></tr><tr><td>Intern Vide02-HD [Wang et al., 2024f]</td><td>7B</td><td>72</td><td>67.2</td><td>63.4</td><td>60.0</td><td></td><td></td><td>49.4</td><td></td></tr><tr><td>VideoChat-TPO [Yan et al., 2024]</td><td>7B</td><td>64</td><td>66.8</td><td></td><td></td><td></td><td>54.7</td><td></td><td></td></tr><tr><td>Intern VL2.5 [Chen et al., 2024b]</td><td>7B</td><td>256</td><td>72.0</td><td>68.2</td><td> 51.5</td><td>60.0</td><td>68.9</td><td>64.2</td><td>38.4</td></tr><tr><td colspan=\"10\">Open-Source LongVideo MLLMs</td></tr><tr><td>LLaMA-VID [Li et al., 2024f]</td><td>7B</td><td>2</td><td>41.9</td><td>44.6</td><td></td><td></td><td>33.2</td><td>25.9</td><td>23.9</td></tr><tr><td>LongVILA [Xue et al., 2024]</td><td>7B</td><td>196</td><td></td><td></td><td>67.7</td><td></td><td>-</td><td>57.5</td><td></td></tr><tr><td>Long VA [Zhang et al., 2024a]</td><td>7B</td><td>144</td><td></td><td></td><td></td><td></td><td>56.3</td><td>52.6</td><td></td></tr><tr><td>LongLLaVA [Wang et al., 2024e]</td><td>9B</td><td>144</td><td>49.1</td><td></td><td></td><td></td><td></td><td>43.7</td><td></td></tr><tr><td>LongVU [Shen et al., 2024]</td><td>7B</td><td>64</td><td>66.9</td><td>-</td><td>67.6</td><td></td><td>65.4</td><td></td><td></td></tr><tr><td>VideoChat-Flash [Li et al, 2024e]</td><td>7B</td><td>16</td><td>73.2</td><td>75.6</td><td></td><td>64.2</td><td>74.5</td><td>64.0</td><td>47.2</td></tr><tr><td>Intern Video2.5 (InternVL2.5+LRC)</td><td>7B</td><td>16</td><td>75.7(+3.7)</td><td>74.9(+6.7)</td><td>63.9(+12.4)</td><td>60.6(+0.6)</td><td>72.8(+3.9)</td><td>65.1(+0.9)</td><td> 46.4(+8.0)</td></tr></table></body></html>\n\n",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/213cbe39ace4d83646248eec8121904514e6b3fe69e732af4220e04748739ece.jpg",
        "img_caption": [
            "Figure 3: Single-hop Needle-in-a-haystack evaluation results using 5,000 frames using 16 A100 (80G) GPUs. "
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.1 Video Understanding ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "As given in Table 2, InternVideo2.5 achieves almost the leading performance in all popular short and long video question-answering benchmarks in around 7B LLM capacity level. Compared with the employed base MLLM InternVL2.5, InternVideo2.5 gives a overall increase no matter on short or long duration predictions. The rises are notable when dealing with short videos, InternVideo2.5 increases more than 3 points on MVBench and Perception Test based on InternVL2.5. Concerning long video understanding, the whole trend is still rising but the variations are changing on different benchmarks. The brought increases are clear on EgoSchema (test) $( + 1 2 . 4 )$ [Mangalam et al., 2023], MLVU $( + 3 . 9 )$ [Zhou et al., 2024], and LVBench $\\left( + 8 . 0 \\right)$ [Wang et al., 2024b] while they are relatively marginal on LongVideoBench $\\left( + 0 . 6 \\right)$ [Wu et al., 2024a] and VideoMME (wo sub) $( + 0 . 9 )$ [Fu et al., 2024]. We attribute this to the questions in the latter two benchmarks are more dependent of world knowledge and reasoning rather than perceived evidences, since the performance increases over them are more apparent with the larger language models. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In the comparison with renown proprietary models e.g. GPT4-o and Gemini-1.5-Pro, InternVideo2.5 shows superiority over short duration spatiotemporal understanding while gives inferior results on long videos (except on MLVU). This may imply that we still have much room to explore for the vision-language fusion and long context modeling though we do make a huge step in vision areas from the academic and open-sourced perspective. ",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/85f4400481d05eb2773e6a82653b0c1d01cdf0879c53ad94e397224e9c83a532.jpg",
        "img_caption": [
            "Figure 4: An example of InternVideo2.5: it can depict spatiotemporal movements with precise temporal references "
        ],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/bf747d0c236508e2245441f1d2ed967d642f83099583a6cacc358496708f2a81.jpg",
        "img_caption": [
            "Figure 5: An example of InternVideo2.5: it can track the target specified by users and reason over it. "
        ],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Needle-In-The-Haystack (NIAH). With HiCo and long video learning corpus, InternVideo2.5 notably enhances InternVL2.5’s implicit memory (8B model). Using a single-hop needle-in-a-haystack (NiAH) task with 5,000 frames, InternVideo2.5 demonstrates superior recall. All models were evaluated on 16 A100 (80GB) GPUs. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "As shown in Figure 3, InternVL2.5-8B struggles to recall target frames accurately even within 500 frames, falling short of the $9 5 \\%$ accuracy threshold for memorization established in [Zhang et al., 2024a, Xue et al., 2024]. Furthermore, processing long video inputs leads to its out-of-memory (OOM) errors beyond 1,000 frames. In contrast, InternVideo2.5, benefiting from HiCo and long video training, accurately recalls frames from up to 3,000 frame sequences and processes over 10,000 frames without OOM issues. While the LRC variant $\\mathrm { ( H i C o + T P O ) }$ maintains high recall up to 3,000 frames, its performance degrades beyond this point, likely due to the discrepancy between its training data (primarily short videos) and the long video evaluation setting. Future work could explore adjusting the data ratio and incorporating long-form video annotations for TPO to mitigate this issue. Additionally, the context modeling of the LLM directly impact its MLLM’s context. Therefore, enhancing the LLM’s context or utilizing a more effective one could further improve performance. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Qualitative Evaluations. We give several real cases evaluated by InternVideo2.5 and VideoChat2 [Li et al., 2024c] in Figure 4, 5, and 6. Note InternVideo2.5 can give detailed motion descriptions as well as their specific times, track (or segment) the target for further reasoning, and understand over a long visual input. ",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/bbbee7f11f0ba9a5a60507d969ebf55b93e8462f95a75645839199c048f42b89.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Figure 6: Examples of InternVideo2.5: it can can comprehend extended surveillance videos for moment retrieval or abnormal event detection. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.2 Specific Vision Tasks ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "After the optimization of TPO, MLLMs can not only improve the understanding ability of video conversations, but also have the ability to handle specific densely annotated visual tasks, such as moment retrieval, reference tracking, and so on. With TPO, MLLMs master classical vision capabilities e.g. tracking. Table 3 presents that InternVideo2.5 can conduct track, video referring segmentation, temporal grounding, and other tasks with expert model-level performance, clearly outperforming other MLLMs in most tasks. It also validates that the joint learning between mutimodal question-answering and typical vision tasks could benefit each other as in [Yan et al., 2024]. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.3 Ablation Studies ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "How token representation length affects Model Performance. Table 4 gives how token number for each frame in HiCo affects InternVL2.5 on video benchmarks. Note the decreases from fewer token number in HiCo on short video benchmarks are relatively marginal (around 0.5 points) while these are non-trivial (around 1-3 points) on long ones. This empirically verifies that there is still significant performance headroom to explore in long multimodal context. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The compatibility between HiCo and TPO. We further show HiCo and TPO are compatible and orthogonal about improving MLLMs in Table 5. Combing the training of HiCo and TPO together, it not only simplifies the overall learning with three stages, but also notably rises short and long video benchmark results as well as enabling NIAH of MLLMs. Compared with InternVL2.5-HiCo, InternVideo2.5 gets non-trivial increases by 1.7, 3.5, 1.0, 1.0, and 1.3 points on MVBench, Percpetion Test, EgoSchema, LongVideoBench, and MLVU, respectively. The effectivenss on VideoMME is barely observed as the increase is only 0.2. It shows that TPO can improve both short and long video perception via classical vision supervision, but this benefits would vanish when the input videos are quite long (around and more than 15 minutes). ",
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/ad3f17d364db9c7e5df61c7cd764e15f6a1740994a6f57f63eee214fd80e9d8e.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 3: Performance on Specific Visual Tasks. The grey means is an expert model without LLM, and the tasks they can handle are limited to those that are fine-tuned. "
        ],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Charades-STA</td><td colspan=\"2\">Highlight Detection HIT@1</td><td rowspan=\"2\">Ref-YouTube-VOS J&F</td><td rowspan=\"2\">MeViS</td><td colspan=\"2\">LaSOT</td><td colspan=\"2\">GOT-10k</td></tr><tr><td>R@0.5</td><td>mloU</td><td>mAP</td><td></td><td>J&F</td><td>Success</td><td>Overlap</td><td>SR0.5</td></tr><tr><td>UniVTG [Lin et al., 2023]</td><td>25.2</td><td>27.1</td><td>40.5</td><td>66.3</td><td>-</td><td>-</td><td></td><td>Pnorm</td><td></td><td>-</td></tr><tr><td>ReferFormer [Wu et al., 2022]</td><td></td><td></td><td></td><td></td><td>62.9</td><td>31.0</td><td></td><td></td><td></td><td></td></tr><tr><td>OnlineRefer [Wu et al.,2023]</td><td></td><td></td><td></td><td>-</td><td>62.9</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>TFVTG(GPT-4T) [</td><td>49.9</td><td>44.5</td><td></td><td></td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td></tr><tr><td>VideoChat2 [Li et al., 2024c] </td><td>14.3</td><td>24.6</td><td></td><td></td><td>-</td><td>-</td><td></td><td>-</td><td></td><td>-</td></tr><tr><td>VTimeLLM [Huang et al., 2024a]</td><td>27.5</td><td>31.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>TimeChat [Ren et al., 2024]</td><td>32.2</td><td>-</td><td>21.7</td><td>37.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>-</td></tr><tr><td>HawkEye [Wang et al., 2024g]</td><td>31.4</td><td>33.7</td><td></td><td></td><td>-</td><td></td><td>-</td><td></td><td></td><td></td></tr><tr><td>ChatVTG [Qu et al., 2024]</td><td>33.0</td><td>34.9</td><td></td><td>-</td><td>-</td><td>-</td><td></td><td></td><td>-</td><td>-</td></tr><tr><td>LISA [Lai et al., 2024]</td><td>-</td><td>-</td><td></td><td></td><td>52.6</td><td></td><td></td><td></td><td></td><td>-</td></tr><tr><td>VideoLISA [Bai et al., 2024]</td><td></td><td></td><td></td><td></td><td>63.7</td><td>44.4</td><td>-</td><td></td><td></td><td>-</td></tr><tr><td>SiamFC [Bertinetto et al., 2016]</td><td>-</td><td></td><td></td><td></td><td></td><td></td><td>33.6</td><td>42.0</td><td>34.8</td><td>35.3</td></tr><tr><td>ATOM [Danelljan et al., 2019]</td><td>-</td><td></td><td></td><td></td><td>-</td><td>-</td><td>51.5</td><td>-</td><td>55.6</td><td>63.4</td></tr><tr><td>SiamRPN++ [Li et al., 2019]</td><td></td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td>49.6</td><td>56.9</td><td>51.8</td><td>61.8</td></tr><tr><td>SiamFC++ [Xu et al., 2020]</td><td>-</td><td>-</td><td></td><td></td><td>-</td><td>-</td><td>54.4</td><td>62.3</td><td>59.5</td><td>69.5</td></tr><tr><td>LLaVA-1.5 [Liu et al., 2024]</td><td>-</td><td></td><td></td><td>-</td><td>-</td><td>-</td><td>19.4</td><td>16.5</td><td>23.5</td><td>20.2</td></tr><tr><td>Merlin [Yu et al., 2025]</td><td></td><td>-</td><td></td><td></td><td></td><td>-</td><td>39.8</td><td>40.2</td><td>51.4</td><td>55.9</td></tr><tr><td>VideoChat-TPO</td><td>40.2</td><td>38.1</td><td>38.8</td><td>66.2</td><td>63.9</td><td>47.0</td><td>69.4</td><td>80.1</td><td>70.6</td><td>79.8</td></tr><tr><td>Intern Video2.5</td><td>43.3</td><td>41.7</td><td>34.7</td><td>60.3</td><td>34.2</td><td>32.0</td><td>71.5</td><td>82.1</td><td>72.4</td><td>83.0</td></tr></table></body></html>\n\n",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/cea8cdb3d9d4c968eedc78e076f161d5b068f1d227d10a33818dc83739bde423.jpg",
        "table_caption": [
            "Table 4: Performance of InternVL2.5 using HiCo with varying numbers of tokens per frame. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Base Model</td><td>#Tokens</td><td>MVBench</td><td>PerceptionTest</td><td>EgoSchema</td><td>LongVideoBench</td><td>MLVU</td><td>VideoMME</td></tr><tr><td>InternVL2.5-HiCo</td><td>64</td><td>74.4</td><td>71.9</td><td>65.7</td><td>62.7</td><td>72.6</td><td>66.4</td></tr><tr><td>InternVL2.5-HiCo</td><td>16</td><td>74.0</td><td>71.4</td><td>62.9</td><td>59.6</td><td>71.5</td><td>64.9</td></tr></table></body></html>\n\n",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/6717fbec86247c1b40c21da7c12fa1203555272f894abc94d2acbe9fa5c3c86e.jpg",
        "table_caption": [
            "Table 5: Evaluating HiCo and TPO compatibility on InternVL2.5. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Base Model</td><td rowspan=\"2\">LRC HiCo</td><td rowspan=\"2\">TPO</td><td rowspan=\"2\">MVBench</td><td rowspan=\"2\">PerceptionTest</td><td rowspan=\"2\">EgoSchema</td><td rowspan=\"2\">LongVideoBench</td><td rowspan=\"2\">MLVU</td><td rowspan=\"2\">VideoMME</td></tr><tr><td></td></tr><tr><td>Intern VL2.5</td><td></td><td></td><td>72.0</td><td>68.2</td><td>51.5</td><td>60.0</td><td>68.9</td><td>64.2</td></tr><tr><td>InternVL2.5</td><td></td><td></td><td>74.0</td><td>71.4</td><td>62.9</td><td>59.6</td><td>71.5</td><td>64.9</td></tr><tr><td>InternVL2.5</td><td></td><td></td><td>75.7</td><td>74.9</td><td>63.9</td><td>60.6</td><td>72.8</td><td>65.1</td></tr></table></body></html>\n\n",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "5 Concluding Remarks ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "This paper has introduced InternVideo2.5, a new version of video MLLM to improve the original perception and understaning ability through long and rich context (LRC) modeling. By focusing on improving context resolution—both in terms of length (memory) and fineness (focus)—InternVideo2.5 enables current MLLMs to understand longer videos with detail emphasis. Our approach leverages direct preference optimization to transfer dense visual annotations to MLLMs, and employs adaptive hierarchical token compression for efficient spatiotemporal representation. Experimental results demonstrate that InternVideo2.5 achieves the state-of-the-art performance on various video understanding benchmarks in around 7B model size, significantly increasing input video sequences by six times in length compared to the applied MLLMs. Furthermore, InternVideo2.5 exhibits enhanced visual capabilities, including object tracking, showcasing the effectiveness of LRC in improving both fundamental vision tasks and higher-level reasoning. This work highlights the critical role of multimodal context resolution and richness in advancing MLLM capabilities and provides a promising direction for future research on improving MLLM performance through enhanced context processing. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Limitations. While we demonstrate improved performance on long video sequences, the computational cost of processing such extended contexts remains significant. Further research is needed to explore more efficient learning techniques to reduce this overhead. Additionally, our current implementation primarily focuses on visual context properties. Extending LRC to reasoning-related areas, presents a promising avenue for future work and could further validate MLLM abilities. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "References   \nNiket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025.   \nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \nZechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. arXiv preprint arXiv:2409.19603, 2024.   \nMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1728–1738, 2021.   \nLuca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In ECCV, pages 850–865, 2016.   \nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022.   \nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR, pages 961–970, 2015.   \nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.   \nKeshigeyan Chandrasegaran, Agrim Gupta, Lea M Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour video-language understanding. arXiv preprint arXiv:2411.04998, 2024.   \nGuiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a.   \nKang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma, Tianning Zhang, Rui Su, et al. Fengwu: Pushing the skillful global medium-range weather forecast beyond 10 days lead. arXiv preprint arXiv:2304.02948, 2023a.   \nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023b.   \nZhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b.   \nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. CoRR, abs/2404.16821, 2024c.   \nZesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. CoRR, abs/2406.07476, 2024.   \nXTuner Contributors. Xtuner: A toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023.   \nMartin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking by overlap maximization. In CVPR, pages 4660–4669, 2019.   \nHenghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: A large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023.   \nDanny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-e: An embodied multimodal language model. In ICML, 2023.   \nYue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pages 75–92. Springer, 2025.   \nJiarui Fang and Shangchun Zhao. A unified sequence parallelism approach for long context generative ai. arXiv preprint arXiv:2405.07719, 2024.   \nXinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: A long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024.   \nHao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Forty-first International Conference on Machine Learning, 2024a.   \nJiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024b.   \nChaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024.   \nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.   \nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, pages 18995–19012, 2022.   \nLisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, Oct 2017.   \nLingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: A benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13480–13492, 2023.   \nYihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023.   \nBin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, 2024a.   \nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21807–21818, 2024b.   \nSam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023.   \nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.   \nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:32–73, 2017.   \nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, pages 9579–9589, 2024.   \nBo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. SiamRPN $^ { + + }$ : Evolution of siamese visual tracking with very deep networks. In CVPR, pages 4282–4291, 2019.   \nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024a.   \nFeng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b.   \nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   \nKunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In CVPR, pages 22195–22206, 2024c.   \nQingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024d.   \nXinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024e.   \nYanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, volume 15104 of Lecture Notes in Computer Science, pages 323–340. Springer, 2024f.   \nYunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: A versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024g.   \nKevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified video-language temporal grounding. In ICCV, pages 2794–2804, 2023.   \nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.   \nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, volume 36, 2024.   \nRuyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 1–18. Springer, 2025.   \nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024.   \nKarttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:46212–46244, 2023.   \nWonJun Moon, Sangeek Hyun, Su Been Lee, and Jae-Pil Heo. Correlation-guided query-dependency calibration in video representation learning for temporal grounding. CoRR, 2023.   \nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024.   \nAndreea-Maria Oncescu, Joao F Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: A video dataset with high-quality text and audio narrations. In ICASSP, pages 2265–2269. IEEE, 2021.   \nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.   \nOpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.   \nMengxue Qu, Xiaodong Chen, Wu Liu, Alicia Li, and Yao Zhao. Chatvtg: Video temporal grounding via chat with video dialogue large language models. In CVPR, pages 1847–1856, 2024.   \nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020.   \nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714.   \nRuchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: A long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024.   \nMichaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:25–36, 2013.   \nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024.   \nShuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. CVPR, abs/2312.02051, 2024.   \nShare. Sharegemini: Scaling up video caption data for multimodal large language models, June 2024. URL https: //github.com/Share14/ShareGemini.   \nXiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024.   \nYan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024.   \nEnxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18221–18232, 2024.   \nReuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13581–13591, 2024.   \nHaibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. arXiv preprint arXiv:2410.03290, 2024a.   \nWeihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024b.   \nWeiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024c.   \nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, volume 36, 2024d.   \nXiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 58–76. Springer, 2025.   \nXidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. CoRR, abs/2409.02889, 2024e.   \nYi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022.   \nYi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023.   \nYi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. In ECCV, 2024f.   \nYueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024g.   \nHongchen Wei and Zhenzhong Chen. Visual context window extension: A new perspective for long video understanding. arXiv preprint arXiv:2409.20018, 2024.   \nYuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In ECCV, pages 453–470. Springer, 2025.   \nDongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: A simple online baseline for referring video object segmentation. In ICCV, pages 2761–2770, 2023.   \nHaoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024a.   \nJiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In CVPR, pages 4974–4984, 2022.   \nJiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024b.   \nYinda Xu et al. Siamfc $^ { + + }$ : Towards robust and accurate visual tracking with target estimation guidelines. In AAAI, pages 140–148, 2020.   \nFuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024.   \nZiang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Task preference optimization: Improving multimodal large language models with vision task alignment. arXiv preprint arXiv:2412.19326, 2024.   \nDongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: A video is worth thousands of words. arXiv preprint arXiv:2406.06040, 2024.   \nJiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024.   \nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.   \nEn Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. In ECCV, pages 425–443. Springer, 2025.   \nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In ECCV, pages 69–85. Springer, 2016.   \nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, pages 13807–13816, 2024.   \nAbhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In CVPR, pages 23056–23065, 2023.   \nAo Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua. Next-chat: An lmm for chat, detection and segmentation. arXiv preprint arXiv:2311.04498, 2023a.   \nBeichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pages 310–325. Springer, 2025.   \nHongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: A benchmark of versatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817, 2023b.   \nPeiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. CoRR, abs/2406.16852, 2024a.   \nRuohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024b.   \nYuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024c. URL https://llava-vl.github.io/ blog/2024-04-30-llava-next-video/.   \nYuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024d.   \nZijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing Liu. Needle in a video haystack: A scalable synthetic framework for benchmarking video mllms. arXiv preprint arXiv:2406.09367, 2024.   \nJunjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024.   \nOrr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 16
    }
]